\documentclass{article}
%\usepackage[top=30pt,left=30pt,right=30pt]{geometry}
\usepackage[german,english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{pxfonts}
\usepackage{wasysym}
\usepackage{framed}
\usepackage{xcolor}
\usepackage{makeidx}
\usepackage{csquotes}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage{stmaryrd}
\usepackage{titlesec}
\titleformat{\paragraph}{\normalfont\itshape}{}{}{}

\newtheorem{theorem}{Theorem}  \numberwithin{theorem}{section}
\newtheorem{problem}{Problem}  \numberwithin{problem}{section}
\newtheorem{example}{Example}  \numberwithin{example}{section}
\newtheorem*{hypothesis}{Hypothesis}%  \numberwithin{hypothesis}{section}
\newtheorem{definition}{Definition}  \numberwithin{definition}{section}
\newtheorem{lemma}{Lemma}  \numberwithin{lemma}{section}
\newtheorem*{claim}{Claim}%  \numberwithin{claim}{section}
\newtheorem{remark}{Remark}  \numberwithin{remark}{section}
\newtheorem*{corollary}{Corollary}%  \numberwithin{corollary}{section}
\newtheorem{proposition}{Proposition}  \numberwithin{proposition}{section}

\algnewcommand{\algorithmicgoto}{\textbf{go to}}%
\algnewcommand{\Goto}[1]{\algorithmicgoto~\ref{#1}}%
\algrenewcommand{\algorithmiccomment}[1]{\hskip2em$\triangleright$ {\footnotesize #1}}

% definitions
\newcommand{\drawing}[1]{%
 \begin{figure}[t]
  \begin{center}
   \includegraphics{#1}
  \end{center}
 \end{figure}
}
\newcommand{\pic}[2]{%
 \begin{figure}[t]
  \begin{center}
   \includegraphics{#1}
   \caption{#2}
  \end{center}
 \end{figure}
}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\setdef}[2]{\left\{\left.#1\,\middle|\,#2\right.\right\}}
\newcommand{\angel}[1]{\left\langle#1\right\rangle}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\card}[1]{\left|#1\right|}
\newcommand{\given}[1]{\textbf{Given.} #1\par}
\newcommand{\find}[1]{\textbf{Find.} #1\par}
\newcommand{\dateref}[1]{\paragraph{\textit{This lecture took place on #1.}}}
\newcommand{\exist}{\;\exists\,}
\newcommand{\fall}{\;\forall\,}
\newcommand{\noproof}[1]{A proof for Theorem~\ref{#1} is not provided.}
\newcommand{\vectwo}[2]{\begin{pmatrix} #1 \\ #2 \end{pmatrix}}
\makeatletter
\newcommand{\xRightarrow}[2][]{\ext@arrow 0359\Rightarrowfill@{#1}{#2}}
\makeatother

\newcommand{\mtn}{(\mu\times\nu)} % mu times nu

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\detm}{det}
\DeclareMathOperator{\perm}{det}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\degree}{deg}
\DeclareMathOperator{\prop}{probability}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\vol}{vol}  % volume
\DeclareMathOperator{\sinc}{sinc}
\DeclareMathOperator*{\bigtimes}{\vartimes}

\makeatletter
\providecommand*{\dotcup}{%
  \mathbin{%
    \mathpalette\@dotcup{}%
  }%
}
\newcommand*{\@dotcup}[2]{%
  \ooalign{%
    $\m@th#1\cup$\cr
    \hidewidth$\m@th#1\cdot$\hidewidth
  }%
}
\makeatother


% metadata
\title{
  Analysis 2 \\
  \large{Lecture notes, University (of Technology) Graz} \\
  based on the lecture by Wolfgang Ring
}
\date{\today}
\author{Lukas Prokop}

% settings
\parindent0pt
\setlength{\parskip}{0.4\baselineskip}
%\setcounter{tocdepth}{2}

\makeindex

\begin{document}
\maketitle
\tableofcontents

\dateref{2018/03/06}

\section{Mathematical Redux and topological fundamentals}
\subsection{Metric}

\index{Metric}
\index{Distance function}
\index{Metric space}
\begin{definition}
  Let $X \neq \emptyset$ be a set. We define a map $d: X \times X \to [0,\infty)$.
  $d$ should behave like a geometrical distance. We require $\forall x, y, z \in X$:
  \begin{itemize}
    \item $d(x, y) = d(y, x)$ [called \emph{symmetry}]
    \item $d(x, y) = 0 \iff x = y$ [called \emph{positive definiteness}]
    \item $\forall x,y,z \in X: d(x, z) \leq d(x, y) + d(y, z)$ [called \emph{triangle inequality}]
  \end{itemize}
  Then $d$ is called \emph{metric} or \emph{distance function} on $X$.
  $(X, d)$ is called \emph{metric space}.
\end{definition}

\begin{example}
  \begin{itemize}\hfill{}
    \item $X \subseteq \mathbb C$, $d(x, y) = \card{x - y}$.
          It satisfies $\card{x - z} \leq \card{x - y} + \card{y - z}$
    \item $X \subseteq \mathbb R^n$, $\norm{x - y} = \angel{x - y, x - y}^{\frac12}$
  \end{itemize}
\end{example}

\begin{claim}
  \[ \angel{x, y} = \sum_{i=1}^n x_i y_i \]
  \[ \norm{x} = \angel{x,x}^{\frac12} = \sqrt{\sum_{i=1}^n x_i^2} \]
  \[ \norm{x} = \sqrt{x_1^2 + x_2^2} \]
  It holds that $\norm{x+y} \leq \norm{x} + \norm{y}$ [triangle inequality].
\end{claim}

\begin{proof}
  \begin{align*}
    \norm{x + y}^2
      &= \angel{x+y, x+y} \\
      &= \angel{x,x} + \angel{x,y} + \angel{y,x} + \angel{y,y} \\
      &= \norm{x}^2 + 2\angel{x,y} + \norm{y}^2 \\
      &\leq \norm{x}^2 + 2 \norm{x} \norm{y} + \norm{y}^2  & [\text{see Cauchy-Schwarz inequality}] \\
      &= (\norm{x} + \norm{y})^2 \\
    \norm{x - y}^2
      &= \angel{x - y, x - y} \\
      &= \norm{x}^2 - 2\angel{x,y} + \norm{y}^2 \\
    \norm{x + y}^2 + \norm{x - y}^2
      &= 2 \left(\norm{x}^2 + \norm{y}^2\right)
  \end{align*}
\end{proof}

\subsection{Cauchy-Schwarz inequality}

\begin{theorem}[Cauchy-Schwarz inequality]
  \[ \card{\angel{x, y}} \leq \norm{x} \norm{y} \]
\end{theorem}
\begin{proof}
  \[ 0 \leq \angel{x - \lambda y, x - \lambda y} = \norm{x}^2 - 2\lambda \angel{x, y} + \lambda^2 \norm{y}^2 \qquad \forall \lambda \in \mathbb R \]
  Let $\lambda = \frac{\angel{x, y}}{\norm{y}^2}$. Then,
  \[ 0 \leq \norm{x}^2 - 2 \frac{\card{\angel{x, y}}^2}{\norm{y}^2} + \frac{\card{\angel{x, y}}^2}{\norm{y}^4} \cdot \norm{y}^2 \]
  \[ \implies 0 \leq \norm{x}^2 - \frac{\card{\angel{x, y}}^2}{\norm{y}^2} \]
  \[ \implies \card{\angel{x, y}}^2 \leq \norm{x}^2 \cdot \norm{y}^2 \]
\end{proof}

\subsection{Euclidean norm}

\index{Euclidean norm}
\index{Norm}
\index{Normed vector space}
\begin{definition}
  $\norm{x} = \sqrt{\sum_{i=1}^n x_i^2}$ is called \emph{Euclidean norm} (length) of vector $x \in \mathbb R^n$. $\norm{x} = \angel{x, x}^{\frac12}$
  It holds that
  \begin{enumerate}
    \item $\norm{\lambda x} = \card{\lambda} \norm{x} \forall x \in \mathbb R^n, \lambda \in \mathbb R$
    \item $\norm{x} = 0 \iff x = 0$ in $\mathbb R^n$
    \item $\norm{x + y} \leq \norm{x} + \norm{y}$
  \end{enumerate}

  In general: Let $V$ be a vector space over $\mathbb R$. A map $\norm\cdot$, which assigns every vector $x$ a non-negative real number satisfying the properties above,
  is called \emph{norm on $V$}. Then $(V, \norm\cdot)$ is called a \emph{normed vector space}.
\end{definition}

Let $X \subseteq \mathbb R^n$ ($V$ is a normed vector space), then $d(x, y) = \norm{x - y}$ is a metric on $X$.
\[ \norm{y - x} = \norm{(-1) (x - y)} = \card{-1} \cdot \norm{x - y} = \norm{x - y} \]
\[ d(x, y) = 0 \iff \norm{x - y} = 0 \iff x - y = 0 \iff x = y \]
\[ d(x, z) = \norm{z - x} = \norm{z - y + y - x} \leq \norm{z - y} + \norm{y - x} = d(z, y) + d(y, x) \]

\subsection{Metric space}

\begin{example}[metric space]
  Metric space, distance is not a norm.
  Consider an area in $\mathbb R^3$.

  $d(x,y)$ is the shortest path, connecting $x$ and $y$ in $X$.
  See Figure~\ref{img:example-r3}

  \begin{figure}
    \begin{center}
      \includegraphics{img/01_example.pdf}
      \caption{Example in $\mathbb R^3$. The red line illustrates the shortest path}
      \label{img:example-r3}
    \end{center}
  \end{figure}
\end{example}

\begin{example}[French railway]
  All connections between two cities pass through Paris except one city is Paris.
\end{example}

\begin{example}
  $X = \mathbb R^2$. Let $p \in \mathbb R^2$ be fixed.
  \[
    d(x, y) = \begin{cases}
      \card{x - y}                & \text{if } x, y, p \text{ are on one line} \\
      \card{x - p} + \card{p - y} & \text{if } x, y, p \text{ are not on one line}
    \end{cases}
  \]
\end{example}

\subsection{Open sets, convergence and accumulation points}

Now we put some terminology into the context of a metric space.
$(X, d)$ is a metric space.

\index{Open sphere}
\begin{definition}
  Let $x \in X$, $r \geq 0$.
  \[ K_r(x) = \setdef{z \in X}{d(x, z) < r} \]
  Is an \emph{open sphere} with radius $r$ and center $x$.
\end{definition}

\begin{definition}
  \[ \overline{K_r(x)} = \setdef{z \in X}{d(x, z) \leq r} \]
  Closed sphere with center $x$ and radius $r$.
\end{definition}

\index{Convergence}
\index{Cauchy sequence}
\begin{definition}[Sequences in $X$]
  Let $(x_n)_{n\in\mathbb N}$ be a sequence in $X$ (hence, $x_n \in X \forall n \in \mathbb N$)
  \begin{enumerate}
    \item $(x_n)_{n\in\mathbb N}$ is called \emph{convergent} and limit $x \in X$ if
      \[ \forall \varepsilon > 0 \exists N \in \mathbb N: n \geq N \implies d(x_n, x) < \varepsilon \]
      Denoted as $\lim_{n\to\infty} x_n = x$.
    \item $(x_n)_{n\in\mathbb N}$ is a Cauchy sequence if
      \[ \forall \varepsilon > 0 \exists N \in \mathbb N: n, m \geq N \implies d(x_n, x_m) < \varepsilon \]
  \end{enumerate}
  Every convergent sequence is also a Cauchy sequence.
\end{definition}

\begin{proof}
  Let $(x_n)_{n\in\mathbb N}$ be convergent with limit $x$. Let $\varepsilon > 0$ be arbitrary.
  Because $(x_n)_{n\in\mathbb N}$ is convergent, there exists $N \in \mathbb N$ such that $n \geq N \implies d(x_n, x) < \frac{\varepsilon}{2}$.
  Now let $n,m \geq N$. Then it holds that
  \[ d(x_n, x_m) \leq \underbrace{d(x_n, x)}_{< \frac{\varepsilon}2} + \underbrace{d(x, x_m)}_{< \frac\varepsilon2} < \varepsilon \]
\end{proof}

\index{Complete metric space}
\begin{definition}
  $(X, d)$ is called \emph{complete metric space} if every Cauchy sequence in $X$ is also convergent (has a limit).
\end{definition}

$\mathbb R$ is complete. $\mathbb R^n$ is also complete. $\mathbb Q \subseteq \mathbb R$ is incomplete.

\index{Accumulation point}
\begin{definition}
  Let $(x_n)_{n\in\mathbb N}$ be a sequence of $X$ is called \enquote{accumulation point} (\foreignlanguage{german}{dt. H\"aufungspunkt}) of the sequence.
  $\forall \varepsilon > 0$, it holds that $K_{\varepsilon}(x)$ contains infinitely many sequence elements.
\end{definition}

\dateref{2018/03/08}

TODO

\[ d(x,y) =0 \iff x = y \]
\[ \forall x,y \in X: d(x,y) = d(y,x) \]
\[ d(x,z) \leq d(x,y) + d(y,z) \forall x,y,z \in X \]

\subsection{Norm}

\index{Norm}
Let $V$ be a vector space. $\norm{\cdot}$ is called \emph{norm on $V$}.
\[ \norm{x} = 0 \iff x = 0 \]
\[ \forall \lambda \in \mathbb R, \mathbb C: \forall x \in V: \norm{\lambda x} = \card{\lambda} \norm{x} \]
\[ \forall x,y,z \in V: \norm{x + y} \leq \norm{x} + \norm{y} \]

Let $X \subseteq V$ be a subset of normed vector space $V$.
Then $X$ is a metric space with $d(x,y) = \norm{x - y}$.

\index{Euclidean norm}
For $V = \mathbb R^n$. Then
\[ \norm{x} = \left(\sum_{i=1}^n x_i^2\right)^{\frac12} \]
is a norm on $\mathbb R^n$. $\norm{x}_2$ is called \emph{Euclidean norm on $\mathbb R^n$}.

Other norms in $\mathbb R^n$:
\[ \norm{x}_{\infty} = \max\setdef{\card{x_i}}{i = 1,\dots,n} \]
\[ \norm{x}_1 = \sum_{i=1}^n \card{x_i} \]
for $1 \leq p < \infty$.
\[ \norm{x}_p = \left(\sum_{i=1}^n \card{x_i}^p\right)^{\frac1p} \]
e.g. $\norm{x}_1$ in $\mathbb R^2$

\begin{figure}[!ht]
  \begin{center}
    \includegraphics{img/02_1norm.pdf}
    \caption{Visualizing $\norm{x}_1$}
    \label{img:1norm}
  \end{center}
\end{figure}

\index{Manhattan metric}
\[ \norm{x - y} = \card{x_1 - y_1} + \card{x_1 - y_2} \]
is the so-called \emph{Manhattan metric}.

The concepts \enquote{subsequence}, \enquote{final element of a sequence}, \enquote{reordering of a sequence} correspond one-by-one to metric spaces.

\index{Accumulation point}
\begin{definition}[Accumulation point]
  Let $(X_n)_{n\in\mathbb N}$ be a sequence in $X$. $x \in X$ is called \emph{accumulation point of sequence $X$}
  if $\forall \varepsilon > 0$ the sphere $K_{\varepsilon}(x)$ contains infinitely many elements.
\end{definition}

\begin{lemma}
  $x \in X$ is accumulation point of sequence $(x_n)_{n\in\mathbb N}$
  if and only iff there exists a subsequence $(x_{n_k})_{k \in \mathbb N}$ such that
  $x = \lim_{k\to\infty} x_{n_k}$.
\end{lemma}

\begin{proof}
  See Analysis 1 course
\end{proof}

%\subsection{Sets in metric spaces}
\subsection{Contact point}

Let $B \subseteq X$, $X$ is a metric space. Then $B$ with $d$ is a metric space itself.

\index{Contact point}
\begin{definition}
  Let $B \subseteq X$ and $x \in X$. We say, $x$ is a \emph{contact point of $B$}
  if $\forall \varepsilon > 0: K_{\varepsilon}(x) \cap B \neq \emptyset$.

  [ $y \in X$ is not a contact point of $B \iff \exists \varepsilon > 0: K_{\varepsilon}(y) \cap B = \emptyset$ ]

  See Figure~\ref{img:cp}.
\end{definition}

\begin{figure}
  \begin{center}
    \includegraphics{img/03_contact_point.pdf}
    \caption{Contact points in set $B$}
    \label{img:cp}
  \end{center}
\end{figure}

We let $\overline{B} = \setdef{x \in X}{x \text{ is contact point of } B}$.

$\overline{B}$ is called closed hull of $B$.

$B$ is called closed if $B = \overline{B}$, hence, every contact point is also element of $B$.

\begin{remark}
  Because $\forall x \in B$ holds $K_r(x) \cap B \supseteq \set{x} \forall r > 0$ is $x$ always contact point of $B$.
  Also $B \subseteq \overline{B}$ (always)
\end{remark}

\begin{lemma}
  $x$ is contact point of $B \iff \exists (x_n)_{n\in\mathbb N}$ with $x_n \in B$ and $\lim_{n\to\infty} x_n = x$.
\end{lemma}
\begin{proof}
  Let $x$ be a contact point of $B$.

  Direction $\Rightarrow$:
  Because $K_{\frac1n}(x) \cap B \neq \emptyset$, choose $X_n \in K_{\frac1n}(x) \cap B$.
  The sequence $(x_n)_{n\in\mathbb N}$ has property $d(x_n, x) < \frac1n$.
  Let $\varepsilon > 0$ be arbitrary. Choose $N \in \mathbb N$ sch that $N > \frac1\varepsilon$ (consider the Archimedean axiom).
  Then for $n \geq N$, $d(x_n, x) < \frac1n \leq \frac1N < \varepsilon$, hence $\lim_{n\to\infty} x_n = x$.

  Direction $\Leftarrow$:
  Let $x = \lim_{n\to\infty} x_n$ and $x_n \in B$.
  Let $\varepsilon > 0$ be arbitrary and $N \in \mathbb N$ such that $d(x_n, x) < \varepsilon \forall n \geq N$.
  Then $d(x_N, x) < \varepsilon$, hence
  \[ x_N \in \underbrace{K_{\varepsilon}(x) \cap B}_{\neq \emptyset} \]
  So $x$ is contact point of $B$.
\end{proof}

\begin{lemma}
  It holds that $\forall B \subseteq X: \overline{B} = \overline{\overline{B}}$,
  hence $\overline{B}$ itself is closed.
\end{lemma}
\begin{proof}
  Show that $x \in \overline{B}$. Let $x \in \overline{\overline{B}}$.
  \[ \iff \forall \varepsilon > 0: K_{\varepsilon}(x) \cap \overline{B} \neq \emptyset \]
  Therefore let $\varepsilon > 0$ be arbitrary and $x \in \overline{\overline{B}}$.

  Show that $K_{\varepsilon}(x) \cap B \neq \emptyset$.

  Because $x \in \overline{\overline{B}}: \exists y \in \overline{B}: y \in K_{\frac\varepsilon2}(x)$.
  Because $y \in \overline{B}: \exists z \in B: z \in K_{\frac\varepsilon2}(y)$. Hence,
  \[ d(z,x) \leq \underbrace{d(z,y)}_{<\frac\varepsilon2} + \underbrace{d(y,x)}_{<\frac\varepsilon2} < \varepsilon \]
  so $z \in K(x,\varepsilon) \cap B$. So $x$ is contact point of $B \implies x \in \overline{B}$.
\end{proof}

\begin{lemma}
  \label{lemma4}
  Let $X$ be a metric space.
  \begin{itemize}
    \item
      $A_i \subseteq X$ be closed $\forall i in I$.
      Then $A = \bigcap_{i \in I} A_i = \setdef{x \in X}{x \in A_i \forall i \in I}$
      is closed itself.
    \item
      $A_1, \dots, A_n \subseteq X$ are closed. Then $\bigcup_{k=1}^n A_k$ is closed in $X$.
    \item
      $\varphi$ is closed, $X$ is closed.
  \end{itemize}
\end{lemma}

\begin{proof}
  See Analysis 1 course.
\end{proof}

\begin{definition} % definition 8
  Let $x \in X$ is called accumulation point of set $B \subseteq X$ if $\forall \varepsilon > 0: (K_{\varepsilon}(x) \setminus \set{x}) \cap B \neq \emptyset$.
\end{definition}

\begin{remark}
  Accumulation \emph{points} only exist in the context of \emph{sets}.
  Accumulation \emph{values} only exist in the context of \emph{sequences}.

  For example $(+1, -1, +1, -1, +1, \dots)$ has accumulation \emph{values} $+1$ and $-1$.
\end{remark}

\begin{lemma}
  Let $x \in X$ is accumulation point on $B \iff$ every sphere $K_{\varepsilon}(x)$ contains infinitely many points of $B$.
\end{lemma}
\begin{proof}
  Direction $\Leftarrow$ is trivial.

  Direction $\Rightarrow$:
  Choose $x_1 \in (K_1(x) \setminus \set{x}) \cap B$, hence $x_1 \neq x$, $x_1 \in B$ and $d(x_1, x) < 1$.
  Let $r_1 = 1$.

  Inductive: choose $r_n = \min(\frac1n, d(x_{n-1}, x))$ and $x_n \in (K_{r_n}(x) \setminus \set{x}) \cap B$.
  Then $d(x_n, x) > 0$ (because $x_n \neq x$) where $d(x_n, x) < r_n < \frac1n$.
  \[ 0 < d(x_n, x) < \frac1n \]
  Furthermore, $d(x_n, x) < r_n \leq d(x_{n-1}, x)$.
  So $x_n \neq x_{n-1}$.

  Inductive: $x_n \neq x_{n-1} \neq x_{n-2} \neq \dots \neq x_1$.
  Now consider arbitrary $\varepsilon > 0$ and $N$ large enough such that $\frac1N < \varepsilon$.

  Then it holds that $\forall n \geq N: 0 < d(x_n, x) < \frac1n \leq \frac1N < \varepsilon$.
  So $K_{\varepsilon}(x) \cap B$ contains infinitely many points $x_N, x_{N+1}, x_{N+2}, \dots$.
\end{proof}

\index{Inner point}
\begin{definition} % definition 9
  Let $U \subseteq X$ and $x \in U$.
  We say $x$ is an \emph{inner point of $U$} if $\exists r > 0: K_r(x) \subseteq U$.
  We let $\mathring{U} = \setdef{x \in U}{x \text{ is inner point of } U}$ and call it \emph{interior of $U$} (\foreignlanguage{german}{offenen Kern von $U$} or \foreignlanguage{german}{das Innere von $U$}).
  $O \subseteq X$ is called \emph{open} (open set), if every point $x \in O$ is also an inner point of $O$.
  Hence $\mathring{O} = O$.
\end{definition}

% TODO drawing 04_open.svg

\begin{example}
  Let $K_r(x)$ with $r > 0$ be an open sphere in $X$.
  Then $K_r(x)$ is an open set in $X$.
\end{example}

Why? Let $y \in K_r(x)$. Show that $y$ is an inner point of the sphere.
$d(y,x) = s < r$. Define $r' = r - s > 0$.
Claim: $K_r'(y) \subseteq K_r(x)$.

TODO drawing %05_open_sphere.pdf

TODO

So it holds that $z \in K_r(x)$ and therefore $K_{r'}(y) \subseteq K_r(x)$.

\begin{lemma}
  Let $U \subseteq X$ be arbitrary. Then $\mathring{U} \subseteq X$ be an open set in $X$.
\end{lemma}
\begin{proof}
  Let $x \in \mathring{U}$, hence $x$ is an inner point of $U$.
  Show that $x$ is an inner point of $\mathring{U}$, also $\exists r > 0: K_r(x) \subseteq \mathring{U}$.

  Because $x \in \mathring U$, $r > 0 exists$: $K_r(x) \subseteq U$.
  Claim: Every point $y \in K_r(x)$ is also an inner point of $U$.
  Obvious (previous example), because $r' > 0$ exists such that
  $K_{r'}(y) \subseteq K_r(x) \subseteq U$ so $y \in \mathring U$ and $K_r(x) \subseteq \mathring U$.
\end{proof}

\begin{theorem}
  \label{saetzchen1}
  Let $X$ be a metric space.
  \[ A \subseteq X \text{ is closed in $X$ } \iff O = X \setminus A = A^C \text{ is open} \]
\end{theorem}

\begin{proof}
  Direction $\Leftarrow$.
  Let $A$ be closed and $O + A^C$. We choose $x \in O$ and show that $x$ is in the interior of $O$.

  Assume the opoosite.
  \[ \forall \varepsilon > 0: \underbrace{\neg \left(K_{\varepsilon}(x) \subseteq O\right)}_{\iff K_{\varepsilon}(x) \cap O^C \neq \emptyset} \]
  where $O^C = A$. So $x$ is contact point of $A$. Because $A$ is closed, it holds that $x \in A$.
  This contradicts with $x \in O = A^C$.

  Direction $\Rightarrow$.
  TODO
  $K_r(x) \cap \underbrace{A}_{=O^C} = \emptyset$. Hence $x$ is not a contact point of $A$.

  So every contact point of $A$ is also an element of $A$ and $A$ is closed.
\end{proof}

\begin{theorem}
  \label{satz2}
  Let $X$ be a metric space. Then it holds that
  \begin{itemize}
  	\item
  	  If $O_i \subseteq X$ is open in $X$ $\forall i \in I$.
  	  Then also $O = \bigcup_{i \in I} O_i$ is open in $X$.
  	\item
  	  If $O_1, O_2, \dots, O_n$ is open in $X$, then $\bigcap_{k=1}^n O_k$ is open in $X$.
  	\item
  	  $X$ is open, $\emptyset$ is open.
  \end{itemize}
\end{theorem}

\begin{proof}
  By Lemma~\ref{lemma4}, Theorem~\ref{saetzchen1} and De Morgan's Laws:
  \[ \left(\bigcup_{i \in I} A_i\right)^C = \bigcap_{i in I} A_i^C \]
\end{proof}

\subsection{Topology}

\index{Topology}
\index{Topological spcae}
\begin{definition}
  Given a set $X$. If a subset $T \subseteq \mathcal P(X)$ is defined
  such that the elements $O \in T$ (hence $O \subseteq X$) satisfy the conditions of Theorem~\ref{satz2},
  then $T$ is called \emph{topology on $X$}.
  $(X, T)$ is called topological space.

  The sets $O \in T$ are called \emph{open sets} in terms of $T$.
  The complements $A = O^C$ for $O \in T$ are called \emph{closed sets}.
\end{definition}

\begin{definition}
  Let $x \in U \subseteq X$. We claim that $U$ is a neighborhood of $x$,
  if $r > 0$ exists such that $x \in K_r(X) \subseteq U$

  See Figure~\ref{img:neigh}
\end{definition}

\begin{figure}
  \begin{center}
    \includegraphics{img/06_neighborhood.pdf}
    \caption{Neighborhood of $x$}
    \label{img:neigh}
  \end{center}
\end{figure}

\begin{remark}
  $O \subseteq X$ is open iff $O$ is neighborhood of every point $x \in O$.
\end{remark}

\begin{definition}
  Let $X$ and $Y$ be metric spaces and $x_0 \in X$.
  Let $f: X \to Y$ be given. We say $f$ is continuous in $x_0$ if
  \[ \forall \varepsilon > 0 \exists \delta > 0: \forall x \in X: d_X(x, x_0) < \delta \implies d_Y(f(x), f(x_0)) < \varepsilon \]
  Here, $d_X$ is a metric on $X$ and $d_Y$ is a metric on $Y$.
\end{definition}

\dateref{2018/03/13}

TODO I missed the first twenty minutes (including Satz~3 and 4)

\begin{proof}
  Direction $\Rightarrow$.

  Let $f$ be continuous in $X$ and let $O \subseteq Y$ be open. Let $U = f^{-1}(O)$ and choose $x_0 \in U$. Then $f(x_0) \in O$, hence $O$ is a neighborhood of $f(x_0)$. By Theorem~\ref{satz3}~(b), it follows that $U = f^{-1}(O)$ is a neighborhood of $x_0$.

  Hence, $U$ is neighborhood of every of its points, hence open in $X$.

  Direction $\Leftarrow$.

  Let the preimages of open sets be open and $x_0 \in X$ and $y_0 = f(x_0)$.
  Let $V$ be a neighborhood of $y_0 = f(x_0)$, hence $\exists \varepsilon > 0: K_{\varepsilon}(f(x_0)) \subseteq V$. Because $K_{\varepsilon}(f(x_0))$ is an open set, it holds that $f^{-1}(K_{\varepsilon}(f(x_0))) \in x_0$ is open in $X$.

  Therefore, there exists $\delta > 0$ such that $K_{\delta}(x_0) \subseteq f^{-1}(K_{\varepsilon}(f(x_0))) \subseteq f^{-1}(V)$. Hence, $f^{-1}(V)$ is a neighborhood of $x_0$. Then by Theorem~\ref{satz3}~(b), it follows that $f$ is continuous in $x_0$ (chosen arbitrarily). Hence $f$ is continuous on $X$.
\end{proof}

\section{Variations of continuity notions}

\index{Uniformly continuous}
\begin{definition} % Definition 13
  Let $f: X \to Y$ be given.
  We call \enquote{f uniformly continuous on $X$} if
  \[
  	\forall \varepsilon > 0 \exists \delta > 0:
  	\forall x, y \in X \land d_X(x,y) < \delta \implies d_Y(f(x), f(y)) < \varepsilon
  \]
\end{definition}

\begin{remark}
  Compare it with the definition of \enquote{continuous in $X$}:
  \[ \forall x \in X \forall \varepsilon > 0 \exists \delta > 0: \forall y \in X: d_X(x,y) < \delta \implies d_Y(f(x), f(y)) < \varepsilon \]
  The difference is the location of the $\forall x \in X$ quantifier.

  Every uniformly continuous map is continuous.

  Example: $f: (0, \infty) \to (0, \infty)$ with $f(x) = \frac1x$ is continuous, but not continuously continuous.
\end{remark}

\index{Lipschitz continuity}
\begin{definition} % Definition 14
  $f: X \to Y$ is called \emph{Lipschitz continuous} with \emph{Lipschitz constant} $L \geq 0$ if $\forall x, y \in X: d_Y(f(x), f(y)) \leq L \cdot d_X(x,y)$.

  Rudolf Lipschitz [1832--1903], University of Bonn
\end{definition}

\begin{theorem}
  Every Lipschitz continuous function is uniformly continuous.
\end{theorem}
\begin{proof}
  For $\varepsilon > 0$, choose $\delta = \frac{\varepsilon}{L+1}$. Then it holds that
  $d_X(x,y) < \delta = \frac{\varepsilon}{L+1} \implies d_Y(f(x), f(y)) \leq L \cdot d_X(x,y) < \frac{L}{L+1} \cdot \varepsilon < \varepsilon$.
\end{proof}

\begin{itemize}
  \item Most often $X \subseteq V$, $Y \subseteq W$. $V$ and $W$ are normed vector spaces and $d(x, y) = \norm{x - y}$
\end{itemize}

\index{Contraction}
\begin{definition} % Definition 15
  A Lipschitz continuous map $f: X \to X$ with Lipschitz constant $L < 1$ is called \emph{contraction on $X$}. Compare with Figure~\ref{img:contraction}
\end{definition}

\begin{figure}
  \begin{center}
    \includegraphics{img/07_contraction.pdf}
    \caption{A contraction maps to points closer to each other}
    \label{img:contraction}
  \end{center}
\end{figure}

\index{Fixed point}
\begin{theorem}[Banach fixed-point theorem] % Satz 5
  Let $f: X \to X$ be a contraction and $X$ be complete. Then there exists a uniquely defined $\hat{x} \in X$ such that $\hat{x} = f(\hat{x})$. $\hat{x}$ is called fixed point on $f$.
  Furthermore it holds that $x_0 \in X$ is arbitrary and $x_n = f(x_{n-1})$ for all $n \geq 1$.
  \[ \lim_{n \to \infty} x_n = \hat{x} \]
\end{theorem}

TODO drawing Banach's fixed point theorem

% Common exam question
\begin{proof}
  Let $x_0 \in X$ be arbitrary.
  $x_n$ is constructed inductively by $x_n = f(x_{n-1})$ for all $n \geq 1$.

  \begin{claim}
    $(x_n)_{n \in \mathbb N}$ is a Cauchy sequence in $X$.
  \end{claim}

  \begin{align*}
    d(x_n, x_{n+k}) &\leq d(x_n, x_{n+1}) + d(x_{n+1}, x_{n+2}) + \dots + d(x_{n+k-1}, x_{n+k}) \\
  \intertext{by triangle inequality}
      &= d(x_n, x_{n+1}) + d(f(x_n), f(x_{n+1})) + d(f(x_{n+1}), f(x_n + 2)) + \dots + d(f(x_{n+k-2}), f(x_{n+k-1})) \\
      &\leq d(x_n, x_{n+1}) + L\left(d(x_n, x_{n+1}) + d(x_{n+1}, x_{n+2}) + \dots + d(x_{n+k-2}, x_{n+k-1})\right) \\
  \intertext{this inequality is given by contraction}
      &= d(x_n, x_{n+1}) (1 + L) + L \left(d(f(x_n), f(x_{n+1})) + \dots + d(f(x_{n+k-3}), f(x_{n+k-2}))\right) \\
      &\leq d(x_n, x_{n+1}) (1 + L) + L^2 \left[d(x_n, x_{n+1} + \dots + d(x_{n + k - 3}, x_{n+k-2})\right] \\
      &\leq \dots \leq d(x_n, x_{n+1}) (1 + L + L^2 + \dots + L^{k-1}) \\
      &= d(f(x_{n-1}, f(x_n)) \left(\sum_{j=0}^{k-1} L^j\right) \leq L d(x_{n-1}, x_n) \cdot \left(\sum_{j=0}^{k-1} L^j\right) \\
      &\leq L^n d(x_0, x_1) \cdot \underbrace{\left(\sum_{j=1}^{k-1} L^j\right)}_{\leq \sum_{j=0}^\infty L^j = \frac{1}{1 - L}} \\
      &\leq \frac{L^n}{1 - L} d(x_0, x_1) \\
    d(x_n, x_{n+k}) &\leq \frac{L^n}{1 - L} d(x_0, x_1) \forall n \in \mathbb N \forall k \in \mathbb N_0
  \end{align*}
  with $0 \leq L < 1$.
  \[ \frac{L^n}{\underbrace{1 - L}_{> 0}} d(x_0, x_1) < \varepsilon \impliedby \]
  \[ L^n < \frac{\varepsilon}{d(x_0, x_1) + 1} (1 - L) \qquad (L > 0) \]
  \[ \iff n \underbrace{\ln{L}}_{<0} < \ln \frac{\varepsilon}{d(x_0, x_1) + 1} (1 - L) \]
  \[ \iff n > \frac{1}{\ln{L}} \ln \frac{\varepsilon}{d(x_0, x_1) + 1} (1 - L) \]

  Hence $(x_n)_{n \in \mathbb N}$ is a Cauchy sequence in $X$. $X$ is complete, hence $\exists \hat{x} \in X: \hat{x} = \lim_{n \to \infty} x_n$.
  Because $\hat{x} = \lim_{n\to\infty} x_{n+1} = \lim_{n\to\infty} f(x_n) = f(\hat{x})$ where the last equality is given by continuity of $f$.
  Therefore $\hat{x} = f(\hat{x})$ is a fixed point on $f$.

  It remains to prove uniqueness:

  Let $\tilde{x} = f(\tilde{x})$. Then it holds that $d(\hat{x}, \tilde{x}) = d(f(\hat{x}), f(\tilde{x})) \leq L d(\hat{x}, \tilde{x})$ with $L < 1$.
  If $d(\hat{x}, \tilde{x}) > 0$, then $1 \leq L$. This is a contradiction.
  Hence $d(\hat{x}, \tilde{x}) = 0$ must hold, hence $\hat{x} = \tilde{x}$.
\end{proof}

\begin{remark}
  \begin{itemize}
  	\item The Fixed Point Theorem provides an algorithm for numeric computation of $\hat{x}$.
    \item It can reformulate problems $f(x) = 0$ (in $\mathbb R^n$) to
      \[ f(x) + x = g(x) = x \]
    \item Attention: The conditions of the Fixed Point Theorem cannot be changed to the structure
      \[ d(f(x), f(y)) < L \cdot d(x, y) \land L \leq 1 \]
      or
      \[ d(f(x), f(y)) \leq L \cdot d(x, y) \land L < 1 \]
      This will be discussed in the practicals.
  \end{itemize}
\end{remark}

\begin{lemma} % Lemma 7
  \label{lemma7}
  Let $X$ be a complete metric space. Let $A \subseteq X$ be closed.
  Then $(A, d)$ is itself a complete, metric space.
\end{lemma}

\begin{proof}
  Let $(x_n)_{n \in \mathbb N}$ be a Cauchy sequence in $A$ ($x_n \in A$).
  Then $(x_n)_{n \in \mathbb N}$ is also a Cauchy sequence in $X$.
  Because $X$ is complete, there exists $\hat{x} = \lim_{n\to\infty} x_n$.
  Therefore $\hat{x}$ is a contact point of $A$.
  Because $A$ is closed, it holds that $\hat{x} \in A$.

  Therefore every Cauchy sequence in $A$ has a limit point in $A$,
  hence $A$ is complete.
\end{proof}

\section{Compactness}

\index{Compactness}
\begin{definition} % Definition 16
  A metric space $(X, d)$ is called \emph{compact} if every sequence $(x_n)_{n \in \mathbb N}$
  has a convergent subsequence.

  Specifically, this definition is called sequence compactness. The other definition defines compactness as closed and bounded subset of an Euclidean space. The latter definition only works for a subset of branches in mathematics. Therefore the generalization is recommended to be remembered.
\end{definition}

\begin{lemma} % Lemma 8
  Let $X$ be a compact, metric space. Then $X$ is complete.
\end{lemma}
\begin{proof}
  Let $(x_n)_{n\in\mathbb N}$ be a Cauchy sequence in $X$. By compactness, it follows that
  $\exists (x_{n_k})_{k \in \mathbb N}$ with $\lim_{k\to\infty} x_{n_k} = \hat{x}$.
  Choose $\varepsilon > 0$ arbitrary and $L$ large enough such that $k \geq L \implies d(x_{n_k}, \hat{x}) < \frac\varepsilon2$. Furthermore choose $N \in \mathbb N$ large enough such that $n,m \geq N \implies d(x_n, x_m) < \frac\varepsilon2$ (satisfied, because $(x_n)_{n \in \mathbb N}$ is a Cauchy sequence).
  Choose $K \geq L$ and $n_k \geq N$. Let $n_k$ be fixed this way.
  Then it holds $\forall n \geq N: d(x_n, \hat{x}) \leq d(x_n, x_{n_k}) + d(x_{n_k}, \hat{x}) < \frac\varepsilon2 + \frac\varepsilon2 = \varepsilon$. The first summand $\frac\varepsilon2$ results from the Cauchy sequence property, the second summand $\frac\varepsilon2$ results by convergence of $(x_{n_k})$. Hence $(x_n)_{n\in\mathbb N}$ is convergent with limit $\hat{x}$.
\end{proof}

\index{Boundedness}
\begin{definition} % Definition 17
  A metric space $X$ is called \emph{bounded} if there exists $M \geq 0$, such that $d(x,y) \leq M \forall x,y \in X$.
\end{definition}
It holds for arbitrary $x \in X$ that $\forall y \in X: y \in K_M(x)$.
So, $X \subseteq K_M(x)$.
On the contrary, let $X \subseteq \overline{K_M(x)}$ and let $y \in X$ and $z \in X$ be arbitrary. Then it holds that $d(y, z) \leq d(y, x) + d(x, z) \leq M + M = 2M$. Hence, $X$ is bounded.

So, $X$ is bounded $\iff \exists x \in X \land M \geq 0: X \subseteq \overline{K_M(x)}$.

\begin{lemma} % Lemma 9
  Every compact, metric space is also bounded.
\end{lemma}

\begin{proof}
  Assume $X$ is unbounded.

  We construct a sequence of points $(x_n)_{n \in \mathbb N}$ with $d(x_n, x_m) \geq 1 \forall n,m \in \mathbb N$ with $n \neq m$.

  We use the following auxiliary result: Let $B = \bigcup_{j=1}^n K_1(z_j)$ for arbitrary $n \in \mathbb N$ and arbitrary $z_j \in X$. Then $B$ is bounded. This result will be part of the practicals.

  We construct $(x_n)_{n\in\mathbb N}$ inductively. Choose arbitrary $x_0 \in X$. Assume $(x_1, \dots, x_{n-1})$ are already found. Then it holds that
  \[ \underbrace{X}_{\text{unbounded}} \not\subseteq \underbrace{\bigcup_{j=1}^{n-1} K_1(x_j)}_{\text{bounded}} \]
  hence $\exists x_n \in X \setminus \bigcup_{j=1}^{n-1} K_1(x_j)$.
  Because $x_n \not\in K_1(x_j)$ for $j = 0, \dots, n-1$ it holds that $d(x_n, x_j) \geq 1 \forall j < n$. We get $(x_n)_{n \in \mathbb N}$ with $d(x_n, x_m) \geq 1 \forall n \in \mathbb N \forall m < n$, hence $m \neq n$. Because $d(x_n, x_m) \geq 1$, i.e. $(x_n)_{n \in \mathbb N}$ does not contain any Cauchy sequence as subsequence, $(x_n)_{n \in \mathbb N}$ does not have a convergent subsequence. Therefore $X$ is not compact.
\end{proof}

\dateref{2018/03/15}

Every compact metric space is bounded. Every compact metric space is complete.
In $\mathbb C$ ($\mathbb R^n$) it holds that $A \subseteq \mathbb C$ is closed.
Then $A$ with metric $d(x,y) = \card{x - y}$ is complete as metric space.

If $A$ is additionally bounded, then $A$ is compact (see course Analysis 1, Bolzano-Weierstrass).

Attention! Let $V$ be an infinite-dimensional, complete, normed vector space.
For example, $V = \mathcal C([a,b], \mathbb R) = \setdef{f: [a,b] \to \mathbb R}{f \text{ is continuous in } [a,b]}$ with norm $\norm{f}_{\infty} = \max\set{\card{f(x)}: x \in [a,b]}$ and metric $\norm{f - g}_{\infty} = \max\set{\card{f(x) - g(x)}: x \in [a,b]}$. $\mathcal C([a,b], \mathbb R)$ is a complete, normed vector space.
It holds that $\overline{K_1(0)}$ is not compact in $\mathcal C([a,b], \mathbb R)$ (i.e. $V$, for every infinite-dimensional vector space).

Again: do not remember \enquote{compactness} not as closed and bounded, as this only holds in the finite-dimensional case.

In the last proof, we have shown: If a sequence $(x_n)_{n \in \mathbb N}$ with $x_n \in X$ and $d(x_n, x_m) \geq 1$ (or $\geq \varepsilon$) $\forall n \neq m \implies X$ is not compact.

\index{Total boundedness}
\begin{definition}
  $X$ is called totally bounded, if for every $\varepsilon > 0$, finitely many points $X_1^\varepsilon$, $X_2^\varepsilon$, \dots, $X_{N(\varepsilon)}^\varepsilon$ such that $X \subseteq \bigcup_{i=1}^{N(\varepsilon)} K_{\varepsilon}(X_i^\varepsilon)$.

  Hence, for every $x \in X$, there exists some $X_j^\varepsilon$ such that $d(X, X_j^\varepsilon) < \varepsilon$.
\end{definition}

\begin{remark}[For the practicals]
  Let $X$ be totally bounded, then there does not exist some sequence $(x_n)_{n \in \mathbb N}$ with $d(x_n, x_m) \geq \varepsilon \forall n \neq m$. It holds, that $X$ is compact if and only if $X$ is totally bounded and complete.
\end{remark}

\begin{theorem} % Satz 6
  \label{satz6}
  Let $f: X \to Y$ be continuous. Let $X$ be compact. Then image $f(X) \subseteq Y$ is also compact.
\end{theorem}

Be aware, that this proof is a common exam question and students often begin with the wrong order.

\begin{proof}
  Let $(y_n)_{n \in \mathbb N}$ be an arbitrary sequence in $f(X)$. Show that $(y_n)_{n\in\mathbb N}$ has a convergent subsequence.
  Because $y_n \in f(X)$, there exists at least one $x_n$ with $y_n = f(x_n)$.
  Then $(x_n)_{n \in \mathbb N}$ is a sequence in $X$, $X$ is compact, hence there exists a subsequence $(x_{n_k})_{k \in \mathbb N}$ with $\lim_{k\to\infty} x_{n_k} = \hat{x} \in X$. Because $f$ is continuous, it holds that $\lim_{k\to\infty} f(x_{n_k}) = \lim_{k\to\infty} y_{n_k} = f(\hat{x}) \eqqcolon \hat{y}$.
  So $(y_n)_{n\in\mathbb N}$ has a convergent subsequence. Hence $f(X) \subseteq Y$ is compact.
\end{proof}

\begin{theorem}[Conclusion] % Satz 7
  \label{satz7}
  Let $X$ be compact, $f: X \to \mathbb R$ continuous on $X$.
  Then there exists $\underline{x}$ and $\overline{x} \in X$, such that
  \[ f(\underline{x}) \leq f(x) \leq f(\overline{x}) \qquad \forall x \in X \]
  Hence, $f$ has a maximum and a minimum.
\end{theorem}

\begin{proof}
  $f(X) \subseteq \mathbb R$ is compact (Theorem~\ref{satz6}), hence $f(X)$ is bounded and complete, hence closed in $\mathbb R$.
  There exists $\xi \in \mathbb R$ with $\xi = \sup{f(X)}$, because $f(X)$ is complete and $\xi$ is a contact point of $f(X)$, it holds that $\xi \in f(X)$, hence $\exists \overline{x} \in X: \xi = f(\overline{x})$. Furthermore, $\xi$ is an upper bound of $f(X) \to f(x) \leq \xi = f(\overline{x}) \forall x \in X$.

  For $\underline{x}$, it works the same way.
\end{proof}

\begin{theorem} % Satz 8
  \label{satz8}
  Let $f: X \to Y$ is continuous on $X$ and $X$ is compact. Then $f$ is uniformly continuous on $X$.
\end{theorem}

\begin{proof}[Indirect proof]
  Assume $X$ is compact, $f: X \to Y$ is continuous, but not uniformly continuous. Uniform continuity:
  \[ \forall \varepsilon > 0 \exists \delta > 0: \forall x, y \in X: d_X(x,y) < \delta \implies d_Y(f(x), f(y)) < \varepsilon \]
  Not uniformly continuous:
  \[ \exists \varepsilon > 0 \forall \delta_n = \frac1n (n \in \mathbb N) \exists x_n, y_n \in X: d_X(x_n, y_n) < \frac1n \land d_Y(f(x_n), f(y_n)) \geq \varepsilon \]
  Now choose some $(x_n)$ and $(y_n)$. We will use a specific $\varepsilon$ later.
  Because $X$ is compact, there exists a convergent subsequence of $(x_n)_{n \in \mathbb N}$, hence $\lim_{k\to\infty} x_{n_k} = \hat{x}$.
  The sequence $(y_{n_k})_{k \in \mathbb N}$ has a convergent subsequence itself:
  \[ \lim_{l\to\infty} y_{(n_k)_l} = \hat{y} \]
  Because $(x_{n_k})_{n \in \mathbb N}$ is convergent, the subsequence $(x_{(n_k)_l})_{l \in \mathbb N}$ converges towards the same limit $\hat{x}$.
  \[ \tilde{x}_l \coloneqq x_{n_{k_l}} \qquad \tilde{y}_l \coloneqq y_{n_{k_l}} \]
  because $l \leq x_{n_l}$ and
  \[ d_X(\tilde{x}_l, \tilde{y}_l) = d_X(x_{n_{k_l}}, y_{n_{k_l}}) \underbrace{<}_{\text{by assumption}} \frac{1}{n_{k_l}} \leq \frac1l \]

  \begin{claim}
    For $\hat{x} = \lim_{l\to\infty} \tilde{x}_l$ and $\hat{y} = \lim_{l\to\infty} \tilde{y}_l$, it holds that $\hat{x} = \hat{y}$.
    Let $\varepsilon' > 0$ be arbitrary, $l$ large enough such that
    \begin{itemize}
      \item $\frac1l < \frac{\varepsilon'}{3}$
      \item $d_X(\tilde{x}_l, \hat{x}) < \frac{\varepsilon'}{3}$
      \item $d_X(\tilde{y}_l, \hat{y}) < \frac{\varepsilon'}{3}$
    \end{itemize}
  \end{claim}
  Therefore it holds that
  \[
    d_X(\hat x, \hat y) \leq d_X(\hat x, \tilde x_l) + d_X(\tilde x_l, \tilde y_l) + d_X(\tilde y_l, \hat y)
    < \frac{\varepsilon'}{3} + \frac1l + \frac{\varepsilon'}{3} < \varepsilon'
  \]
  Therefore it holds that $d_X(\hat{x}, \hat{y}) = 0$, hence $\hat{x} = \hat{y}$.
  Because $f$ is continuous and $\tilde{x_l} \to \hat{x}$ and $\tilde{y}_l \to \hat{x}$, there exists $l \in \mathbb N$ such that
  \[ d_Y(f(\tilde x_l), f(\hat{x})) < \frac\varepsilon2 \]
  and also
  \[ d_Y(f(\tilde y_l), f(\hat{x})) < \frac\varepsilon2 \]
  where $\varepsilon$ is the epsilon from the very beginning of the proof.

  \[ \implies d_Y(f(\tilde x_l), f(\hat{x})) + d_Y(f(\tilde y_l), f(\hat{x})) < \varepsilon \]
  This contradicts to
  \[ d_Y(f(\tilde{x}_l), f(\tilde{y}_l)) = d_Y(f(x_{n_{k_l}}), f(y_{n_{k_l}})) \geq \varepsilon \]

  Hence, $f$ is uniformly continuous.
\end{proof}

Subsets of $(\mathbb R^n, \norm{\cdot})$ (or $(V, \norm{\cdot})$) as metric spaces.

We consider $\Omega \subseteq V$ where $V$ is a normed vector space.
$(\Omega, d)$ is $d(x, y) = \norm{x - y}$ is a metric space.
\[ K_r^\Omega(x) = \setdef{y \in \Omega}{\norm{y - x} < r} \]
is a sphere with center $x$ and radius $r$ in $\Omega$.
\[ K_r^V(x) = \setdef{y \in V}{\norm{y - x} < r} \]
obvious: $K_r^\Omega(x) = \Omega \cap K_r^V(x)$.

TODO drawing 08

\begin{lemma} % Lemma 10
  Let $O' \subseteq \Omega \subseteq V$.

  Then it holds that $O'$ is open in $\Omega \iff$ there exists $O \subseteq V$ is open in $V$ such that $O' = O \cap \Omega$.
\end{lemma}

\begin{proof}
  \begin{description}
    \item[$\Rightarrow$]
      Let $O' \subseteq \Omega$ be open in $\Omega$ and $x \in O'$ be arbitrary.
      Then there exists $r(x) > 0: x \in K_{r(x)}^\Omega(x) = K_{r(x)}^V(x) \cap \Omega \subseteq O'$.
      Then it holds that
      \[
        O' = \bigcup_{x \in O'} = \set{x} \subseteq \bigcup_{x \in O'} K_{r(x)}^\Omega(x)
        = \left(\bigcup_{x \in O'} (K_{r(x)}^V(x)\right) \cap \Omega)
        = \underbrace{\left(\bigcup_{x \in O'} K_{r(x)}^V(x)\right)}_{= O \subseteq V \text{ is open in } V} \cap \Omega
        \subseteq O'
      \]
      So every $\subseteq$ in this inclusion chain is actually an equality. So $O' = O \cap \Omega$.

    \item[$\Leftarrow$]
      Let $O' = O \cap \Omega$ and $x \in O'$ be chosen arbitrarily.
      Because $x \in O$ and $O$ is open in $V$.
      \[ \exists r > 0: K_r^V(x) \subseteq O \implies \underbrace{K_r^V(x) \cap \Omega}_{= K_r^\Omega(x)} \subseteq O \cap \Omega = O' \]
      So $O'$ is open in $\Omega$.
  \end{description}
\end{proof}

\begin{remark}
  $A' \subseteq \Omega$ is closed in $\Omega \iff \exists A \subseteq V$ closed in $V$ with $A' = A \cap \Omega$.
\end{remark}

\index{Relative topology}
\index{Trace topology}
\index{Subspace topology}
\begin{remark}
  Let $T$ be an arbitrary topological space with topology $\tau$ on $T$ (a system of open sets).
  Furthermore let $\Omega \subseteq T$.

  Then $\Omega$ itself is a topological space with $O' \subseteq \Omega$ is open $\iff \exists O \subset T$ open in $T$ with $O' = O \cap \Omega$.

  Also called \enquote{subspace topology}, \enquote{trace topology} or \enquote{relative topology}.
\end{remark}

Attention!
\[ O' \subseteq \Omega \text{ open in } \Omega \implies O' \text{ open in } V \]
does \emph{not} hold in general.

\begin{example}
  \[ \Omega = [0,1] \cap [0,1) \]
  $K_{\frac12}(p) \cap \Omega$ is open in $\Omega$ but not open in $\mathbb R^2$.
\end{example}

Analogously,
\[ A' \subseteq \Omega \text{ is closed} \implies A' \text{ closed in } V \]
does \emph{not} hold in general.

\begin{remark}
  $K$ is compact in $\Omega \implies K$ is compact in $V$
\end{remark}

Let $(x_n)_{n \in \mathbb N}$ is a sequence in $K$.
Compactness $\implies \exists (x_{n_k})_{k \in \mathbb N}: x_{n_k} \to \hat{x}$ for $k \to \infty$ and $K \subseteq \Omega \subseteq V$.

Then $(x_n)_{n \in \mathbb N}$ also has a convergent subsequence in $V$.

\subsection{Normed vector spaces}

\begin{definition} % Definition 19
  Let $V$ be a vector space and $\norm{\cdot}_1$ and $\norm{\cdot}_2$ are normed on $V$.
  We say, $\norm{\cdot}_1$ is equivalent to norm $\norm{\cdot}_2$, if $0 < m \leq M$ exist such that
  \[ m \norm{v}_1 \leq \norm{v}_2 \leq M \norm{v}_1 \forall v \in V \]
\end{definition}

\begin{remark}
  Equivalence of norms is an equivalence relation.
  \begin{description}
    \item[reflexivity]
      Let $m = M = 1$. TODO
    \item[symmetry]
      \[ m \norm{v}_1 \leq \norm{v}_2 \implies \norm{v}_1 \leq \frac1m \norm{v}_2 \land \norm{v}_2 \leq M \cdot \norm{v}_1 \implies \frac1M \norm{v}_2 \leq \norm{v}_1 \]
      \[ \implies \underbrace{\frac1M}_{m'} \norm{v}_2 \leq \norm{v_1} \leq \underbrace{\frac1m}_{M'} \norm{v}_2 \]
      hence the equivalence relations of norms are symmetrical.
    \item[transitivity]
      Let $\norm{\cdot}_1$ and $\norm{\cdot}_2$ be equivalent.
      Let $\norm{\cdot}_2$ and $\norm{\cdot}_3$ be equivalent.

      \[ m \cdot \norm{v}_1 \leq \norm{v}_2 \leq M \norm{v}_1 \forall v \in V \]
      \[ m' \cdot \norm{v}_2 \leq \norm{v}_3 \leq M' \norm{v}_2 \forall v \in V \]
      \[ \implies m \cdot m' \norm{v}_1 \leq m' \norm{v}_2 \leq \norm{v}_3 \leq M' \norm{v}_2 \leq M \cdot M' \norm{v}_1 \]
  \end{description}
\end{remark}

\dateref{2018/03/20}

Addendum:
\begin{itemize}
  \item Let $(x_n)_{n \in \mathbb N}$ be in $(X, d)$, then it holds that
    \[ \underbrace{x = \lim_{n\to\infty} x_n}_{\text{in } X} \iff \underbrace{\lim_{n\to\infty} d(x_n, x) = 0}_{\text{in } \mathbb R} \]
    \[ (\iff \lim_{n\to\infty} \norm{x_n - x} = 0 \text{ in normed vector spaces } V) \]
  \item Inversed triangle inequality: Let $V$ be a normed vector space. Let $x, y \in V$.
    \[ \norm{x} = \norm{x - y + y} \leq \norm{x - y} + \norm{y} \]
    Hence,
    \[ \norm{x} - \norm{y} \leq \norm{x - y} \]
    By exchanging $x$ and $y$,
    \[ \norm{y} - \norm{x} \leq \norm{x - y} \]
    Hence, it holds that
    \[ \card{\norm x - \norm y} \leq \norm{x - y} \]
  \item Define the map $n: V \to [0, \infty)$ on $(V, \norm{\cdot})$ with $n(x) = \norm{x}$.
    Then $n$ is continuous on $V$ because
    \[ \card{n(x_1) - n(x_2)} = \card{\norm{x_1} - \norm{x_2}} \leq \norm{x_1 - x_2} \]
    Hence, $n$ is Lipschitz continuous with constant $1$.
\end{itemize}

Regarding the equivalence of norms:

\begin{lemma} % Lemma 11
  Let $\norm{\cdot}_1$ and $\norm{\cdot}_2$ be equivalent norms on $V$. Then it holds that
  \begin{enumerate}
    \item $\lim_{n\to\infty} \norm{x_n - x}_1 = 0 \iff \lim_{n\to\infty} \norm{x_n - x}_2 = 0$,
      hence $(x_n)_{n\in\mathbb N}$ is convergent with limit $x$ in regards of $\norm{\cdot}_1$
      $\iff$ $(x_n)_{n\in\mathbb N}$ is convergent with limit $x$ in regards of $\norm{\cdot}_2$.
    \item $O \subseteq V$ is open in regards of $\norm{\cdot}_1 \iff O$ is open in regards of $\norm{\cdot}_2$,
      hence $\tau_1 = \tau_2$ (topologies are equivalent).
    \item $K \subseteq V$ is compact in regards of $\norm{\cdot}_1 \iff K$ is compact in regards of $\norm{\cdot}_2$.
  \end{enumerate}
\end{lemma}

\begin{proof}
  Let $\norm{\cdot}_1$ and $\norm{\cdot}_2$ are equivalent, hence $\exists m, M > 0: m \norm{x}_1 \leq \norm{x}_2 \leq M \norm{x}_1 \forall x \in V$.
  \begin{enumerate}
    \item Let $\varepsilon > 0$ and $\lim_{n\to\infty} \norm{x_n - x}_1 = 0$.
      Choose $N \in \mathbb N$ such that $n \geq N \implies \norm{x_n - x}_1 < \frac{\varepsilon}{M}$. For those $n$ it holds that
      \[ \norm{x_n - x}_2 \leq M \norm{x_n - x}_1 < \frac{\varepsilon}{M} \cdot M = \varepsilon \]
      Hence, $\lim_{n\to\infty} \norm{x_n - x}_2 = 0$.
    \item $K_r^2(x) = \setdef{y \in V}{\norm{y - x}_2 < r}$. For $y \in K_r^2(x)$ it holds that
      \[ m \norm{y - x}_1 \leq \norm{y - x}_2 < r \]
      hence,
      \[ \norm{y - x}_1 < \frac rm \implies y \in K_{\frac rm}^1(x) \]
      hence $K_r^2(x) \subseteq K_{\frac rm}^1(x)$.
      Let $y \in K_{\frac rM}^1(x)$. Then it holds that,
      \[ \norm{y - x}_2 \leq M \norm{y - x}_1 < M \cdot \frac rM = r \]
      hence $y \in K_r^2(x)$. $\implies K_{\frac rM}^1(x) \subseteq K_r^2(x)$.
      Now let $O$ be open in regards of $\norm{\cdot}_2$, hence
      \[ \forall x \in O \exists r > 0: K_r^2(x) \subseteq O \implies K_{\frac rM}^1(x) \subseteq K_r^2(x) \subseteq O \]
      so $O$ is open in regards of $\norm{\cdot}_1 \implies O$ is open in regards of $\norm{\cdot}_2$ analogously.
    \item
      Let $K$ be compact in regards of $\norm{\cdot}_1$ and $(x_n)_{n \in \mathbb N}$ be a sequence in $K$.
      Then there exists a subsequence $(x_{n_k})_{k \in \mathbb N}$ with $\norm{x_{n_k} - x}_1 \to 0$ for $k \to \infty$
      $\xRightarrow{\text{by the first property}} \norm{x_{n_k} - x}_2 \to 0$. Hence $(x_{n_k})_{k \in \mathbb N}$ is also a convergent subsequence in regards of $\norm{\cdot}_2$.
  \end{enumerate}
\end{proof}

\begin{remark}[Proven in the practicals]
  Let $(x_n)_{n\in\mathbb N}$ be a sequence in $\mathbb R^k$
  \[ \norm{x}_{\infty} = \max\setdef{\card{x^i}}{i = 1, \dots, n} \]
  \[ x = \begin{bmatrix} x^1 \\ x^2 \\ \vdots \\ x^k \end{bmatrix} \]
  It holds that $\lim_{n\to\infty} \norm{x_n - x}_{\infty} = 0 \iff \lim_{n\to\infty} \card{x_n^i - x^i} = 0$ for all $i \in \set{1, \dots, k}$.
\end{remark}

\begin{theorem}[Bolzano-Weierstrass theorem in $\mathbb R^k$]
  Let $K \subseteq \mathbb R^k$ be closed and bounded.
  Then $K$ is compact in $(\mathbb R^k, \norm{\cdot}_{\infty})$.
\end{theorem}

\begin{proof}
  Let $\norm{x}_{\infty} \leq M \forall x \in K \iff \card{x^i} \leq M \forall x \in K$ and $i \in \set{1, \dots, k}$.
  Choose $(x_n)_{n \in \mathbb N}$ an arbitrary sequence in $K$ $(x_n^i)_{n \in \mathbb N}$ is a bounded sequence in $\mathbb R$.
  Because $(x_n^1)_{n \in \mathbb N}$ is bounded, there exists a convergent subsequence $\left(x_{n_{l_1}}^1\right)_{l_1 \in \mathbb N}$
  \[ \lim_{l_1 \to \infty} x_{n_{l_1}}^1 = x^1 \]
  Consider $(x_{n_{l_1}}^2)_{l_1 \in \mathbb N}$, a subsequence of a bounded sequence, hence bounded itself.
  By the Bolzano-Weierstrass theorem in $\mathbb R$, there exists a convergent subsequence $(x_{n_{{l_1}_{l_2}}}^2)_{l_2 \in \mathbb N}$ with $\lim_{l_2 \to \infty} x_{n_{{l_1}_{l_2}}}^2 = x^2$.
  Consider $x_{n_{{l_1}_{l_2}}}^1$ as subsequence of $x_{n_{l_1}}^1$ is already convergent, hence $\lim_{l_2 \to \infty} x_{n_{{l_1}_{l_2}}}^1 = x^1$. Furthermore, up to index $i$, it holds that:
  \[ \lim_{l_k \to \infty} x_{n_{{{{l_1}_{l_2}}_{\ldots}}_{l_k}}} = x^i \qquad \text{ for } i = 1, \dots, k \]
  Hence, with $\tilde{x_{l_k}} = x_{n_{{{{l_1}_{l_2}}_{\ldots}}_{l_k}}}$ gives a subsequence of $x_n$, converging by each coordinate. Thus,
  \[ \lim_{l_k \to \infty} \norm{\tilde{x}_{l_k} - x}_{\infty} = 0 \]
  Because $\tilde{x}_{l_n} \in K$ and $K$ be closed, it holds that $x \in K$.
  Hence $K$ is compact.
\end{proof}

\begin{theorem}[Norm equivalence in $\mathbb R^k$] % Satz 10
  In $\mathbb R^k$, all norms are equivalent.
\end{theorem}

\begin{proof}
  We show: Let $\norm{\cdot}$ be an arbitrary norm on $\mathbb R^n$. Then $\norm{\cdot}$ is equivalent to $\norm{\cdot}_{\infty}$. By transitivity of norm equivalence, two arbitrary norms are equivalent to each other.
  \begin{enumerate}
    \item Let $(e_1, e_2, \dots, e_k)$ be the canonical basis in $\mathbb R^k$.
      \[ x = \begin{bmatrix} x^1 \\ \vdots \\ x^k \end{bmatrix} = \sum_{j=1}^k x^j e_j \]
      Furthermore let $M' = \max\set{\norm{e_j}: j = 1, \dots, k}$ with $\norm{e_j} \neq 0$ and $M' > 0$.
      Then it holds that
      \[ \norm{x} = \norm{\sum_{j=1}^k x^j e_j} \leq \sum_{j=1}^k \norm{x^j e_j} = \sum_{j=1}^k \card{x^j} \norm{e_j} \leq M' \sum_{j=1}^k \underbrace{\card{x_j}}_{\leq \norm{x}_\infty} \leq \underbrace{M' \cdot k}_{M} \norm{x}_{\infty} = M \norm{x}_\infty \]
    \item
      We consider $\nu: \mathbb R^k \to [0, \infty)$.
      $\nu(x) = \norm{x}$ as map on $(\mathbb R^k, \norm{\cdot}_{\infty})$.

      \begin{claim}
        $\nu$ is continuous on $(\mathbb R^k, \norm{\cdot}_{\infty})$.
      \end{claim}
      \begin{proof}
        Show that,
        \[ \card{\nu(x) - \nu(y)} = \card{\norm{x} - \norm{y}} \underbrace{\leq}_{\text{inversed triangle ineq.}} \norm{x - y} \underbrace{\leq}_{\text{because of (1)}} M \norm{x - y} \]
        Hence $\nu$ is Lipschitz continuous.
      \end{proof}

      We consider $S_{\infty}^{k-1} = \set{x \in \mathbb R^k}{\norm{x}_{\infty} = 1} = \operatorname{boundary}(K_1^\infty(0)$.
      $S_{\infty}^{k-1}$ is bounded.

      Let $(x_n)_{n \in \mathbb N}$ is a sequence in $S_{\infty}^{k-1}$ with $x = \lim_{n\to\infty} x_n$. Because $n(x) = \norm{x}_{\infty}$ is continuous, it holds that
      \[ \lim_{n\to\infty} \underbrace{\norm{x_n}_{\infty}}_{=1} = \underbrace{\norm{x}}_{=1} \]
      Hence $x \in S_{\infty}^{k-1}$. Hence, $S_{\infty}^{k-1}$ is closed in $(\mathbb R^k, \norm{\cdot}_{\infty})$.
      Hence $S_{\infty}^{k-1}$ is compact in $(\mathbb R^k, \norm{\cdot}_{\infty})$, $\nu: S_{\infty}^{k-1} \to [0, \infty)$, with $S_{\infty}^{k-1}$ compact, is continuous.
      Has a minimum $n$ on $S_{\infty}^{k-1}$. Thus there exists $\overline{x} \in S_{\infty}^{k-1}: \underbrace{m}_{> 0} = \norm{\underbrace{\overline{x}}_{\neq 0}} \leq \norm{x} \forall x \in S_{\infty}^{-1}$.
      Let $x \in \mathbb R^k$ be arbitrary with $x \neq 0$. Then it holds that $\frac{x}{\norm{x}_{\infty}} \in S_{\infty}^{k-1}$ and it holds that
      \[ m \leq \norm{\frac{x}{\norm{x}_{\infty}}} = \frac{1}{\norm{x_{\infty}}} \norm{x} \implies m \norm{x}_{\infty} \leq \norm{x} \]
      Inequality also holds true for $x = 0$.
  \end{enumerate}
\end{proof}

\section{Integral calculus}

\index{Partition of an interval}
\index{Step function}
\begin{definition}
  Let $a < b$ with $a, b \in \mathbb R$. We consider functions of $[a,b]$.
  We call $(x_j)_{j = 0}^n$ a \emph{partition of $[a,b]$} if $a = x_0 < x_1 < x_2 < \dots < x_n = b$.
  $x_j$ decomposes $[a,b]$ in subintervals $(x_{j-1}, x_j)$.
  $\varphi: [a,b] \to \mathbb R$ is called \emph{step function} in $[a,b]$ in regards of partition $(x_j)_{j=0}^n$
  if $\varphi|_{(x_{j-1}, x_j)} = c_j$, so constant for $j=1,\dots,n$.

  \begin{figure}
    \begin{center}
      \includegraphics{img/09_subintervals.pdf}
      \caption{Illustration of a partition}
      \label{img:subintervals}
    \end{center}
  \end{figure}

  $\varphi$ is called \emph{step function} in $[a,b]$ if there exists a partition such that $\varphi$ is a subsequence.
  \[ \tau[a,b] = \set{\varphi: [a,b] \to \mathbb R: \varphi \text{ is subsequence}} \]
  \begin{itemize}
    \item Let $(\xi_i)_{i=0}^m$ be a partition of $[a,b]$ and $(x_j)_{j=0}^n$ is a partition as well.
      Then we call $(\xi_i)_{i=0}^m$ a \emph{refinement} of $[a,b]$ and $(x_j)_{j=1}^n$ as well.
      Then $(\xi_i)_{i=0}^n$ is a refinement of $(x_j)_{j=0}^k$ if
      $\set{x_0, x_1, \dots, x_n} \subseteq \set{\xi_0, \xi_1, \dots, \xi_m}$

      TODO drawing %10_refinement.pdf

      Functions values in boundaries $x_{j-1}$ and $x_j$ do not have any constraints and will be relevant for an integral.
      A $\varphi$ can be a step function in terms of many, various partitions.
  \end{itemize}
\end{definition}

\begin{lemma}
  Let $\varphi \in \tau[a,b]$ be a step function in terms of partition $(x_j)_{j=0}^n$ and let $(x_i)_{i=0}^n$ be a refinement of $(x_j)_{j=0}^n$ in terms of $(x_i)_{i=0}^m$.
\end{lemma}

\begin{proof}
  Refinement: For every $j \in \set{0,\dots,n}$ there exists $i_j \in \set{0,\dots,m}$ such that $X_j = \xi_{i_j}$.
  $i_0 = 0, i_n = m$. $i_{j-1} < i_j$.

  Let $i \in \set{1, \dots, m}$. Then there exists a uniquely determined $j \in \set{1, \dots, n}$ such that $\xi_{i_{j-1}} < \xi_i \leq \xi_j$

  TODO drawing % 11_xi.pdf

  Then it holds that $(\xi_{i-1}, \xi_i) \subseteq \underbrace{(\xi_{i_{j-1}})}_{= (x_{j-1}, x_j)}, \xi_{i_j})$ and $\varphi|_{(\xi_{i-1}, \xi_j)} = c_j = \text{const}$. So $\varphi$ is a subsequence in regards of $(\xi_i)_{i=0}^m$.
\end{proof}

\begin{definition}
  Let $\varphi \in \tau[a,b]$ in terms of partition $(X_j)_{j=0}^n$ with $\varphi|_{(X_{j-1}, X_j)} = c_j$ and $\triangle X_j = X_j - X_{j-1} > 0$ for $g = 1,\dots,n$. Then we define \dots
  \[ \int_a^b \varphi \, dx = \sum_{j=1}^n c_j \triangle x_j \]
  is called \emph{integral of $\varphi$} in terms of partition $(x_j)_{j=0}^n$
\end{definition}

\dateref{2018/03/22}

Step function $\varphi$. $\varphi|_{x_{j-1},x_j} = c_j$
\[ \delta x_j = x_j - x_{j-1} \]
\[ \int_a^b \varphi \, dx = \sum_{j=1}^n c_j \cdot \delta x_j \]

\begin{figure}[!h]
  \begin{center}
    \includegraphics{img/12_integral_of_a_step_function.pdf}
    \caption{Integral of a step function as sum of areas of rectangles}
  \end{center}
\end{figure}

\begin{lemma} % Lemma 2
  \label{lemma2}
  Let $(x_i)_{j=0}^n$ be a partition of $[a,b]$ and $(\xi_i)_{i=0}^m$ be a refinement of $(x_j)_{j=0}^n$.
  Furthermore let $\varphi$ be a subsequence with respect to $(x_j)_{j=0}^n$ (so also with respect to $(\xi_j)_{i=0}^m$).
  Then the integrals of $\varphi$ with respect to $(x_j)_{j=0}^n$ and $(\xi_i)_{i=0}^m$ are equal.
\end{lemma}

\begin{proof}
  There exist indices $i_j$ for $j=0$, $n$ such that $x_j = \xi_{ij}$.
  \[ i_0 = 0 \qquad i_n = m \qquad i_{j-1} < i_j \]
  \[ \delta x_j = x_j - x_{j-1} = \xi_{i_j} - \xi_{i_{j-1}} = \xi_{i_j} - \xi_{i_{j-1}} = \underbrace{\sum_{i=i_{j-1}+1}^{i_j}}_{\text{telescoping sum}} (\xi_i - \xi_{i-1}) = \sum_{i=i_{j-1}+1}^{i_j} \delta \xi_i \]
  \[ \varphi|_{x_{j-1},x_j} = c_j \implies \varphi|_{(\xi_{i-1},\xi_i)} = c_j \text{ for } i = i_{j-1}+1, \dots, i_j \]
  \[ \tilde{c_i} = \varphi|_{(\xi_{i-1},\xi_i)} \]
  \[ \underbrace{\sum_{i=1}^m \tilde{c_i} \delta \xi_i}_{\text{integral of $\varphi$ w.r.t $(\xi_i)_{i=0}^m$}} = \sum_{j=1}^n \sum_{i=i_{j-1}+1}^{i_j} \tilde{c_i} \delta \xi_i = \sum_{j=1}^n c_j \underbrace{\sum_{i=i_{j-1}+1}^{i_j} \delta \xi_i}_{=  x_j} = \sum_{j=1}^n c_j \delta x_j \]
  This is the integral of $\varphi$ with respect to $(x_j)_{j=0}^n$.
\end{proof}

\begin{lemma} % Lemma 3
  \label{lemma3}
  Let $\varphi$ be a step function with respect to $(x_j)_{j=0}^n$ and $(w_i)_{i=0}^L$.
  Then the integrals of $\varphi$ with respect to $(x_j)_{j=0}^n$ and with respect to $(w_l)_{l=0}^L$ equal.
\end{lemma}

\begin{proof}
  Let $\setdef{\xi_i}{i = 1, \dots, m} = \setdef{x_j}{j = 0, \dots, n} \cup \setdef{w_l}{l = 0, \dots, L}$
  with $\xi_0 = a$, $\xi_m = x_n = w_L = b$ and $\xi_{i-1} < \xi_i$ for $i = 1, \dots, m$.
  Then $(\xi_i)_{i=0}^m$ is a refinement of $(x_j)_{j=0}^n$ as well as $(w_l)_{l=0}^L$.
  By Lemma~\ref{lemma2}, the integral of $\varphi$ with respect to $(x_j)_{j=0}^n =$ integral of $\varphi$
  with respect to $(\xi_i)_{i=1}^m =$ integral of $\varphi$ with respect to $(w_l)_{l=0}^L$.
  Here we discard the statement \enquote{with respect to $(x_j)_{j=0}^n$}.
\end{proof}

\begin{lemma} % Lemma 4
  Let $f,g$ be step functions on $[a,b]$. $f,g \in \tau[a,b]$.
  \begin{itemize}
    \item
      for $\alpha, \beta \in \mathbb R$, let $\alpha f + \beta g \in \tau[a,b]$ and
      \[ \int_a^b (\alpha f + \beta g) \, dx = \alpha \int_a^bf \, dx + \beta \int_a^b g \, dx \]
      Hence, the integral is linear on $[a,b]$. $\tau[a,b]$ is a vector space.
    \item $f \leq g$ in $[a,b]$, then $\int_a^b f \, dx \leq \int_a^b g \, dx$ (monotonicity).
    \item $\card{\int_a^b f \, dx} \leq \int_a^b \card{f} \, dx$ ($\card{f(x)}$ s also a step function)
  \end{itemize}
\end{lemma}

\begin{proof}
  \begin{enumerate}
    \item
      Let $f,g \in \tau[a,b]$. Let $(\xi_i)_{i=0}^m$ be a partition such that
      $f|_{(\xi_{i-1},\xi_i)} = c_i$ and $g|_{(\xi_{i-1},\xi_i)} = d_i$.
      Then
      \[
        \int_a^b (\alpha f + \beta g) \, dx = \sum_{i=1}^m (\alpha c_i + \beta d_i) \delta \xi_i
        = \alpha \sum_{i=1}^m c_i \delta \xi_i + \beta \sum_{i=1}^m d_i \delta \xi_i
        = \alpha \int_a^b f \, dx + \beta \int_a^b g \, dx
      \]
      Furthermore,
      \[ (\alpha f + \beta g)|_{(\xi_{i-1},\xi_i)} = \alpha c_i + \beta d_i = \text{ const.} \]
      Thus,
      \[ \alpha f + \beta g \in \tau[a,b] \]
    \item
      Let $h \in \tau[a,b]$ with $h(x) \geq 0 \forall x \in [a,b]$ be a step function
      and $\int_a^b h \, dx = \sum_{i=1}^m \underbrace{h_i}_{\geq 0} \delta \xi_i \geq 0$
      TODO
      Hence, it holds that $0 \leq \int_a^b h \, dx = \int_a^b (g - f) \, dx = \int_a^b g \, dx - \int_a^b f \, dx$.
    \item
      $f \leq \card{f}$, hence $\int_a^b f \, dx \leq \int_a^b \card{f} \, dx$ and also
      $-f \leq \card{f}$, so
      \[ \int_a^b (-f) \, dx = -\int_a^b f \, dx \leq \int_a^b \card{f} \, dx \]
      \[ \implies \card{\int_a^b f \, dx} \leq \int_a^b \card{f} \, dx \]
      It is left to prove: $\card{f} \in \tau[a,b]$ (i.e. $\card{f}$ is a step function)

      Let $f|_{(\xi_{i-1}, \xi_i)} = c_i \implies \card{f}|_{(\xi_{i-1}, \xi_i)} = \card{c_i} = $ constant.
      Hence $\card{f} \in \tau[a,b]$.
  \end{enumerate}
\end{proof}

\index{Characteristic function}
\index{Indicator function}
\begin{definition} % Definition 3
  Let $a \subseteq \mathbb R^k$. We call $\chi_A: \mathbb R^n \to \mathbb R$ with
  \[
    \chi_A(x) = \begin{cases}
      1 & \text{ if } x \in A \\
      0 & \text{ else}
    \end{cases}
  \]
  a \emph{characteristic function} (indicator function) of set $A$.
  Often denoted as $\chi_A = \mathbbm{1}$.
\end{definition}

\begin{remark}
  TODO drawings

  Let $A = (a', b')$ with $a \leq a' < b' \leq b$.
  Then $\chi_{(a',b')} \in \tau[a,b]$. Also for $x \in [a,b]$, it holds that $\chi_{\set{x}} = \tau[a,b]$.
  Therefore every linear combination of characteristic functions of open subintervals ($a',b'$) of $[a,b]$
  as characteristic functions of one-point sets $\chi_{\set{x}}, x \in [a,b]$ a step function on $[a,b]$.
  \[ \sum_{j=1}^n \alpha_j \chi_{(a_j,b_j)} + \sum_{k=1}^m \beta_k \chi_{\set{x_k}} \in \tau[a,b] \]
  On the opposite, $f \in \tau[a,b]$, hence
  \[ f|_{(x_{j-1},x_j)} \underbrace{=}_{j=1,\dots,n} c_j \text{ and } f(x_j) \underbrace{=}_{j=0,\dots,n} d_j \]
  \[ f = \sum_{j=1}^n c_j \chi_{(x_{j-1},x_j)} + \sum_{j=0}^n d_j \chi_{\set{x_j}} = (*) \]
  for $x \in (x_{j-1}, x_j)$ it holds that $\xi_{(x_{j-1},x_j)}(x) = 1$.
  \[ \chi_{(x_{l-1},x_l)}(x) = 0 \text{ for } l \neq j \]
  \[ \chi_{\set{x_l}}(x) = 0 \text{ for } l = 0, \dots, n \]
  i.e. $\sum_{j=1}^n c_l \chi_{(x_{l-1},x_l)}(x) + \sum_{l=0}^n d_j \chi_{\set{x_l}}(x) = c_j \cdot 1 + 0 = c_j$
  hence $(*) = c_j$ on $(x_{j-1}, x_j)$. Therefore $f \in \tau[a,b] \iff f$ is linear combination of characteristic functions
  of open intervals or one-pointed sets.
\end{remark}

\subsection{Regulated functions}

\index{Limit point}
\begin{definition} % Definition 4
  Let $X$ be a metric space $A \subseteq X$ and $x \in X$ is an accumulating point\footnote{An accumulation point has 3 equivalent definitions (sequence, intersection, infinitely many elements in sphere).} of $A$.
  Let $f: A \to \mathbb R$. We say, $f$ has limit $c \in \mathbb R$ in $x$ ($\lim_{\xi\to x} f(\xi) = c$) if
  \[
    \forall \varepsilon > 0 \exists \delta > 0: \forall \xi \in A, \xi \neq x \text{ and } d(\xi, x) < \delta:
    \card{f(\xi) - c} < \varepsilon
  \]
\end{definition}

\begin{remark}
  $x \in A$ and $c = f(x) \implies f$ is continuous in $x$.

  We usually consider $A = [a,b] \subseteq \mathbb R$, $x \in [a,b]$.

  It is possible, that $f$ in $x$ has a limit, $x \in A$ and $c = \lim_{\xi\to x} f(\xi) \neq f(x)$.
\end{remark}

TODO drawing

\begin{definition}
  Now let $A \subseteq \mathbb R$ and $x$ is a accumulation point of $A$.
  Let $f: A \to \mathbb R$ be given. We say $f$ has a right-sided limit $c$ in $x$
  with $c = \lim_{\xi \to x^+} f(\xi) = c$ if $\forall \varepsilon > 0 \exists \delta > 0: \forall \xi \in A, \xi > x$
  \[ \land  \card{\xi - x} = \xi - x < \delta \implies \card{f(\xi) - c} < \varepsilon \]

  The left-sided limit follows analogously.
  \[ c = \lim_{\xi \to x^-} f(\xi) \]

  \[ c = \lim_{\xi \to x^+} f(\xi) \qquad d = \lim_{\xi \to x^-} f(\xi) \]

  TODO drawing
\end{definition}

\begin{lemma}[Sequence criterion for limits of functions] % Lemma 5
  \label{lemma5}
  Let $f: A \subseteq X \to \mathbb R$ be given. $x$ is an accumulation point of $A$.
  Then it holds that
  \[ \lim_{\xi \to x} f(\xi) = c \iff \forall (\xi_n)_{n\in\mathbb N}: \xi_n \in A, \xi_n \neq x \text{ and } \lim_{n\to\infty} \xi_n = x \text{ it holds that } \lim_{n\to\infty} f(\xi_n) = c \]
  For one-sided limits $A \subseteq \mathbb R$ it holds that
  \[ c = \lim_{\xi\to x^{+}} f(\xi) \iff \forall (\xi_n)_{n\in\mathbb N}: \xi \in A \qquad \xi_n > x \text{ with } \lim_{n\to\infty} \xi_n = x \text{ it holds that } \lim_{n\to\infty} f(\xi_n) = c \]
\end{lemma}

\begin{remark}
  Attention! We, therefore, use two different definitions of limits.
\end{remark}

\begin{lemma}[Cauchy criterion of limits of functions] % Lemma 6
  \label{cauchy-crit}
  Let $f: A \subseteq X \to \mathbb R$. Let $x$ be an accumulation point of $A$.
  Let $X$ be a metric space. Then it holds that
  $f$ has a limit in $x$ if and only if
  \[ \forall \varepsilon > 0 \exists \delta > 0: \forall \xi, \eta \in A: \xi \neq x_i: \eta \neq x: \]
  with $d(\xi, x) < \delta$ and $d(\eta, x) < \delta$ it holds that $\card{f(\xi) - f(\eta)} < \varepsilon$.
  Analogously for one-sided limits with $A \subseteq \mathbb R$.
  Additionally, we need the constraint that $\xi > X$ and $\eta > x$ for $\lim_{\xi \to x^+} f(\xi)$ or equivalently,
  $\xi < x$ and $\eta < x$ for $\lim_{\xi \to x^-} f(\xi)$.

  TODO normalize and visualize equivalent statements for left-sided and right-sided limit (using Ring's notes)
\end{lemma}

\begin{proof}
  \begin{description}
    \item[$\Leftarrow$] 
      Let $c = \lim_{\xi\to x} f(\xi)$ and let $\varepsilon > 0$ be chosen arbitrarily.
      Then there exists $\delta > 0$ such that $d(\xi,x) < \delta$ and $\xi \neq x$
      \[ \implies \card{f(\xi) - c} < \frac\varepsilon2 \]
      For $\xi, \eta$: $d(\xi, x) < \delta$ and $d(\eta, x) < \delta$ with $\xi, \eta \neq x$
      is therefore
      \[ \card{f(\xi) - f(\eta)} = \card{f(\xi) - c + c - f(\eta)} \leq \card{f(\xi) - c} + \card{f(\eta) - c} < \frac\varepsilon2 + \frac\varepsilon = \varepsilon \]
    \item[$\Rightarrow$]
      Assume the Cauchy criterion holds. We show that
      \begin{enumerate}
        \item for every sequence $(\xi_n)_{n\in\mathbb N}$, $\xi_n \in A \setminus \set{x}$ with $\lim_{n\to\infty} \xi_n = x$ it holds that
        $(f(\xi_n))_{n\in\mathbb N}$ is a Cauchy sequence in $\mathbb R$ and therefore convergent in $\mathbb R$.
        \item all Cauchy sequences have the \emph{same} limit $c$.
      \end{enumerate}
      We prove (1.)

      Let $(\xi_n)_{n\in\mathbb N}$ be as above. Let $\varepsilon > 0$ be arbitrary.
      and $N_{\varepsilon}$ large enough such that $\forall n \in N_{\varepsilon}$ it holds that
      $d(\xi_n, x) < \delta$ ($\delta$ chosen appropriately to $\varepsilon$ according to the Cauchy criterion).

      By the Cauchy criterion, $\card{f(\xi_n) - f(\xi_m)} < \varepsilon$ for all $m,n \geq N_{\varepsilon}$.
      Therefore $(f(\xi_n))_{n\in\mathbb N}$ is a Cauchy sequence in $\mathbb R$.
      If $\mathbb R$ is complete, then there exists $c = \lim_{n\to\infty} f(\xi_n)$. QED.

      We prove (2.)

      Let $\xi_n \to x$ as above and $\xi_n' \to x$ as above and $c = \lim_{n\to\infty} f(\xi_n)$ as well as $c' = \lim_{n\to\infty} f(\xi_n')$. Let $\varepsilon > 0$ be arbitrary, $N_{\varepsilon}$ such that $n \geq N_{\varepsilon} \implies \card{f(\xi_n) - c} < \frac{\varepsilon}{3}$ and $N_{\varepsilon}' \in \mathbb N$ such that $n \geq N_{\varepsilon}' \implies \card{f(\xi_n') - c'} < \frac\varepsilon3$.

      Furthermore choose $\delta > 0$ such that
      \[ d(\xi, x) < \delta \land d(\eta, x) < \delta \implies \card{f(\xi) - f(\eta)} < \frac{\varepsilon}{3} \]
      (because of the Cauchy criterion).
      $M_{\varepsilon}$ such that 
      \[ n \geq M_{\varepsilon} \implies d(\xi_n, x) < \delta \land M_{\varepsilon}': n \geq M_{\varepsilon}' \implies d(\xi_n', x) < \delta \]
      Let $n \geq \max\set{N_{\varepsilon}, N_{\varepsilon}', M_{\varepsilon}, M'_{\varepsilon}}$.

      \dateref{2018/04/10}

      Then it holds that
      \[ \card{c - c'} \leq \underbrace{\card{c - f(\xi_n)}}_{< \frac\varepsilon3} + \underbrace{\card{f(\xi_n) - f(\xi'_n)}}_{< \frac\varepsilon3} + \underbrace{\card{f(\xi'_n) - c'}}_{< \frac\varepsilon3} \qquad \forall \varepsilon > 0 \]
      Hence, $c = c'$.
      We have shown that $\exists c \in \mathbb R: \forall (\xi_n)_{n \in \mathbb N}$ with $\lim_{n\to\infty} \xi_n = x$ it holds that
      $\lim_{n\to\infty} f(\xi_n) = c$. So $\lim_{\xi\to\infty} f(\xi) = c$ because of Lemma~\ref{lemma5}. QED.
  \end{description}
\end{proof}

\index{Regulated function}
\begin{definition}[Regulated function] % Definition 6
  Let $a < b$, $f: [a,b] \to \mathbb R$. We call $f$ a \emph{regulated function on $[a,b]$} if
  \begin{enumerate}
    \item $\forall x \in (a,b)$, $f$ in $x$ has a right-sided and a left-sided limit.
    \item in $x = a$, $f$ has a right-sided limit.
    \item in $x = b$, $f$ has a left-sided limit.
  \end{enumerate}

  \[ \mathcal R[a,b] = \setdef{f: [a,b] \to \mathbb R}{f \text{ is a regulated function}} \]
\end{definition}

\begin{definition}[Equivalent definition]
  \begin{enumerate}
    \item $\forall x \in [a,b)$, $f$ has a right-sided limit in $x$
    \item $\forall x \in (a,b]$, $f$ has a left-sided limit in $x$
  \end{enumerate}
\end{definition}

\begin{example}
  Let $f$ be continuous in $[a,b]$.
  Let $\varphi \in \tau[a,b]$ be a regulated function. Then $\varphi \in \mathcal R[a,b]$.

  Rationale:

  Let $x_0 = a < x_1 < \dots < x_n = b$ and $\varphi|_{(x_{j-1}, x_j)} = c_j$.

  Let $x \in [a,b]$ be chosen arbitrarily.

  \begin{description}
    \item[Case 1]
      Let $x \in (x_{j-1}, x_j)$ for some $j \in \set{1, \dots, n}$
      \[ \implies \lim_{\xi\to x} \varphi(\xi) = c_j \]
      Choose $\delta$ small enough such that $(x - \delta, x + \delta) \subseteq (x_{j-1}, x_j)$.
      $\forall \xi$ with $\xi \in (x - \delta, x + \delta)$ it holds that
      \[ \card{\varphi(\xi) - c_j} = 0 \]
    \item[Case 2]
      Let $x = x_j$ for $j = 1, \dots, n-1$.
      \[ \implies \lim_{\xi \to x_j^+} \varphi(\xi) = c_{j+1} \]
      \[ \lim_{\xi \to x_j^-} \varphi(\xi) = c_j \]
      Compare with Figure~\ref{img:regf}.
      \begin{figure}
        \begin{center}
          \includegraphics{img/15_regulated_function.pdf}
          \caption{Regulated function}
          \label{img:regf}
        \end{center}
      \end{figure}
    \item[Case 3]
      Let $x = x_0 = a \implies \lim_{\xi \to a^+} \varphi(\xi) = c_1$.
      \[ x = x_n = b \implies \lim_{\xi \to b^-} \varphi(\xi) = c_n \]
  \end{description}

  Let $f: [a,b] \to \mathbb R$ be monotonically increasing oder monotonically decreasing.
  Then $f \in \mathcal R[a,b]$. The proof will be done in the practicals.
\end{example}

\begin{definition}[Boundedness] % Definition 7
  Let $X \neq \emptyset$ be a set.
  $f: X \to \mathbb K$ with $\mathbb K = \mathbb R$ or $\mathbb K = \mathbb C$.
  We say: $f$ is bounded on $X$, if $f(X) \subseteq \mathbb K$ is a bounded set in $\mathbb K$.
  Hence, $\exists m \geq 0: \card{f(x)} \leq m \forall x \in X$.
  We let,
  \[ \mathcal B(X) = \setdef{f: X \to \mathbb K}{f \text{ is bounded}} \]
  $\mathcal B(X)$ has vector space structure.
  $f, g \in \mathcal B(X), \lambda \in \mathbb K$.
  \[ (f + g)(x) = f(x) + g(x) \]
  \[ (\lambda \cdot f)(x) = \lambda \cdot f(x) \]
  $f + g \in \mathcal B(X)$ and $\lambda f \in \mathcal B(X)$.
  Let $\card{f(x)} \leq m \forall x \in X$ and $\card{g(x)} \leq m' \forall x \in X$.
  Then it holds that
  \[ \card{(f + g)(x)} = \card{f(x) + g(x)} \leq \card{f(x)} + \card{g(x)} \leq m + m' \]
\end{definition}

\begin{remark}
  It is very interesting, that $X$ does not require any kind of algebraic structure.
\end{remark}

We let
\[
  \norm{f}_{\infty}
  = \sup\underbrace{\setdef{\card{f(x)}}{x \in X}}_{\text{bounded in } \mathbb R}
  = \min\setdef{m \geq 0}{\card{f(x)} \leq m \forall x \in X}
\]
Some work is required to show that $\norm{\cdot}_{\infty}$ is a norm on $\mathcal B(X)$.

Hence, $(\mathcal B(X), \norm{\cdot}_{\infty})$ is a normed vector space.
Convergence in $\mathcal B(X)$: It holds that $f_n \to f$ in $(\mathcal B(X), \norm{\cdot}_{\infty})$
if and only if $\forall \varepsilon > 0 \exists N \in \mathbb N: n \geq N \implies \norm{f_n - f}_{\infty} < \varepsilon$.

\[ \norm{f_n - f}_{\infty} < \varepsilon \iff \sup\set{\card{f_n(x) - f(x)}: x \in X} \]
\[ \iff \card{f_n(x) - f(x)} \leq \varepsilon \forall x \in X \]
Hence, $f_n \to f$ in $(\mathcal B(X), \norm{\cdot}_{\infty}) \iff \forall \varepsilon > 0 \exists N \in \mathbb N: n \geq N \implies \card{f_n(x) - f(x)} \leq \varepsilon \forall x \in X$.
We say \enquote{$f_n$ converges \emph{uniformly} to $f$ on $X$}.

\begin{theorem}[Approximation theorem for regulated function]
  Let $f: [a, b] \to \mathbb R$. Then it holds that $f \in \mathcal R[a,b] \iff \forall \varepsilon > 0$ there exists some step function $\varphi \in \tau[a,b]$ such that $\card{\varphi(x) - f(x)} < \varepsilon \forall x \in [a,b]$ ($\norm{\varphi - f}_{\infty} < \varepsilon$).

  Especially $\varepsilon_n = \frac1n$ and $\varphi_n$ as above.
  Then it holds that $\norm{\varphi_n - f}_{\infty} < \frac1n$, hence $f = \lim_{n\to\infty} \varphi_n$ uniformly on $[a,b]$.
\end{theorem}

\begin{proof}
  Direction $\Rightarrow$. Let $f \in \mathcal R[a,b]$.

  Proof by contradiction. We negate our hypothesis:
  \begin{align}
    \exists \varepsilon > 0: \forall \varphi \in \tau[a,b] \exists x \in [a,b]: \card{\varphi(x) - f(x)} \geq \varepsilon
    \label{neghypo}
  \end{align}
  Assume \eqref{neghypo} holds for $f \in [a,b]$.
  We construct nested intervals $[a_n, b_n]$ with $[a_{n+1}, b_{n+1}] \subseteq [a_n, b_n]$
  and $b_{n+1} - a_{n+1} = \frac12 (b_n - a_n)$ and \eqref{neghypo} holds on $[a_n, b_n] \forall n \in \mathbb N$.
  Hence $\forall \varphi \in \tau[a_n, b_n] \exists x \in [a_n, b_n]$ such that $\card{\varphi(x) - f(x)} \geq \varepsilon$.
  This is what we want to show.

  Let $a_0 = a$ and $b_0 = b$. Then \eqref{neghypo} holds on $[a_0, b_0]$ by assumption.
  $n \to n+1$: Construction of $[a_{n+1}, b_{n+1}]$. Let $m_n = \frac12(a_n + b_n)$.
  We need to prove: \eqref{neghypo} holds either on $[a_n, m_n]$ or on $[m_n, b_n]$.

  Because if the opposite of \eqref{neghypo} holds on $[a_n, m_n]$ as well as $[m_n, b_n]$,
  then there exists $\varphi_1^n \in \tau[a_n, m_n]$ with $\card{\varphi_n^1(x) - f(x)} < \varepsilon \forall x \in [a_n, m_n]$
  and if the opposite of \eqref{neghypo} holds on $[m_n, b_n]$:
  \[ \exists \varphi_n^2 \in \tau[m_n, b_n]: \card{\varphi_n^2(x) - f(x)} < \varepsilon \forall x \in [m_n, b_n] \]
  Let
  \[
    \varphi^n(x) = \begin{cases}
      \varphi_n^1(x) & \text{ if } x \in [a_n, m_n] \\
      \varphi_n^2(x) & \text{ if } x \in (m_n, b_n]
    \end{cases}
  \]
  Then $\varphi^n$ is piecewise constant, hence $\varphi^n \in \tau[a_n, b_n]$ and it holds that
  \[
    \card{\varphi^n(x) - f(x)} = \begin{cases}
      \underbrace{\card{\varphi_1^n(x) - f(x)}}_{< \varepsilon}  & \text{ for } x \in [a_n, m_n] \\
      \underbrace{\card{\varphi_2^n(x) - f(x)}}_{< \varepsilon}  & \text{ for } x \in [m_n, b_n]
    \end{cases} < \varepsilon
  \]
  This contradicts with \eqref{neghypo} on $[a_n, b_n]$.

  Hence: \eqref{neghypo} holds on $[a_n, m_n]$ or on $[m_n, b_n]$.

  Choose $[a_{n+1}, b_{n+1}]$ as one of the subintervals in which \eqref{neghypo} holds.
\end{proof}

% TODO: fix structure
Let $X \in \bigcap_{n \in \mathbb N} [a_n, b_n]$ (by completeness of $\mathbb R$).
\begin{enumerate}
  \item Let $x \in (a, b)$. Let $\varepsilon$ as above such that \eqref{neghypo} holds on every interval $[a_n, b_n]$.
    Let $c_+ = \lim_{\xi \to x^+} f(\xi)$ and $c_- = \lim_{\xi \to x^-} f(\xi)$ (possible, because $f \in \mathcal R[a,b]$).

    Limes property: $\exists \delta > 0: \card{\xi - x} < \delta$ and $\xi < x$, then $\card{f(\xi) - c_-} < \varepsilon$
    and $\card{\xi - x} < \delta$ and $x < \delta$ then $\card{f(\xi) - c_+} < \varepsilon$.

    Additionally, choose $\delta$ sufficiently small enough such that
    $(x - \delta, x + \delta) \subseteq [a, b]$.
    Let
    \[
      \hat\varphi(\xi) = \begin{cases}
        0 & \text{ for } \xi \in [a,b] \setminus (x - \delta, x + \delta) \\
        c_- & \text{ for } \xi \in (x - \delta, x) \\
        c_+ & \text{ for } \xi \in (x, x + \delta) \\
        f(x) & \text{ for } \xi = x
      \end{cases}
    \]

    \begin{figure}[!h]
      \begin{center}
        \includegraphics{img/16_construction.pdf} % TODO
      \end{center}
    \end{figure}

    $\hat{\varphi} \in \tau[a,b]$ and it holds that
    \[
      \forall \xi \in (x - \delta, x + \delta):
      \card{\hat\varphi(\xi) - f(\xi)} = \begin{cases}
        \underbrace{\card{c_- - f(\xi)}}_{< \varepsilon} & \text{ for } \delta \in (x - \delta, x) \\
        \underbrace{\card{f(x) - f(x)}}_{= 0} & \text{ for } \xi = x \\
        \underbrace{\card{c_+ - f(\xi)}}_{< \varepsilon} & \text{ for } \xi \in (x, x + \delta)
      \end{cases} < \varepsilon
    \]
    Now let $N$ be sufficiently large enough such that $[a_N, b_N] \subseteq (x - \delta, x + \delta)$
    (possible because $([a_n, b_n])_{n \in \mathbb N}$ gives nested intervals tightening on $x$).
    Then it holds on $[a_N, b_N]$ that:
    \[ \hat\varphi|_{[a_N, b_N]} \in \tau[a_N, b_N] \]
    and $\forall \xi \in [a_N, b_N] \subseteq (x - \delta, x + \delta)$ it holds that
    $\card{\hat{\varphi}(\xi) - f(\xi)} < \varepsilon$. This contradicts with \eqref{neghypo} on $[a_N, b_N]$.

    We also need to cover the special cases $x = a$ and $x = b$.
    But this works analogously with one-sided limits.

    Direction $\Leftarrow$:
    Let $f = \lim_{n\to\infty} \varphi_n$ uniform on $[a,b]$.
    Show that $\forall x \in [a,b)$ there exists a right-sided limit of $f$ in $x$.

    Let $\varepsilon > 0$ be arbitrary. $N \in \mathbb N$ sufficiently large such that
    $\card{f(\xi) - \varphi_N(\xi)} < \frac\varepsilon2 \forall \xi \in [a,b]$.
    $\varphi_N$ is piecewise constant. Choose $\delta > 0$ such that
    $\varphi_N|_{(x, x+\delta)} = c$.
    Now let $\xi, \eta \in (x, x+\delta)$ be chosen arbitrarily.
    Then it holds that
    \[
      \card{f(\xi) - f(\eta)} \leq \card{f(\xi) - \underbrace{c}_{= \varphi_N(\xi)}} + \card{\underbrace{c}_{= \varphi_N(\eta)} - f(\eta)}
    \] \[
      = \card{\underbrace{f(\xi) - \varphi_N(\xi)}_{< \frac\varepsilon2}} + \card{\underbrace{\varphi_N(\eta) - f(\eta)}_{< \frac\varepsilon2}} < \varepsilon
    \]
    Therefore $f$ has a right-sided limit in $x$ by the Cauchy criterion.
    $f$ has left-sided limit in every point $x \in (a, b]$ analogously.
\end{enumerate}

\begin{corollary}
  Every regulated function $f \in \mathcal R[a,b]$ is bounded.
  Let $\varphi \in \tau[a,b]$ with $\norm{f - \varphi}_{\infty} < 1$.
  $\varphi$ is bounded, hence $\exists m \in [0, \infty)$:
  $\card{\varphi(x)} \leq m \forall x \in [a,b]$.
  Then it holds that $\card{f(x)} \leq \card{f(x) - \varphi(x)} + \card{\varphi(x)} < 1 + m \forall x \in [a,b]$,
  hence $f \in \mathcal B[a,b]$.
  \[ \mathcal R[a,b] \subseteq \mathcal B[a,b] \]
\end{corollary}

\begin{corollary}
  Let $f \in \mathcal R[a,b] \iff f = \sum_{j=0}^\infty \psi_j$ with $\psi_j \in \tau[a,b]$ and the series converges uniformly on $[a,b]$.
\end{corollary}

\begin{proof}
  Direction $\impliedby$.

  Let $f = \sum_{j=0}^\infty \psi_j$ with uniform convergence.
  Let $\varphi_n = \sum_{j=0}^\infty \psi_j \in \tau[a,b]$ and $f = \lim_{n\to\infty} \phi_n$ uniform on $[a,b] \underbrace{\implies}_{\text{Satz~1?!}} f \in \mathcal R[a,b]$. % TODO Satz 1

  Direction $\implies$.

  Let $f \in \mathcal R[a,b]$ and $f = \lim_{n\to\infty} \varphi_n$ with $\varphi_n \in \tau[a,b]$ (by Satz~1?!).

  \begin{align*}
    \psi_0 &= \varphi_0 \\
    \psi_j &= \varphi_j - \varphi_{j-1} \qquad \text{ for } j \geq 1 \\
    \sum_{j=0}^n \psi_j &= \varphi_0 + \sum_{j=1}^n (\varphi_j - \varphi_{j-1}) = \varphi_0 + \sum_{j=1}^n \varphi_j - \sum_{j=0}^{n-1} \varphi_j = \varphi_n
  \end{align*}
  converges uniformly to $f$.
\end{proof}

\section{Integration of regulated functions}

\begin{definition}[Definition with a theorem] % Definition and Saetzchen 8
  Let $f \in \mathcal R[a,b]$ and $\varphi_n \in \tau[a,b]$ with $f = \lim_{n\to\infty} \varphi_n$ is uniform on $[a,b]$.
  We let
  \[ \int_a^b f \, dx = \lim_{n\to\infty} \int_a^b \varphi_n \, dx \]
  for the integral of $f$ on $[a,b]$.

  Theorem: This limit (on the right-hand side) always exists and is independent of the particular choice of the approximating sequence.
\end{definition}

\begin{proof}
  $\varphi_n$ is chosen as above.
  \[ i_n = \int_a^b \varphi_n \, dx \]
  Show: $i_n$ is cauchy sequence in $\mathbb R$.

  \dateref{2018/04/12}

  Let $\varepsilon > 0$ be chosen arbitrary. Choose $N \in \mathbb N$ such that
  \[ n \geq N \implies \norm{f - \varphi_n}_{\infty} < \frac{\varepsilon}{2 (b - a)} \]
  For $n,m \geq N$ it holds for $x \in [a,b]$ that
  \[ \card{\varphi_n(x) - \varphi_m(x)} \leq \card{\varphi_n(x) - f(x)} + \card{f(x) - \varphi_m(x)} \]
  \[
    \leq \norm{\varphi_n - f}_\infty + \norm{f - \varphi_m}_{\infty}
    < \frac{\varepsilon}{2 (b - a)} + \frac{\varepsilon}{2 (b - a)} = \frac{\varepsilon}{b - a}
  \]
  $\card{\varphi_n - \varphi_m}$ is a step function.
  \[ \card{\varphi_n - \varphi_m} \leq \frac{\varepsilon}{b - a} \cdot \underbrace{\chi_{[a,b]}}_{\in \tau[a,b]} \]

  Integral for subsequence is monotonous:
  \[
    \card{i_n - i_m} = \card{\int_a^b \varphi_n \, dx - \int_a^b \varphi_m \, dx}
    = \card{\int_a^b (\varphi_n - \varphi_m) \, dx} \leq \int_a^b \card{\varphi_n - \varphi_m} \, dx
  \] \[
    \underbrace{<}_{\text{by monotonicity}}
    \int_a^b \frac{\varepsilon}{b - a} \cdot \chi_{[a,b]} \, dx
    = \frac{\varepsilon}{b - a} \underbrace{\int_a^b \chi_{[a,b]}}_{1 \cdot (b - a)} \, dx
    = \varepsilon
  \]
  So $(i_n)_{n \in \mathbb N}$ is a Cauchy sequence.
  $\mathbb R$ is complete, hence $i = \lim_{n\to\infty} i_n$ exists.

  Uniqueness: (dt. \foreignlanguage{german}{mithilfe des Reissverschlussprinzips})

  Let $(\varphi_n)_{n \in \mathbb N}$, $(\Phi_n)_{n \in \mathbb N}$ be two sequences of step functions,
  converging uniformly towards $f$.
  \[
    i_n = \int_a^b \varphi_n \, dx \quad \text{ and } \quad j_n = \int_a^b \Phi_n \, dx
  \] \[
    i = \lim_{n\to\infty} i_n \qquad j = \lim_{n\to\infty} j_n
  \]
  Show that $i = j$.

  Now we construct a sequence $(\mu_n)_{n \in \mathbb N}$ of step functions.
  \[ \underbrace{(\varphi_1, \Phi_1, \varphi_2, \Phi_2, \dots)}_{(\mu_n)_{n \in \mathbb N}} \]
  $\mu_n$ is a sequence of step functions converging uniformly towards $f$ (the proof is left as an exercise to the reader).

  Because of part~1 of the proof:
  \[ m_n = \int_a^b \mu_n \, dx \text{ converges with limit } m \]
  $(i_n)_{n\in\mathbb N}$ as well as $(j_n)_{n \in \mathbb N}$ are subsequences of $(m_n)_{n \in \mathbb N}$.
  Hence it holds that $i = \lim_{n\to\infty} i_n = m = \lim_{n\to\infty} j_n = j$.
\end{proof}

\begin{theorem}[Elementary properties of an integral]
  Let $f, g \in \mathcal R[a,b]$, $\lambda, \mu \in \mathbb R$.
  Then it holds that
  \begin{description}
    \item[Linearity]
      \[ \lambda f + \mu g \in \mathcal R[a,b] \text{ and } \int_a^b (\lambda f + \mu g) \, dx = \lambda \int_a^b f \, dx + \mu \int_a^b g \, dx \]
    \item[Monotonicity]
      If $f(x) \leq g(x) \forall x \in [a,b]$ ($f \leq g$) it holds that
      \[ \int_a^b f \, dx \leq \int_a^b g \, dx \]
    \item[Boundedness]
      $\card{f} \in \mathcal R[a,b]$  and
      \[ \card{\int_a^b f \, dx} \leq \int_a^b \card{f} \, dx \]
  \end{description}
\end{theorem}
\begin{proof}
  We prove linearity.

  Let $x \in [a,b)$ and $c_+ = \lim_{\xi \to x_+} f(\xi)$ as well as $d_+ = \lim_{\xi \to x_+} g(\xi)$
  ($f,g \in \mathcal R[a,b]$). Then it holds that
  \[
    \lim_{\xi \to x^+} (\lambda f(\xi) + \mu g(\xi))
      = \lambda \lim_{\xi \to x^+} f(\xi) + \mu \lim_{\xi \to x^+} g(\xi)
      = \lambda c_+ + \mu d_+
  \]
  exists. Analogously for the left side, hence $\lambda f + \mu g \in \mathcal R[a,b]$.

  Let $\varphi_n, \Phi_n \in \tau[a,b]$ with $\varphi_n \to f$ and $\Phi_n \to g$ is uniform on $[a,b]$.
  Hence $\lambda \varphi_n + \mu \Phi_n \to \lambda f + \mu g$ is continuous on $[a,b]$.

  Proof of this:

  Let $\varepsilon > 0$ be arbitrary,
  $N$ such that $n \geq N \implies \norm{\varphi_n - f}_{\infty} < \frac{\varepsilon}{2(\card{\lambda} + 1)}$
  and $M$ such that $n \geq M \implies \norm{\Phi_n - g}_{\infty} < \frac{\varepsilon}{2(\card{\mu} + 1)}$.

  Then it holds that
  \[
    \norm{\lambda \varphi_n + \mu \Phi_n - \lambda f - \mu g}_{\infty}
      \leq \card{\lambda} \norm{\varphi_n - f}_{\infty}
      + \card{\mu} \norm{\Phi_n - g}_{\infty}
  \] \[
      < \frac{\card{\lambda}}{2(\card{\lambda} + 1)} \cdot \varepsilon + \frac{\card{\mu}}{2(\card{\mu} + 1)} \cdot \varepsilon
      < \frac\varepsilon2 + \frac\varepsilon2 = \varepsilon
  \]

  We continue:
  \[
    \int_a^b (\lambda f + \mu g) \, dx = \lim_{n\to\infty} \int_a^b (\lambda \varphi_n + \mu \Phi_n) \, dx
      = \lim_{n\to\infty} (\lambda \int_a^b \varphi_n \, dx + \mu \int_a^b \Phi_n \, dx)
  \] \[
    = \lambda \underbrace{\lim_{n\to\infty} \int_a^b \varphi_n \, dx}_{\text{exists}}
    + \mu \underbrace{\lim_{n\to\infty} \int_a^b \Phi_n \, dx}_{\text{exists}}
  \] \[
    = \lambda \int_a^b f \, dx + \mu \int_a^b g \, dx
  \]

  We prove monotonicity.

  Show: Let $h \in \mathcal R[a,b]$ with $h \geq 0$ in $[a,b]$.
  Then it holds that $\int_a^b h \, dx \geq 0$.

  We will show that $(\tilde \varphi_n)_{n\in\mathbb N}$ exists with $\tilde \varphi_n \to h$ uniform on $[a,b]$
  and $\tilde \varphi_n \geq 0$.

  Therefore we prove:
  Let $(\varphi_n)_{n\in\mathbb N}$, $\varphi_n \in \tau[a,b]$ with $\varphi_n \to h$ uniform on $[a,b]$.

  Define $\tilde\varphi_n$ such that
  \[ \varphi_n = \sum_{j=1}^{m_n} c_j \chi_{(x_{j-1}, x_j)} + \sum_{j=0}^{m_n} d_j \chi_{\set{x_j}} \]
  Let
  \[ \tilde\varphi_n = \sum_{j=1}^{m_n} \underbrace{\tilde c_{j}}_{\geq 0} \chi_{(x_{j-1}, x_j)} + \sum_{j=0}^{m_n} \underbrace{h(x_j)}_{\geq 0} \chi_{\set{x_j}} \]
  and $\tilde c_j \coloneqq \max{c_j, 0} \geq 0$.
  So it holds that $\tilde \varphi_n \geq 0$.

  For $x = x_l$ ($l \in \set{0, \dots, m_n}$) it holds that
  \[
    \card{\tilde \varphi_n(x_l) - h(x_l)}
      = \card{\sum_{j=1}^{m_n} \tilde c_j \underbrace{\chi_{(x_{j-1}, x_j)}(x_l)}_{=0 \text{ bc. } x_l \not\in (x_{j-1}, x_j)} + \sum_{j=0}^{m_n} h(x_j) \underbrace{\chi_{\set{x_j}}(x_l)}_{= \delta_{j,l}} - h(x_l)}
  \] \[
    = \card{h(x_l) - h(x_l)} = 0 \leq \card{\varphi_n(x_l) - h(x_l)}
  \]
  For $x \in (x_{j-1}, x_j)$ it holds that
  \[
    \card{\tilde\varphi_n(x) - h(x)}
      = \card{\sum_{j=1}^{m_n} \tilde c_j \underbrace{\chi_{(x_{j-1}, x_j)}(x)}_{\delta_{l,j}} + \sum_{j=0}^{m_n} h(x) \cdot \underbrace{\chi_{\set{x_j}}(x)}_{=0 \text{ bc. } x \neq x_j} - h(x)}
  \] \[
    = \card{\tilde c_l - h(x)}
    = \begin{cases}
      \card{c_l - h(x)} & \text{ if } c_l \geq 0 \\
      \card{h(x)} = h(x) & \text{ if } c_l < 0
    \end{cases}
  \] \[
    \leq \begin{cases}
      \card{c_l - h(x)} & \text{ if } c_l \geq 0 \\
      h(x) - c_l & \text{ if } c_l < 0
    \end{cases}
  \] \[
    = \begin{cases}
      \card{\varphi_n(x) - h(x)} & \text{ if } c_l = \varphi_n(x) \geq 0 \\
      \card{h(x) - \varphi_n(x)} & \text{ if } c_l = \varphi_n(x) < 0
    \end{cases}
  \] \[
    = \card{\varphi_n(x) - h(x)}
  \]
  hence, $\card{\tilde\varphi_n(x) - h(x)} \leq \card{\varphi_n(x) - h(x)}$
  for $x \in (x_{l-1}, x_l)$ as well as $x = x_i$,
  hence
  \[ \norm{\tilde\varphi_n - h}_{\infty} \leq \underbrace{\norm{\varphi_n - h}_{\infty}}_{\to 0 \text{ for } n \to \infty} \]
  Hence $\norm{\tilde\varphi_n - h}_{\infty} \to 0$ for $n \to \infty$, hence $\tilde\varphi_n$ converges uniformly to $h$.
  There exists
  \[ \int_a^b h \, dx = \lim_{n\to\infty} \underbrace{\int_a^b \underbrace{\tilde\varphi_n}_{\geq 0} \, dx}_{\geq 0} \geq 0 \]
  Monotonicity: Let $f \leq g$ in $[a,b]$, hence $h = g - f \geq 0$ in $[a,b]$
  \[ \implies 0 \leq \int_a^b h \, dx = \int_a^b g \, dx - \int_a^b f \, dx \]
  \[ \implies \int_a^b f \, dx \leq \int_a^b g \, dx \]

  And finally, boundedness is left.

  Consider $\card{f} \in \mathbb R[a,b]$. Proving this is left as an exercise.
  $f \leq \card{f}$ in $[a,b] \implies \int_a^b f \, dx \leq \int_a^b \card{f} \, dx$.

  TODO

  \[ -f \leq \card{f} \text{ in } [a,b] \implies \int_a^b (-f) \, dx = -\int_a^b f \, dx \leq \int_a^b \card{f} \, dx
    \implies \card{\int_a^b f \, dx} TODO
  \]
\end{proof}

\begin{remark}
  $\mathcal R[a,b]$ is a vector space.

  \begin{enumerate}
    \item $f, g \in \mathbb R[a,b] \implies \lambda f + \mu g \in \mathcal R[a,b]$.
      $\norm{\cdot}_{\infty}$ is a norm on $\mathcal R[a,b]$.
      ($\mathcal R[a,b], \norm{\cdot}_{\infty}$) is a normed vector space.
      Subspace of ($\mathcal B[a,b], \norm{\cdot}_{\infty}$).
      We will show in the practicals that ($\mathcal R[a,b], \norm{\cdot}_{\infty}$) is complete.
  \end{enumerate}
\end{remark}

\begin{theorem}[Mean value theorem of integral calculus]
  \label{mvt} \label{satz3}
  Let $f$ be continuous on $[a,b]$ and $p \in \mathcal R[a,b]$
  and $p \geq 0$ in $[a,b]$.
  Then $f \cdot p \in \mathcal R[a,b]$ and there exists $\xi \in [a,b]$ such that
  \[ \int_a^b f \cdot p \, dx = f(\xi) \cdot \int_a^b p \, dx \]
\end{theorem}
\begin{proof}
  Let $m = \min\set{f(z): z \in [a,b]}$ (exists because $f$ is continuous and $[a,b]$ is compact).
  \[ M = \max\set{f(z): z \in [a,b]} \]
  \[ f([a,b]) = [m, M] \text{ (by the mean value theorem)} \]
  It holds that
  \[ m \cdot \underbrace{p(x)}_{\geq 0} \leq f(x) \cdot p(x) \leq M \cdot p(x) \]
  By monotonicity,
  \[ m \int_a^b p(x) \, dx \leq \int_a^b fp \, dx \leq M \int_a^b p \, dx \]
  Therefore, there exists $\eta \in [m, M]$.
  \[ \eta \cdot \int_a^b p(x) \, dx = \int_a^b fp \, dx \]
  Mean value theorem: For $\eta \in [m,M]$ there exists $\xi \in [a,b]$ such that
  \[ \eta = f(\xi) \text{ (f is continuous!)} \]
  Hence,
  \[ f(\xi) \int_a^b p \, dx = \int_a^b f \cdot p \, dx \]
  $f \cdot p$ is regulated function (over one-sided limits).
\end{proof}

\begin{lemma} % Lemma 7
  Let $f \in \mathcal R[a,b]$ and $a \leq \alpha < \beta < \gamma \leq b$.
  Then
  \[ f|_{[\alpha,\beta]} \in \mathcal R[\alpha,\beta], f|_{\beta,\gamma} \in \mathcal R[\beta,\gamma] \]
  \[ f|_{[\alpha,\gamma]} \in \mathcal R[\alpha,\gamma] \text{ (immediate over onesided limit)} \]
  and it holds that
  \[ \int_{\alpha}^\gamma f\, dx = \int_{\alpha}^\beta f \, dx + \int_\beta^\gamma f \, dx \]
  Compare with Figure~\ref{img:posneg}.
\end{lemma}

\begin{figure}
  \begin{center}
    \includegraphics{img/17_posneg_area.pdf}
    \caption{Positive and negative area covered by the integral}
    \label{img:posneg}
  \end{center}
\end{figure}

\begin{proof}
  Show that this statement holds for $\varphi \in \tau[a,b]$.
  Without loss of generality, $\alpha = a, \gamma = b$.
  \[
    \gamma = \sum_{j=1}^m c_j \chi_{(x_{j-1}, x_j)} + \sum_{j=0}^m \underbrace{0}_{\text{it does not matter for the integral}} \cdot \chi_{x_j}
  \]
  % TODO insert image 18_step.pdf, not sure where the proper place is
  \begin{description}
    \item[Case 1] 
      $\beta = x_l$ for some $l \in \set{1, \dots, m-1}$
      \[ \int_{\alpha}^\gamma \varphi \, dx = \sum_{j=1}^m c_j (x_j - x_{j-1}) \]
      \[ \int_{\alpha}^\beta \varphi \, dx = \int_{\alpha}^{x_l} \varphi \, dx = \sum_{j=1}^l c_j (x_j - x_{j-1}) \]
      \[ \int_{\beta}^\gamma \varphi \, dx = \int_{x_l}^{\gamma} \varphi \, dx = \sum_{j=l+1}^m c_j (x_j - x_{j-1}) \]
      And now,
      \[ \sum_{j=l+1}^m c_j (x_j - x_{j-1}) + \sum_{j=1}^l c_j (x_j - x_{j-1}) = \sum_{j=1}^m c_j (x_j - x_{j-1}) \]
    \item[Case 2]
      $\beta \in (x_{l-1}, x_l)$ for some $l \in \set{1, \ldots, m}$.
      \[ \int_\beta^\gamma \varphi \,dx = c_l(x_l - \beta)+ \sum_{j=l+1}^m c_j (x_j - x_{j-1}) \]
      \[ \int_{\alpha}^\beta \varphi \,dx + \int_\beta^\gamma \varphi \,dx = \sum_{j=1}^{l-1} c_j (x_j - x_{j-1}) \]
      \[ + c_l(\beta - x_{l-1}) + c_l (x_l - \beta) + \sum_{j=l+1}^m c_j(x_j - x_{j-1}) \]
      \[ = \sum_{j=1}^m c_j(x_j - x_{j-1}) = \int_{\alpha}^\gamma \varphi \, dx \]
      TODO verify previous lines
      Let $\varphi_n \in \tau[\alpha,\beta]$ with $\varphi_n \to f$ uniform on $[\alpha, \beta] \implies \varphi_n|_{[\alpha,\beta]} \to f|_{[\alpha,\beta]}$ uniform on $[\alpha,\beta]$ and also $\varphi_n|_{[\beta,\gamma]} \to f|_{[\beta,\gamma]}$ uniform on $[\beta,\gamma]$.
      \[ \int_{\alpha}^\gamma f \, dx = \lim_{n\to\infty} \int_{\alpha}^\gamma \varphi_n \, dx = \lim_{n\to\infty} (\int_{\alpha}^\beta \varphi_n \, dx + \int_{\beta}^\gamma \varphi_n \, dx) \]
      \[
        = \underbrace{\lim_{n\to\infty} \int_{\alpha}^\beta \varphi_n \, dx}_{\text{exists because } \varphi_n|_{[\alpha,\beta]} \to f|_{[\alpha,\beta]} \text{uniform}} + \lim_{n\to\infty} \int_{\beta}^\gamma \varphi_n \, dx
      \] \[
        = \int_{\alpha}^\beta f \, dx + \int_{\beta}^\gamma f \, dx
      \]
  \end{description}
\end{proof}

\begin{remark}[Notation]
  Let $\alpha < \beta$, $\alpha, \beta \in [a,b]$ and $f \in \mathcal R[a,b]$. We let
  \[ \int_{\beta}^\alpha f \, dx \coloneqq -\int_{\alpha}^\beta f \, dx \]
  By this convention, it holds that
  \[ \int_{\alpha}^\alpha f \, dx = -\int_{\alpha}^\alpha f \, dx \implies \int_{\alpha}^\alpha f \, dx = 0 \]
\end{remark}

\begin{lemma} % Lemma 8
  \label{lemma8}
  Let $f \in \mathcal R[a,b]$ and $\alpha,\beta,\gamma \in [a,b]$ (without particular order).
  Then it holds that
  \[ \int_{\alpha}^\gamma f \, dx = \int_{\alpha}^\beta f \, dx + \int_{\beta}^\gamma f \, dx \]
\end{lemma}

\begin{proof}
  Special case: 2 points are equal
  \[ \alpha = \gamma \implies \int_a^\alpha f \, dx = 0 \]
  \[ \int_\alpha^\beta f \, dx + \int_\beta^\alpha f \, dx = \int_{\alpha}^\beta f \, dx - \int_\alpha^\beta f \, dx = 0 \]
  \[ \beta = \gamma \qquad \beta = \alpha \]

  Case: $\alpha < \beta < \gamma$ follows immediately

  And just as a representative other case: $\alpha < \gamma < \beta$
  \[ \int_{\alpha}^\beta f \, dx \underbrace{=}_{\text{by Lemma~\ref{lemma7}}} \int_{\alpha}^\gamma f \, dx + \underbrace{\int_{\gamma}^\beta f \, dx}_{- \int_{\beta}^\gamma f \, dx} \]
  \[ \int_\alpha^\beta f \, dx + \int_\beta^\gamma f \, dx = \int_\alpha^\gamma f \, dx \]
\end{proof}

\dateref{2018/04/17}

\begin{lemma} % Lemma 9
  \label{lemma9}
  Let $f \in \mathcal R[a,b]$. Then there exists an at most countable set $A \subseteq [a,b]$ such that $f$ is continuous in every point $x \in [a,b] \setminus A$.
\end{lemma}
\begin{proof}
  Let $f \in \mathcal R[a,b]$ and $(\varphi_n)_{n \in \mathbb N}$ with $\varphi_n \in \tau[a,b]$ and $\varphi \to f$ converging uniformly on $[a,b]$.
  \[ \varphi_n = \sum_{j=1}^{m_n} c_j^n \chi_{(X_{j-1}^n, X_j^n)} + \sum_{j=0}^{m_n} d_j^n \chi_{\set{x_j^n}} \]
  \[ x_0^n = a < x_1^n < \ldots < x_{m_n}^n = b \]
  are separating points for $\varphi_n$
  \[ A = \set{X_j^n: n \in \mathbb N, j \in \set{0, \ldots, m_n}} \]
  $A$ is a countable union of finite sets $A_n = \set{x_0^n, x_{m_n}^n}$. A is countable (as unions of finite sets are).

  Now we show: $f$ is continuous in every point $x \in [a,b]: x \not\in A$.
  Let $\varepsilon > 0$ be arbitrary. Choose $N \in \mathbb N$ sufficiently large such that $\norm{\varphi_N - f}_{\infty} < \frac\varepsilon2$. Because $x \in A$, there exists $j \in \set{1, \ldots, m_N}$ such that $x \in (x_{j-1}^N, X_j^N)$ is open.
  Choose $\delta > 0$ such that $(x - \delta, x + \delta) \subset (x_{j-1}^N, x_j^n)$, hence $\forall \xi \in (x - \delta, x + \delta)$ it holds that $\varphi_N(\xi) = c_j^N$. \\
  Now consider $\xi \in (x - \delta, x + \delta)$, hence $\card{\xi - x} < \delta$. Then it holds that
  \[ \card{f(\xi) - f(x)} = \card{f(\xi) - \underbrace{\varphi_N(x)}_{c_j^N = \varphi_N(\xi)} + \varphi_N(x) - f(x)} \]
  \[ \leq \underbrace{\card{f(\xi) - \varphi_N(\xi)}}_{\leq \norm{f - \varphi_N}_{\infty}} + \underbrace{\card{\varphi_N(x) - f(x)}}_{\leq \norm{\varphi_N - f}_{\infty}} < \frac\varepsilon2 + \frac\varepsilon2 = \varepsilon \]
  Hence $f$ is continuous in $x$.
\end{proof}

\begin{remark}[Notation]
  Let $f \in \mathcal R[a,b]$. For $x \in [a,b)$, there exists $f_+(x) \coloneqq \lim_{\xi \to x_+} f(\xi)$.
  For $x \in (a,b]$, there exists $f_-(x) \coloneqq \lim_{\xi \to x_-} f(\xi)$.
  Because of Lemma~\ref{lemma9}, it holds that $f_+(x) = f_-(x) = f(x)$ for all $x \in [a,b] \setminus A$ and $A$ is at most countable.
\end{remark}

\index{Right-sided derivative}
\index{Left-sided derivative}
\index{Half-sided derivative}
\begin{definition}[One-sided derivatives] % Definition 9
  Let $g: [a,b] \to \mathbb R$ and $x \in [a,b)$.
  We say $g$ has the \emph{right-sided derivative} $g'_+(x)$ if
  \[ \lim_{\xi \to x_+} \frac{g(\xi) - g(x)}{\xi - x} \eqqcolon g'_+(x) \]
  exists. Analogously we define the left-sided derivative
  \[ g'_-(x) = \lim_{\xi \to x_-} \frac{g(\xi) - g(x)}{\xi - x} \]
  for $x \in (a,b]$. Compare with Figure~\ref{img:lrderiv}.

  \begin{figure}[!h]
    \begin{center}
      \includegraphics{img/19_left_right_sided_derivative.pdf}
      \caption{In this example, the left- and right-sided derivatives are not equal. $f'_+(x) \neq f'_-(x)$}
      \label{img:lrderiv}
    \end{center}
  \end{figure}
\end{definition}

\begin{remark}
  If $g$ in $x$ has a one-sided derivative, then it holds that
  \[ \lim_{\xi \to x_\pm} (g(\xi) - g(x)) = 0 \]
  Hence $g$ is continuous in $x$.
\end{remark}

\begin{remark}
  $g: [a,b] \to \mathbb R$ is differentiable in point $x \in (a,b)$ with derivative $g'(x)$
  $\iff g$ has a left- and right-sided derivative in $x$ and it holds that $g'_-(x) = g'_+(x)$ ($= g'(x)$).
\end{remark}

\begin{theorem}[Fundamental theorem of differential/integration calculus, variation 1] % Satz 4
  \label{ftofic}
  Isaac Barrow (1630--1677), Isaac Newton (1642--1726), Gottfried Wilhelm von Leibniz (1646--1716).

  Let $f \in \mathcal R[a,b]$, $\alpha \in [a,b]$ and we define
  \[ F(x) = \int_{\alpha}^x f \, d\xi \]
  Then $F$ is right-sided differentiable in every point $x \in [a,b)$ and in every $x \in (a,b]$ left-sided differentiable.
  Furthermore it holds that
  \begin{align}
    \label{ftdi}
    F'_+(x) &= f_+(x) \forall x \in [a,b) \\
    F'_-(x) &= f_-(x) \forall x \in (a,b]
  \end{align}
\end{theorem}

\begin{remark}
  \[ \frac{d}{dx} \left(\int_\alpha^x f \, d\xi\right) = f(x) \]
  for all $x$ such that $f$ is continuous in $x$.
  For those $x$, $F'(x)$ is differentiable in $x$ with $F'(x) = f(x)$.
\end{remark}

\index{Antiderivative}
\begin{definition} % Definition 10
  \label{def10}
  Let $f \in \mathcal R[a,b]$ and $\varphi: [a,b] \to \mathbb R$ such that $\varphi$ is one-sided differentiable on $[a,b]$.
  If $\Phi'_+(x) = f_+(x) \forall x \in [a,b)$ and $\Phi'_-(x) = f_-(x) \forall x \in (a,b]$
  then we call $\Phi$ an antiderivative of regulated function $f$.
\end{definition}

\begin{proof}[Proof of the Theorem~\ref{ftofic}]
  Let $x_1, x_2 \in [a,b]$ be arbitrary. Let $F$ be defined as above. Then it holds that
  \[ \card{F(x_2) - F(x_1)} = \card{\int_\alpha^{x_2} f \, d\xi - \int_\alpha^{x_1} f \, d\xi} \]
  \[ = \card{\int_\alpha^{x_2} f \, d\xi + \int_{x_1}^\alpha f \, d\xi} = \card{\int_{x_1}^{x_2} f \, d\xi} \]
  \[ \leq \int_{x_1}^{x_2} \card{f} \, d\xi \leq \int_{x_1}^{x_2} \underbrace{\norm{f}_{\infty}}_{\text{const independent of } \xi} \, d\xi  = \norm{f}_{\infty} \cdot \card{x_2 - x_1} \]
  Hence $F$ is Lipschitz continuous with Lipschitz constant $\norm{f}_{\infty}$. So $F$ is continuous in $[a,b]$.

  One-sided derivatives: Let $x \in [a,b)$ and $\varepsilon > 0$ be arbitrary.
  Choose $\delta > 0$ such that $\forall \xi \in [x, x+\delta)$ it holds that $\card{f(\xi) - f_+(x)} < \varepsilon$.
  For $\xi \in (x, x+\delta)$ it holds that
  \[ \card{\frac{F(\xi) - F(x)}{\xi - x} - f_+(x)} = \frac1{\card{\xi - x}} \card{\underbrace{\int_x^\xi f \, dy}_{F(\xi) - F(x)} - \underbrace{f_+(x)(\xi - x)}_{\int_x^\xi \underbrace{f_+(x)}_{\text{const.}} \, dy}} = \frac{1}{\card{\xi - x}} \card{\int_x^\xi (f - f_+(x)) \, dy} \leq \frac{1}{\card{\xi - x}} \int_x^\xi \underbrace{\card{f(y) - f_+(x)}}_{< \varepsilon} \, dy \]
  \[ y \in (x, \xi) \subseteq (x, x+\delta) \]
  \[ < \frac{1}{\xi - x} \varepsilon \cdot \underbrace{\int_x^\xi 1 \, dy}_{\card{\xi - x}} = \varepsilon \]
  Hence, $F'_+(x) = f_+(x)$.
  Analogously, $F'_-(x) = f_-(x)$ for $x \in (a,b]$.
\end{proof}

\begin{theorem}[Fundamental theorem of differential/integration calculus, variation 2] % Satz 5
  \label{satz5}
  Let $f \in \mathcal R[a,b]$ and $\phi$ is an arbitrary antiderviative of $f$ according to Definition~\ref{def10}.
  For $\alpha, \beta \in [a,b]$ arbitrary, it holds that
  \[ \int_\alpha^\beta f \, dx = \phi(\beta) - \phi(\alpha) \]
\end{theorem}

\begin{remark}
  Let $f$ be continuous and $\phi$ be an antiderivative of $f$.
  Hence, $\Phi'(x) = f(x) \forall x \in [a,b]$.
  Then it holds that
  \[ \int_{\alpha}^{\beta} \Phi' \, dx = \Phi(\beta) - \Phi(\alpha) \]
  \enquote{Integral of a derivative of $\Phi$ gives $\Phi(\beta) - \Phi(\alpha)$}.
\end{remark}

\begin{lemma} % Lemma 10
  \label{lemma10}
  Let $A \subseteq [a,b]$ countable.
  $f: [a,b] \to \mathbb R$ is continuous and $f$ is differentiable in every point $x \in [a,b] \setminus A$.
  Furthermore let $\card{f'(x)} \leq L$ ($L \geq 0$) for all $x \in [a,b] \setminus A$.
  Then $f$ is Lipschitz continuous on $[a,b]$ with constant $L$, hence
  \[ \card{f(x_2) - f(x_1)} \leq L \card{x_2 - x_1} \forall x_1, x_2 \in [a,b] \]
\end{lemma}
\begin{remark}
  Some people call it \emph{differentiable almost everywhere},
  but this expression collides with a different definition
  pronounced the same way from measure theory.
\end{remark}
\begin{proof}
  Let $x_1, x_2 \in [a,b]$, wlog. $x_1 < x_2$.
  Let $\varepsilon > 0$ be arbitrary. We define
  \[ F_{\varepsilon}(x) = \card{f(x) - f(x_1)} - (L + \varepsilon)(x - x_1) \]
  for $x \in [x_1, b]$.

  Let $\varepsilon > 0$ be arbitrary.
  We prove: $F_{\varepsilon}(x) \leq 0 \forall x \in [x_1, b]$. In particular: $F_{\varepsilon}(x_2) \leq 0$.
  Hence,
  \[ \card{f(x_2) - f(x_1)} \leq (L + \varepsilon)\underbrace{(x_2 - x_1)}_{\card{x_2 - x_1}} \]

  We prove by contradiction:
  Assume there exists $\varepsilon > 0$ and $x_{\varepsilon} > x_1$
  such that $F_{\varepsilon}(x_{\varepsilon}) > 0$.

  We recognize:
  Let $A' = [x_1, b] \cap A$ be countable.
  \begin{enumerate}
    \item hence $F_{\varepsilon}(A') \subseteq \mathbb R$ is countable
    \item $F_{\varepsilon}(x_1) = 0$, $F_{\varepsilon}(x_{\varepsilon}) > 0 \implies x_{\varepsilon} > x_1$
    \item $F_{\varepsilon}$ is continuous on $[x_1, b]$.
      It holds that $0 \in F_{\varepsilon}([x_1, x_{\varepsilon}])$ and because $0 = F_{\varepsilon}(x_1)$
      and $\varepsilon \in F_{\varepsilon}([x_1, x_{\varepsilon}])$ because $\varepsilon = F_{\varepsilon}(x_{\varepsilon})$.

      By the Intermediate Value Theorem, it follows that $[0, \varepsilon] \subseteq $ TODO
      By the Intermediate Value Theorem, it follows that $\underbrace{[0, \eta]}_{\text{uncountable}} \subseteq F_{\varepsilon}([x_1, x_{\varepsilon}])$.
      $F_{\varepsilon}(A')$ is countable, hence there exists $\gamma \in (0, \eta]$ such that $\gamma = F_{\varepsilon}(y)$ and $\gamma \not\in A'$ ($\gamma > 0$) \footnote{remember this as reference (*)}.
      Hence, $y \not\in A'$. So $f$ in $y$ is differentiable.
      Let $B \coloneqq F_{\varepsilon}^{-1}(\set{\gamma}) \cap \left([x_1, x_{\varepsilon}] \setminus A'\right)$.
      Then $B \neq \emptyset$.

      $B \subseteq [x_1, x_{\varepsilon}]$ is therefore bounded, $B \neq 0$.
      Hence, $B$ has a supremum. Let $x = \sup{B}$.
      Choose $(y_n)_{n \in \mathbb N}$ with $y_n \in B$ and $y_n \to x$ for $n \to \infty$.
      Because $F_{\varepsilon}$ is continuous, it holds that
      \[ \lim_{n\to\infty} \underbrace{F_{\varepsilon}(y_n)}_{\gamma} = F_{\varepsilon}(x) \]
      hence $F_{\varepsilon}(x) = \gamma$. This implies $x \not\in A$.

      Furthermore it holds for $w \in (x, x_\varepsilon]$ that $F_{\varepsilon}(w) > \gamma$.
      Because assume the opposite ($F_{\varepsilon}(w) \leq \gamma$ for $w > x$).
      Furthermore it holds that $F_{\varepsilon}(x_{\varepsilon}) = \eta \geq \gamma$.
      Because of the Intermediate Value Theorem, $\exists y \geq w$ with $F_{\varepsilon}(y) = \gamma$.
      This contradicts with the supremum property of $x$.

      Now let $y \in (x, x_{\varepsilon}]$.
      \begin{align*}
        \varphi(y) &= \frac{F_{\varepsilon}(y) - F_{\varepsilon}(x)}{y - x} \\
          &\underbrace{=}_{\substack{\text{definition of} \\ F_{\varepsilon}}}
          \frac{\card{f(y) - f(x_1)} - \card{f(x) - f(x_1)}}{y - x} - \frac{(L + \varepsilon)(y - x_1 - x + x_1)}{y - x} \\
          &\underbrace{\leq}_{\text{inversed triangle ineq.}} \frac{f(y) - f(x)}{y - x} - (L + \varepsilon)
      \end{align*}
      Because $F_{\varepsilon}(y) > \gamma = F_{\varepsilon}(x)$ it holds that $\varphi(y) > 0$ for $y > x$.
      So,
      \[ \frac{\card{f(y) - f(x)}}{y - x} \geq L + \varepsilon \]
      \[ \card{f'(x)} = \lim_{y \to x_+} \card{\frac{f(y) - f(x)}{y - x}} \geq L + \varepsilon \]
      This contradicts with the boundedness of the derivative by $L$ and $f$ is in $x \not\in A$ differentiable.

      So, equations~\ref{ftdi} do not hold. Therefore $\forall x_1, x_2$ with $x_1 < x_2$ in $[a,b]$ and $\forall \varepsilon > 0$,
      \[ \card{f(x_2) - f(x_1)} \leq (L + \varepsilon)\card{x_2 - x_1} \]
      \[ \implies \card{f(x_2) - f(x_1)} \leq L \card{x_2 - x_1} \]
  \end{enumerate}
\end{proof}

\begin{corollary}[Corollary to Lemma~\ref{lemma10}]
  Let $f, g: [a,b] \to \mathbb R$ differentiable for all points $x \in [a,b] \setminus A$
  and $A$ is countable. Furthermore let $f'(x) = g'(x) \forall x \not\in A$.
  Then there exists $K \in \mathbb R$ such that $f(x) = g(x) + K \forall x \in [a,b]$.
\end{corollary}

\begin{proof}
  Let $h = f - g$. Then it holds that
  \[ h'(x) = f'(x) - g'(x) = 0 \forall x \in [a,b] \setminus A \]
  By Lemma~\ref{lemma10} with $L = 0$, it follows that
  \[ \card{h(x_1) - f(x_2)} \leq 0 \cdot \card{x_1 - x_2} = 0 \]
  \[ \implies h(x_1) = h(x_2) \forall x_1, x_2 \in [a,b] \]
  Hence, $h(x) = K \in \mathbb R$.
  \[ \implies f(x) = g(x) + h(x) = g(x) + K \]
\end{proof}

\dateref{2018/04/19}

By reference~(*), $\gamma \in [0, \eta)$ (uncountable) and $\gamma \not\in f(A)$ (countable).
\[ \implies \forall u \in [x_1, b) \text{ with } F_{\varepsilon}(u) = \gamma \]
it holds that $u \not\in A$, hence $f$ is differentiable in $u$.

\begin{proof}[Proof of Theorem~\ref{satz5}]
  Let $f \in \mathcal R[a,b]$, $\phi$ is an antiderivative of $f$,
  hence $\phi_+' = f_+$, $\phi_-' = f_-$. Let $\alpha \in [a,b]$ be arbitrary.
  By the Theorem variant 1, $F(x) = \int_{\alpha}^x f \, d\xi$ is also an antiderivative of $f$.
  By Lemma~\ref{lemma}, $\exists K \in \mathbb R: F(x) = \int_{\alpha}^x f \, d\xi = \phi(x) + K$.
  Determine $K$: Let $x = \alpha \implies F(\alpha) = \int_\alpha^\alpha f \, dx = 0 = \phi(\alpha) - K$
  hence $K = \phi(\alpha)$. Hence,
  \[ \int_\alpha^x f \, d\xi = \phi(x) - \phi(\alpha) \]
  Let $x = \beta$.
\end{proof}

\begin{remark}[Remark for the previous corollary]
  $F$, $\phi$ are differentiable on all points $x$ for which $f$ is continuous
  (all of them except for countable many).
  For those $x$, it holds that $F'(x) = \varphi'(x) = f(x)$.
\end{remark}

\index{Indefinite integral}
\begin{remark}[Notation]
  Let $f \in \mathcal R[a,b]$. Then
  \[ \int f \, dx \]
  \begin{itemize}
    \item is some particular antiderivative of $f$ (usually some arbitrary chosen)
    \item the set of \emph{all} antiderivatives of $f$
      \[ \int f \, dx = \set{F: F \text{ is antiderivative of f}} \]
      If $F_0$ is some fixed antiderivative, then
      \[ \int f \, dx = \set{F_0 + K: K \in \mathbb R} \]
      Then $\int f \, dx$ is the so-called \emph{indefinite integral of $f$}.
      Notation:
      \[ \int x^k \, dx = \frac{x^{k+1}}{k+1} + c \qquad (k \neq -1) \]
  \end{itemize}
\end{remark}

\begin{table}[!h]
  \begin{center}
    \begin{tabular}{c|c|c}
      $f$ & $F$ & remark \\
    \hline
      $x^\alpha$ & $\frac{x^{\alpha+1}}{\alpha + 1} + c$ & $\alpha \in \mathbb R \setminus \set{-1}$; restrict $x$ such that $x^\alpha$ and $x^{\alpha+1}$ are defined \\
      $x^{-1}$ & $\ln{x} + c$ ($x > 0$) & \\
      $\left(\frac1{-x}\right) \cdot (-1) = x^{-1}$ & $\ln{-x} + c$ ($x < 0$) & \\
      $e^x$ & $e^x$ & \\
      $\sin{x}$ & $-\cos{x}$ & \\
      $\cos{x}$ & $\sin{x}$ & \\
      $\sinh{x}$ & $\cosh{x}$ & \\
      $\cosh{x}$ & $\sinh{x}$ & \\
      $\frac{1}{1 + x^2}$ & $\arctan x$ \\
      $\frac{1}{\sqrt{1 - x^2}}$ & $\arcsin{x}$ & $\card{x} < 1$ \\
      $-\frac{1}{\sqrt{1 - x^2}}$ & $\arccos{x}$ &
    \end{tabular}
  \end{center}
  \caption{Table of antiderivatives}
  \label{tbl:antideriv}
\end{table}

\subsection{Integration methods}

In this chapter, we discuss how to determine the antiderivative of a function.
Usually they are composites of basic functions. Some of these are given in Table~\ref{tbl:antideriv}.

\begin{remark}
  Let $F, G: [a,b] \to \mathbb R$ in $x \in [a,b)$ right-sided differentiable.
  Then also $F \cdot G$ in $x$ is right-sided differentiable and it holds that
  \[ (F \cdot G)'_+(x) = F'_+(x) \cdot G(x) + F(x) \cdot G'_+(x) \]
  hence the product law holds.

  Analogously, the same holds for the left-sided derivative.

  Look up the proof in the course Analysis 1.
\end{remark}

\subsubsection{Partial integration}

\index{Partial integration}
\begin{definition}[Partial integration]
  Let $f, g$ be given. Let $F, G$ be its antiderivatives respectively.
  Then $F \cdot G$ is an antiderivative of $F \cdot g + f \cdot G$.

  This is immediate, because
  \[
    (F \cdot G)'_+ = F'_+ \cdot G + F \cdot G'_+
    = f_+ \cdot G + F \cdot g_+ = f_+ G_+ + F_+ \cdot g_+
  \]
  Hence, it holds that
  \[
    \int_a^b (F g + f G) \, dx
    = \underbrace{F(b) \cdot G(b) - F(a) G(a)}_{\eqqcolon \left.F \cdot G\right|_a^b}
  \]
  Usually, this is rewritten as
  \[ \int_a^b F \cdot g \, dx = \left. F \cdot G\right|_a^b - \int_a^b f G \, dx \]
  If $F = u$ is continuously differentiable and $G = v$ as well,
  then $f = u'$ and $g = v'$ and the law has the structure
  \begin{framed}
    \[ \int_a^b uv' \, dx = \left.u\cdot v\right|_a^b - \int_a^b u'v \, dx \]
  \end{framed}
\end{definition}

\begin{example}
  Let $a \neq -1$ and $x > 0$.
  \[
    \int \underbrace{x^a}_{v'} \cdot \underbrace{\ln{x}}_{u} \, dx =
    \underbrace{\begin{vmatrix}
      u = \ln{x} & u' = \frac1x \\
      v' = x^\alpha & v = \frac{x^{\alpha+1}}{\alpha + 1}
    \end{vmatrix}}_{\text{scribble notes}}
    \quad
    \frac{x^{\alpha + 1}}{\alpha + 1} \cdot \ln{x} - \int \frac1{x} \cdot \frac{x^{\alpha+1}}{\alpha + 1} \, dx
  \] \[
    = \frac{x^{\alpha+1}}{\alpha + 1} \cdot \ln{x} - \frac{1}{\alpha + 1} \int x^\alpha \, dx
    = \frac{x^{\alpha+1}}{\alpha + 1} \cdot \ln{x} - \frac{1}{(\alpha + 1)^2} x^{\alpha+1}
  \]
\end{example}

\begin{example}
  Let $k \in \set{2,3,4,\ldots}$.
  \[
    \int \cos^k(x) \, dx =
    \begin{vmatrix}
      u = \cos^{k-1}(x) & u' = (k-1) \cdot \cos^{k-2}(x) \cdot (-\sin{x}) \\
      v' = \cos{x} & v = \sin{x}
    \end{vmatrix}
  \] \[
    \cos^{k-1}(x) \sin{x} + (k - 1) \int \cos^{k-2}(x) \cdot \underbrace{\sin^2(x)}_{(1 - \cos^2{x})} \, dx
  \] \[
    = \cos^{k-1}(x) \cdot \sin(x) + (k-1) \int \cos^{k-2}(x) \, dx - (k-1) \int \cos^k(x) \, dx
  \]
  Then we can use the following identity:
  \[
    k \int \cos^k(x) \, dx = \cos^{k-1}(x) \cdot \sin(x) + (k-1) \int \cos^{k-2}(x) \, dx
  \]
  This gives a recursive formula:
  \[
    \int \cos^k(x) \, dx = \frac1k \cos^{k-1}(x) \cdot \frac{k-1}{k} \sin(x) + (k-1) \int \cos^{k-2}(x) \, dx
  \]
  Analogously,
  \[ \int \sin^k(x) \, dx = -\frac1k \sin^{k-1}(x) \cdot \cos(x) + \frac{k-1}{k} \int \sin^{k-2}(x) \, dx \]
  Let $c_m = \int_0^{\frac\pi2} \cos^m(x) \, dx$. Then the following formula holds:
  \begin{align*}
    c_{2n} &= \frac{2n - 1}{2n} \cdot \frac{2n - 3}{2n - 2} \cdot \frac{2n - 5}{2n - 4} \ldots \frac{1}{2} \cdot \frac{\pi}{2} \\
      &= \prod_{k=1}^n \frac{2k - 1}{2k} \cdot \frac{\pi}{2} \\
    c_{2n+1} &= \prod_{k=1}^n \frac{2k}{2k + 1}
  \end{align*}
  \begin{proof}[Proof by induction]
    Let $n = 1$.
    \begin{align*}
      c_2 &= \int_0^{\frac\pi2} \cos^2{x} \, dx = \left.\frac12 \cos{x} \sin{x} \right|_0^{\frac\pi2} + \frac12 \int_0^{\frac\pi2} 1 \, dx = 0 - 0 + \frac\pi4 \\
          &= \underbrace{\prod_{k=1}^1 \frac{2k - 1}{2k}}_{\frac12} \cdot \frac\pi2 \\
      c_1 &= \int_0^{\frac\pi2} \cos{x} \, dx = \left. \sin{x} \right|_0^{\frac\pi2} = 1 - 0 = 1 \\
      \underbrace{\prod_{k=1}^0 \frac{2k}{2k + 1}}_{\text{empty product}} &= 1 \\
    \end{align*}
    We make the induction step $n \to n+1$:
    \begin{align*}
      c_{2(n+1)} &= \left.\frac{1}{2n+2} \cdot \underbrace{\cos^{2n+1}(x)}_{=0 \text{ for } x = \frac\pi2} \cdot \underbrace{\sin(x)}_{=0 \text{ for } x = 0}\right|_{0}^{\frac\pi2} + \frac{2n + 1}{2n + 2} \int_0^{\frac\pi2} \cos^{2n}(x) \, dx \\
        &= \frac{2n+1}{2n+2} \prod_{k=1}^n \frac{2k - 1}{2k} \cdot \frac\pi2 = \prod_{k=1}^{n+1} \frac{2k-1}{2k} \cdot \frac\pi2 \\
    \end{align*}
    $c_{2(n+1)+1}$ analogously.
  \end{proof}
\end{example}

\begin{theorem}[Wallis product]
  John Wallis (1616--1703), result from 1655

  Let $w_n = \prod_{k=1}^n \frac{(2k)^2}{(2k - 1)(2k + 1)} = \frac{2\cdot 2}{1 \cdot 3} \cdot \frac{4 \cdot 4}{3 \cdot 5} \ldots$.
  Then it holds that $\lim_{n\to\infty} w_n = \frac\pi2$.
\end{theorem}
\begin{proof}
  \[
    \frac\pi2 \cdot \frac{c_{2n+1}}{c_{2n}}
    = \frac\pi2 \cdot \prod_{k=1}^n \frac{\frac{2k}{2k+1}}{\prod_{k=1}^n \frac{2k-1}{2k} \cdot \frac\pi2}
    = \prod_{k=1}^n \frac{(2k)^2}{(2k-1)(2k+1)} = w_n
  \]
  It remains to show that $\lim_{n\to\infty} \frac{c_{2n+1}}{c_{2n}} = 1$ in $[0, \frac\pi2]$ it holds that $0 \leq \cos{x} \leq 1$.
  \[ \implies \cos^{2n+2}(x) \leq \cos^{2n+1}(x) \leq \cos^{2n}(x) \]
  So, $c_{2n+2} \leq c_{2n+1} \leq c_{2n}$ for $n \geq 1$.
  \[ 1 \geq \frac{c_{2n+1}}{c_{2n}} \]
  \[ \implies 1 \geq \frac{c_{2n+1}}{c_{2n}} \geq \frac{c_{2n+2}}{c_{2n}} = \frac{\prod_{k=1}^{n+1} \frac{2k-1}{2k} \frac\pi2}{\prod_{k=1}^n \frac{2k-1}{2k} \frac\pi2} \]
  \[ = \frac{2n + 2 - 1}{2n + 2} \to 1  \text{ for } n \to \infty \]
  Because of the sandwich lemma for convergent sequences, the intermediate expression must also converge to $1$, hence
  \[ \lim_{n\to\infty} \frac{c_{2n+1}}{c_{2n}} = 1 \qquad \land \qquad \frac\pi2 \cdot \lim_{n\to\infty} \frac{c_{2n+1}}{c_{2n}} = \underbrace{\lim_{n\to\infty}}_{=1} w_n \]
\end{proof}

\subsubsection{Integration by substitution}
\begin{definition}[Integration by substitution]
  Let $f: [a,b] \to \mathbb R$ be continuous.
  Let $t: [\alpha,\beta] \to [a,b]$ be continuously differentiable.
  Let $F$ be an antiderivative of $f$ ($F$ is therefore continuously differentiable).
  Then $F \circ t: [\alpha, \beta] \to \mathbb R$ is also continuously differentiable and the chain rule holds:
  \[ (F \circ t)' = (F' \circ t) \cdot t' = (f \circ t) \cdot t' \]
  Hence $F \circ t$ is an antiderivative of $(f \circ t) \cdot t'$. We apply it to integration:
  \[
    \int_{\alpha}^\beta (f \circ t)(u) \cdot t'(u) \, du
    = (F \circ t)(\beta) - (F \circ t)(\alpha)
    = F(t(\beta)) - F(t(\alpha)) = \int_{t(\alpha)}^{t(\beta)} f(x) \, dx
  \]
  Then we get the substitution integration method:
  \begin{framed}
  \[
    \int_{t(\alpha)}^{t(\beta)} f(x) \, dx = \int_{\alpha}^\beta f(t(u)) \cdot t'(u) \, du
  \]
  \end{framed}
\end{definition}
\begin{remark}[Mnemonic]
  Consider the left-hand side and right-hand side simultaneously.
  Let $x = t(u)$ (expressions inside parentheses). Then $dx = t'(u) \cdot du$ (expressions on the right).
  Let $u = \alpha \implies x = t(\alpha)$ and $u = \beta \implies x = t(\beta)$ (interval boundaries).
\end{remark}

\begin{example}
  \[ \int_0^1 2x \sqrt{1 - x^2} \, dx \]
  Usually we have some expression, we want to substitute with $u$.
  \[ 1 - x^2 = u \qquad x = \sqrt{1 - u} = t(u) \]
  \[ x = 0 = t(1) \qquad x = 1 = t(0) \]
  \[ dx = \frac12 \cdot \frac{1}{\sqrt{1 - u}} \cdot (-1) \, du \]
  \[ \int_0^1 2x \sqrt{1 - x^2} \, dx = \int_1^0 2 \cdot \sqrt{1 - u} \cdot u \cdot \frac12 (-1) \frac{1}{\sqrt{1 - u}} \, du = \int_0^1 \sqrt{u} \, du = \left. \frac{u^{\frac32}}{\frac32} \right|_0^1 = \frac23 \]

  \[
    \int_0^1 2x \sqrt{\underbrace{1 - x^2}_{u}} \, dx =
    \begin{vmatrix}
      u = 1 - x^2 & \\
      x = 0 & \Leftrightarrow u = 1 \\
      x = 1 & \Leftrightarrow xu = 0 \\
      1 \cdot du &= -2x \, dx
    \end{vmatrix}
    = -\int_1^0 \sqrt{u} \, du = \int_0^1 \sqrt{u} \, du
  \]

  In general: we set $h(u) = g(x)$, then it holds that $h'(u) \, du = g'(x) \, dx$.
\end{example}

\begin{theorem} % Satz 7
  \label{satz7countable}
  Let $f, \tilde f \in \mathcal R[a,b]$ and $A \subseteq [a,b]$ countable.
  Furthermore $f(x) = \tilde f(x) \forall x \in [a,b] \setminus A$.
  Then it holds that
  \[ \int_a^b \card{f - \tilde f} \, dx = 0 \]
  Then it follows especially that
  \[ \int_a^b f \, dx = \int_a^b \tilde f \, dx \]
\end{theorem}

\dateref{2018/04/24}

\begin{proof}
  Show: $r \in \mathcal R[a,b], r \geq 0$. $\int_a^b r \, dx = 0$
  and $r(x) = 0$ for $x \in [a,b] \setminus A$. Then it holds that $\int_a^b r \, dx = 0$.
  Let $r$ be as above. First, we show: $r_+(x) = \lim_{\xi\to x_+} r(\xi) = 0 \forall x \in [a,b)$
  and also $r_-(x) = 0 \forall x \in (a,b]$.

  Proof of that: Let $x \in [a,b)$ and $y = r_+(x)$ (exists because $r \in \mathcal R[a,b]$).
  Choose $\delta_n = \frac1n$. $(x, x + \frac1n) \cap [a,b)$ is an open interval with uncountable many points,
  so there is certainly one point in $A$. So there exists $\xi_n \in ((x, x + \frac1n) \cap [a,b)) \setminus A$
  and $\card{\xi_n - x} < \delta_n = \frac1n$.
  Hence, $\lim_{n\to\infty} \xi_n = x$ and $r(\xi_n) = 0$. Therefore, $\lim_{n\to\infty} r(\xi_n) = 0$ where $r(\xi_n) = y = r_+(x)$.

  Analogously, $r_-(x) = 0$ on $(a,b]$.

  Let $\varepsilon > 0$ be arbitrary. We let $A_{\varepsilon} = \setdef{w \in [a,b]}{r(w) > \varepsilon}$.
  We show: $A_{\varepsilon}$ is finite.

  Assume $A_{\varepsilon}$ would have infinitely many points. Choose a sequence $(w_n)_{n \in \mathbb N}$ with $w_n \in A_{\varepsilon}$ and $w_n \neq w_m$ for $n \neq m$ (works because $A_{\varepsilon}$ is infinite). $(w_n)_{n \in \mathbb N}$ is bounded, hence there exists a convergent subsequence $(w_{n_k})_{k \in \mathbb N}$ with $x = \lim_{k\to\infty} w_{n_k} \in [a,b]$ and $w_{n_k} \in [a,b]$.

  Either $(w_{n_k})$ contains infinitely many sequence element $w_{n_k} < x$ (variant (a)) or infinitely many $w_{n_k} > x$ (variant (b)). Let variant b hold without loss of generality.

  Combine all $w_{n_k} > x$ to one subsequence $(w_{n_{k_l}})_{l \in \mathbb N}$. This gives $\lim_{l\to\infty} w_{n_{k_l}} = x$ and $w_{n_{k_l}} > x$, thus $\lim_{l\to\infty} \underbrace{r(w_{n_{k_l}})}_{\geq \varepsilon \text{ because } w_{n_{k_l}} \in A_{\varepsilon}} = r_+(x) = 0$. This gives a contradiction. $A_{\varepsilon}$ must be finite.

  Consider
  \[ A_{\frac1n} = \set{w_1^n, \dots, w_{m_n}^n} \]
  finite. Let $\varphi_n = \sum_{k=1}^{m_n} r(w_k^n) \cdot \chi_{\set{w_k^n}} \in \tau[a,b]$.

  For $x = w^n_k \in A_{\frac1n}$ it holds that
  \[ \varphi_n(w_k^n) = \sum_{k=1}^{m_n} r(w_k^n) \cdot \underbrace{\chi_{\set{w_k^n}} (w_j^n)}_{\delta_{jk}} = r(w_j^n) \]
  so $\card{\varphi_n(x) - r(x)} = 0 \forall x \in A_{\frac1n}$.
  Let $x \in [a,b] \setminus A_{\frac1n}$. Then it holds $0 \leq r(x) < \frac1n$ and for $x \not\in A_{\frac1n}$ it holds that $\varphi(x) = 0$.
  Therefore,
  \[ \card{r(x) - \varphi(x)} = r(x) < \frac1n \]
  hence $\norm{r - \varphi_n}_{\infty} < \frac1n$. This means that $\varphi_n \to r$ uniformly on $[a,b]$.
  Therefore
  \[ \lim_{n\to\infty} \underbrace{\int_a^b \varphi_n \, dx}_{= 0} = \int_a^b r \, dx = 0 \]

  Now we want to finish the proof of our theorem: Let $r(x) = \card{f(x) - \tilde f(x)} \geq 0$ and $r(x) = 0$ for $x \not\in A$.
  So, $\int_a^b \card{f - \tilde f} \, dx = 0$ (first part proven).
  \[ \card{\int_a^b f \, dx - \int_a^b \tilde f \, dx} = \card{\int_a^b (f - \tilde f) \, dx} \leq \int_a^b \card{f - \tilde f} \, dx = 0 \]
  \[ \implies \int_a^b f\, dx = \int_a^b \tilde f \, dx \]
  Second part proven.
\end{proof}

\begin{lemma} % Lemma 11
  Let $f \in \mathcal R[a,b]$. Then it holds that $f_+ \in \mathcal R[a,b]$ and also $f_- \in \mathcal R[a,b]$.
\end{lemma}
\begin{proof}
  Only for $f_+$: First, we show: Let $x \in [a,b)$.
  \[ f_+(x) = \lim_{\xi \to x_+} f(\xi) = \lim_{\xi \to x_+} f_+(x) \]
  (the plus is important on the right-hand side!).

  Proof of this: Let $\varepsilon > 0$ be arbitrary. Then there exists $\delta > 0$ such that
  $\forall \xi \in (x, x + \delta)$: $\card{f(\xi) - f_+(x)} < \frac\varepsilon2$.
  Now let $z \in (x, x + \delta)$ be arbitrary chosen. For $z$ there exists $\xi \in (z, x + \delta)$.

  \begin{figure}[t]
    \begin{center}
      \includegraphics{img/20_x_z.pdf}
      \caption{$x$ and $z$}
      \label{img:xz}
    \end{center}
  \end{figure}

  $\xi$ sufficiently close enough to $z$ such that $\card{f(\xi) - f_+(z)} \leq \frac\varepsilon2$ because $f_+(z)$ exists.

  \[ \card{f_+(z) - f_+(x)} \leq \card{f_+(z) - f(\xi)} + \card{f(\xi) - f_+(x)} < \frac\varepsilon2 + \frac\varepsilon2 \]
  TODO some content missing here

  It remains to show: $f_+$ has left-sided limits.
  Let $x \in (a,b]$ be arbitrary and $f_-(x) = \lim_{\xi \to x_-} f(\xi)$. We show: $f_-(x) = \lim_{\xi \to x_-} f_+(x)$ (again: the plus is important).

  Let $\varepsilon > 0$ be arbitrary. Choose $\delta > 0$ such that $\forall z \in (x - \delta, x)$ it holds that $\card{f(z) - f_-(x)} < \frac\varepsilon2$.

  \begin{figure}[t]
    \begin{center}
      \includegraphics{img/21_xi_z.pdf}
      \caption{$\xi$ and $z$}
      \label{img:xiz}
    \end{center}
  \end{figure}

  Now let $\xi \in (x - \delta, x)$ (compare with Figure~\ref{img:xiz}) and choose $x > z > \xi$ with the property that $\card{f(z) - f_+(\xi)} < \frac\varepsilon2$ (possible because $f$ in $\xi$ has a right-sided limit):
  \[
    \card{f_+(\xi) - f_-(x)}
    \leq \underbrace{\card{f_+(\xi) - f(z)}}_{< \frac\varepsilon2} + \underbrace{\card{f(z) - f_-(x)}}_{< \frac\varepsilon2}
  \]
  because of the choice of $\delta$ and $z \in (\xi, x) \subseteq (x - \delta, x)$.

  Hence, $\lim_{\xi \to x_-} f_+(\xi) = f_-(x)$. Analogously for $f_-$
\end{proof}

\begin{remark}
  \[ \lim_{\xi \to x_+} f_+(\xi) = f_+(x) \]
  \[ \lim_{\xi \to x_-} f_-(\xi) = f_-(x) \]
  from the proof. So $f_+$ is right-sided continuous and $f_-$ is left-sided continuous.
\end{remark}

\begin{lemma} % Lemma 12
  Let $f \in \mathcal R[a,b]$. Then it holds that
  \[ \int_a^b f \, dx = \int_a^b f_+ \, dx = \int_a^b f_- \, dx \]
\end{lemma}

\begin{proof}
  For $f_+$:
  \[ f, f_+ \in \mathcal R[a,b] \]
  $\forall x \in [a,b]$ with $f$ is continuous in $x$ it holds that
  \[ f(x) = \lim_{\xi\to x} f(\xi) = \lim_{\xi\to x_+} f(\xi) = f_+(x) \]
  $f$ has at most countable many discontinuity points. By Satz~\ref{satz7countable},
  \[
    \int_a^b \card{f - f_+} \, dx = 0
    \quad \text{ or equivalently } \quad
    \int_a^b f \, dx = \int_a^b f_+ \, dx
  \]
\end{proof}

\subsection{Improper integrals}

Let $I$ be an interval in $\mathbb R$ with marginal points $a$ and $b$ with $-\infty \leq a < b \leq +\infty$.
Let $f$ be a regulated function on $I$.
We define
\begin{enumerate}
  \item If $I = [a,b)$, $\int_a^b f \, dx = \lim_{\beta \to b_-} \int_a^\beta f \, dx$
  \item If $I = (a,b]$, $\int_a^b f \, dx = \lim_{\alpha \to a_+} \int_\alpha^b f \, dx$
  \item If $I = (a,b)$, $\int_a^b f \, dx = \lim_{\alpha \to a_+} \int_\alpha^c f \, dx + \lim_{\beta \to b_-} \int_c^\beta f \, dx$
\end{enumerate}
for an arbitrarily chosen $c \in (a,b)$ under the constraint that the corresponding limits in $\mathbb R$ exist.

Standard examples will follow:
\begin{example}
  Let $s > 1$.
  \[ \int_1^\infty x^{-s} \, dx = \lim_{\beta \to \infty} \int_1^\beta x^{-s} \, dx = \left.\lim_{\beta \to \infty} \left(\frac{1}{-s + 1} x^{-s+1}\right) \right|_1^\beta \]
  \[ = \frac1{1 - s} \cdot \underbrace{\lim_{\beta \to \infty} \frac{1}{\beta^{\underbrace{s-1}_{> 0}}}}_{= 0} - \frac{1}{1 - s} \cdot 1 = \frac{1}{s-1} \]
  TODO drawing
\end{example}

\begin{example}
  Let $s < 1$.
  \[
    \int_0^1 x^{-s} \, dx = \lim_{\alpha \to 0_+} \int_\alpha^1 x^{-s} \, ds
    = \left. \lim_{\alpha\to 0_+} \frac{1}{-s + 1} x^{-s+1} \right|_\alpha^1
  \] \[
    = \frac1{1 - s} - \frac1{1 - s} \cdot \underbrace{\lim_{\alpha \to 0} \alpha^{\overbrace{1 - s}^{> 0}}}_{= 0} = \frac1{1 - s}
  \]
  TODO drawing

  For $s = 1$, neither $\int_0^1 \frac1x \, dx$ nor $\int_1^\infty \frac1x \, dx$ exists.
\end{example}

\begin{example}
  For $c > 0$,
  \[ \int_0^\infty e^{-cx} \, dx = \lim_{\beta \to \infty} \int_0^\beta e^{-cx} \, dx = \left. \lim_{\beta \to \infty} \left(-\frac1c\right) \cdot e^{-cx} \right|_0^\beta - \frac1c \cdot \underbrace{\lim_{\beta \to \infty} e^{-c\beta}}_{= 0} + \frac1c = \frac1c \]
\end{example}

\begin{theorem}[Direct comparison test for improper integrals] % Satz 8
  \label{dctii}
  In German, \enquote{\foreignlanguage{german}{Majorantenkriterium f\"ur uneigentliche Intergale}}.

  Let $f, g$ be regulated functions on $I$ and it holds that
  \[ \card{f(x)} \leq g(x) \forall x \in I \]
  Assume $\int_a^b g \, dx$ exists as improper integral. Then also the following improper integrals exist:
  \[ \int_a^b \card{f} \, dx \text{ and } \int_a^b f \, dx \]
  In German, $g$ is called \foreignlanguage{german}{Majorante} of $f$ (there is no equivalent terminology in English).
\end{theorem}

\begin{proof}
  Without loss of generality, let $I = [a,b)$.
  Let $G(\beta) = \int_a^\beta g \, dx$. We know that $\lim_{\beta \to b_-} G(\beta)$ exists.
  By Lemma~\ref{cauchy-crit} (Cauchy criterion for existence of limits):
  Let $\varepsilon > 0$ be arbitrary, then there exists a right-sided neighborhood $U$ of $b$
  ($U = (b - \delta, b)$ if $b < \infty$ and $U = (M, \infty)$ if $b = \infty$)
  with $u,v \in U$, then it holds that $\card{G(v) - G(u)} < \varepsilon$.
  \[
    \card{G(v) - G(u)} = \card{\int_a^v g \, dx - \int_a^u g \, dx}
      = \card{\int_u^a g \, dx + \int_a^v g \, dx}
      = \card{\int_u^v g \, dx}
  \]
  Let $F(\beta) = \int_a^\beta \card{f} \, dx$.
  Analogously as for $G$, it holds that $F(v) - F(u) = \int_u^v \card{f} \, dx$.
  Let $u,v \in U$. Then it holds that
  \[
    \card{F(v) - F(u)} = \card{\int_u^v \card{f} \, dx} \leq \card{\int_u^v g \, dx}
      = \card{G(v) - G(u)} < \varepsilon
  \]
  hence by the Cauchy criterion for $F$:
  $\lim_{\beta\to b_-} F(\beta)$ exists, so there exists $\int_a^b \card{f} \, dx$ as improper integral.
  The same applies for the existence of $\int_a^b f \, dx$.
\end{proof}

\begin{example}
  The cardinal sine function is defined as
  \[ \sinc(x) = \frac{\sin{x}}{x} \]
  \[ \lim_{x\to 0} \frac{\sin{x}}{x} = 1 \qquad \sinc(0) = 1 \]
  So $\sinc(x)$ is continuous on $\mathbb R$.
  \[ \int_0^\infty \frac{\sin{x}}{x} \, dx = \underbrace{\int_0^1 \underbrace{\frac{\sin{x}}{x}}_{\text{continuous}} \, dx}_{\text{exists}} + \int_1^\infty \frac{\sin{x}}{x} \, dx \]
  How about $\int_1^\infty \frac{\sin(x)}{x} \, dx$?
  \[
    \lim_{\beta\to\infty} \int_1^\beta \frac{\sin{x}}{x} \, dx =
      \begin{vmatrix}
        u = \frac1x & u' = -\frac1{x^2} \\
        v' = \sin{x} & v = -\cos{x}
      \end{vmatrix}
      = \lim_{\beta\to\infty} \left[
        -\frac1x \cos{x}
      \right]_1^\beta - \int_1^\beta \frac{\cos{x}}{x^2} \, dx
  \] \[
    = \cos(1) - \lim_{\beta\to\infty} \int_1^\beta \frac{\cos(x)}{x^2} \, dx
  \] \[
    \card{\frac{\cos(x)}{x^2}} \leq \frac1{x^2} \text{ on } [1, \beta]
  \]
  and $\int_1^\infty \frac1{x^2} \, dx$ exists.
  So $g(x) = \frac1{x^2}$ is a majorant of $\frac{\cos(x)}{x^2}$ and by Theorem~\ref{dctii},
  $\lim_{\beta\to\infty} \int_1^\beta \frac{\cos(x)}{x^2} \, dx$ eixsts.

  Attention! $\int_0^\infty \card{\frac{\sin(x)}{x}} \, dx$ does not exist. Is not Lebesgue integrable.
\end{example}

\index{Euler's Gamma function}
\index{Gamma function}
\begin{definition} % Definition 11
  Let $x > 0$.
  We call $\Gamma$ \emph{Euler's Gamma function}.
  \[ \Gamma(x) \coloneqq \int_0^\infty t^{x - 1} e^{-t} \, dx \]
\end{definition}

\begin{remark}
  The improper integral in the definition of the $\Gamma$-function exists for all $x > 0$.
\end{remark}

\dateref{2018/04/26}

TODO I missed the first 15 minutes

Proof of this:
\begin{proof}
  \[ \lim_{t\to\infty} \underbrace{t^{x-1}}_{\text{polynomially in } t} \cdot \underbrace{e^{-t}}_{\text{exponentially } \to 0} = 0 \]
  Also there exists $L > 1$, such that $\forall x > L$ it holds that $t^{x-1} e^{-t/2} < 1$ on $[1,L]$ (which is a compact interval) continuous.
  So there exists $M > 0$ such that $t^{x-1} e^{-\frac t2} \leq M \forall t \in [1,L]$.
  Let $c = \max\set{M, 1}$. Therefore it holds on $[1,L]$ and also on $(L,\infty)$.
  \[ t^{x-1} e^{-\frac t2} \leq c \]
  Multiply with $e^{-\frac t2} > 0$, then it holds that $t^{x-1} \cdot e^{-t} \leq ce^{-\frac t2} \forall t \in [1,\infty)$.
  \[ c \int_1^\infty e^{-\frac t2} \, dt \]
  exists.
  By the direct comparison test, we get $\int_1^\infty t^{x-1} e^{-t} \, dt$ exists.
\end{proof}

\begin{lemma} % Lemma 13
  \label{lemma13}
  For all $x > 0$ it holds that
  \[ \Gamma(x + 1) = x \cdot \Gamma(x) \qquad \text{ (functional equation of the $\Gamma$-function)} \]
  Especially with $\Gamma(1) = 1$ it holds that $\Gamma(n+1) = n!$ for all $n \in \mathbb N_0$.
\end{lemma}

\begin{proof}
  \[ \Gamma(x + 1) = \int_0^\infty t^{x+1-1} e^{-t} \, dt = \int_0^\infty t^{x} e^{-t} \, dt \]
  \begin{align*}
    &= \begin{vmatrix} u = t^x & u' = x \cdot t^{x-1} \\ v' = e^{-t} & v = -e^{-t} \end{vmatrix} \\
    &= \underbrace{\left. -t^x \cdot e^{-t} \right|_0^\infty}_{\substack{= 0 \text{ on the upper bound} \\ = 0 \text{ on the lower bound}}}
    + \int_0^\infty x \cdot t^{x-1} \cdot e^{-t} \, dt = x \int_0^\infty t^{x-1} e^{-t} \, dt = x \Gamma(x)
  \end{align*}
  \[ \Gamma(1) = \int_0^\infty \underbrace{t^{1-1}}_{=1} \cdot e^{-t} \, dt = \left. -e^{-t} \right|_0^\infty = 1 \]
  \[ \Gamma(n+1) = n \cdot \Gamma(n) = n \cdot (n - 1) \Gamma(n-1) = n \cdot (n-1) \cdot \ldots \cdot 1 \cdot \underbrace{\Gamma(1)}_{=1} = n! \]
\end{proof}

\begin{figure}[t]
  \begin{center}
    \includegraphics{img/22_gamma_on_C.pdf}
    \caption{$\Gamma$ on $\mathbb C$}
    \label{img:gamma}
  \end{center}
\end{figure}

\begin{remark}
  There exists a power series $\Gamma(x) = \sum_{n=0}^\infty a_n (x - x_0)^n$.
  $\Gamma(z)$ is also defined for $z \in \mathbb C$ with $\Re{z} > 0$.
  Compare with Figure~\ref{img:gamma}.
\end{remark}

\subsection{Young's inequality}

Some important inequalities in integration theory follow.

\index{Young's inequality}
\begin{theorem}[Young's inequality] % Satz 9
  Let $f: [0, \infty) \to [0,\infty)$ be continuous differentiable, strictly monotonically increasing with $f(0) = 0$ and $f$ is unbounded.
  Then $f: [0, \infty) \to [0,\infty)$ bijective and $f^{-1}: [0,\infty) \to [0,\infty)$ is strictly monotonically increasing and continuous.
  Let $a, b \geq 0$ be given. Then it holds that
  \[ ab \leq \int_0^\alpha f(x) \, dx + \int_0^b f^{-1}(y) \, dy \]
  Equality is given if and only if, $b = f(a)$ or $a = f^{-1}(b)$.
  Compare with Figure~\ref{img:young}.
\end{theorem}

\begin{figure}[t]
  \begin{center}
    \includegraphics{img/23_young.pdf}
    \caption{Young's inequality visualized. The blue area denotes $\int_0^\alpha f \, dx$ and $\int_0^b f^{-1}(y) \, dy$ is the green area.}
    \label{img:young}
  \end{center}
\end{figure}

\begin{proof}
  Let $f:[0,\infty) \to [0,\infty)$ be as above.
  Let $x_1 \neq x_2$. Without loss of generality $x_1 < x_2$.
  Then it holds that $f(x_1) < f(x_2) \implies f$ is injective.
  Surjectivity: $f(0) = 0$, hence $0 \in f([0,\infty])$.
  Let $\eta > 0$ be arbitrary. Because $f$ is unbounded, there exists $z \in (0,\infty)$ with $f(z) > \eta$.
  $f(0) = 0 < \eta < f(z)$.

  By the Intermediate Value Theorem ($f$ is continuous), there exists $\xi \in (0,z)$ with $f(\xi) = \eta$.
  So $f$ is surjective.
  \[ f^{-1}: [0,\infty) \to [0,\infty) \]
  \emph{Monotonicity:} Let $y_1 < y_2$. Then it holds that $x_1 = f^{-1}(y_1) < x_2 = f^{-1}(y_2)$.
  If this would not be true (hence, $x_2 \leq x_1$) then $y_2 = f(x_2) \leq y_1 = f(x_1)$ gives a contradiction.

  \emph{Continuity of $f^{-1}$:}
  Let $\varepsilon > 0$ be arbitrary. Let $y \in (0,\infty)$ be chosen arbitrarily.
  We show $f^{-1}$ is continuous in $y$.
  Let $x = f^{-1}(y) > 0$ and choose $\hat\varepsilon = \min\set{\frac{x}2, \frac{\varepsilon}2}$.
  \[ x_1 = x - \hat\varepsilon > 0 \qquad x_2 = x + \hat \varepsilon > 0 \]
  Let $y_1 = f(x_1)$, $y_2 = f(x_2)$, $x_1 = f^{-1}(y_1)$ and $x_2 = f^{-1}(y_2)$.
  By monotonicity of $f$: $x_1 < x < x_2 \implies y_1 < y < y_2$.

  \begin{figure}[t]
    \begin{center}
      \includegraphics{img/24_setup_delta.pdf}
      \caption{$\delta$, $y$, $y_1$ and $y_2$}
      \label{img:dyyy}
    \end{center}
  \end{figure}

  Choose $\delta = \min\set{y - y_1, y_2 - y} > 0$ (compare with Figure~\ref{img:dyyy}).
  Hence $(y - \delta, y + \delta) \subseteq (y_1, y_2) \forall \eta \in (y - \delta, y + \delta)$ it holds that
  \[ f^{-1}(\eta) < f^{-1}(y + \delta) < f^{-1}(y_2) = x_2 = x + \hat\varepsilon \]
  \[ f^{-1}(\eta) < f^{-1}(y - \delta) < f^{-1}(y_1) = x_1 = x - \hat\varepsilon \]
  So $f^{-1}(\eta) \in (x - \hat{\varepsilon}, x + \hat{\varepsilon})$, or equivalently
  \[ \card{\eta - y} < \delta \implies \card{f^{-1}(\eta) - \underbrace{f^{-1}(y)}_{=x}} < c \leq \frac{\varepsilon}{2} < \varepsilon \]
  So $f^{-1}$ is continuous in $y$ and $f^{-1}$ is continuous in $y_0$ analogously.
\end{proof}

Consider
\[
  \int_0^b f^{-1}(y) \, dy = \begin{vmatrix}
    y &= f(x) \\
    dy &= f'(x) \, dx \\
    y = 0 &\implies x = f^{-1}(0) = 0 \\
    y = b &\implies x = f^{-1}(b)
  \end{vmatrix}
  = \int_0^{f^{-1}(b)} \underbrace{f^{-1}(f(x))}_{=x} \cdot f'(x) \, dx = \int_0^{f^{-1}(b)} x \cdot f'(x) \, dx
\] \[
  \underbrace{=}_{\text{integration by parts}}
  \left. x \cdot f(x) \right|_0^{f^{-1}(b)} - 0 \int_0^{f^{-1}(b)} 1 \cdot f(x) \, dx
\] \[
  = f^{-1}(b) \cdot b - \int_0^{f^{-1}(b)} f(x) \, dx
\]
So
\[
  I = \int_0^a f(x) \, dx + \int_0^b f^{-1}(y) \, dy = \int_{f^{-1}(b)}^0 f(x) \, dx + b \cdot f^{-1}(b)
\] \[
  = \int_{f^{-1}(b)}^a f(x) \, dx + b \cdot f^{-1}(b)
\]

\begin{description}
  \item[Case 1] $a = f^{-1}(b)$
    \[ \implies I = \underbrace{\int_a^a f(x) \, dx}_{=0} + b \cdot a \]
  \item[Case 2] $b < f(a)$, or equivalently $f^{-1}(b) < a$
    \[ \implies \int_{f^{-1}(b)}^a \underbrace{f(x)}_{f(f^{-1}(b)) \text{ for } x > f^{-1}(b)} \, dx > \overbrace{b}^{\text{minimal value}} \cdot \underbrace{(a - f^{-1}(b))}_{\text{length of integration interval}}  \]
    Therefore $I > b(a - f^{-1}(b)) + b \cdot f^{-1}(b) = ab$.
  \item[Case 3] $b > f(a)$, or equivalently $f^{-1}(b) > a$
    \[ \int_{f^{-1}(b)}^a f(x) \, dx = \int_a^{f^{-1}(b)} \underbrace{(-f(x))}_{\substack{\text{monotonically decreasing} \\ > -f(f^{-1}(b)) \forall x \in [a,f^{-1}(b))}} \, dx > -f(f^{-1}(b)) \cdot (f^{-1}(b) - a) \]
    \[ = -b(f^{-1}(b) - a) \]
    \[ I > -b (f^{-1}(b) - a) + b \cdot f^{-1}(b) = ab \]
\end{description}

\begin{remark}
  Young's inequality also holds without requiring differentiability of $f$ (but the proof is more complex).
\end{remark}

\index{Conjugate exponents}
\begin{lemma}[Special case of Young's inequality] % Lemma 14
  \label{lemma14}
  Let $A, B \geq 0$ and $p,q > 1$ such that $\frac1p + \frac1q = 1 \iff p + q = p \cdot q$.
  Then $p$ and $q$ are called \emph{conjugate exponents}.
  Then it holds that $AB \leq \frac{A^p}{p} + \frac{B^q}{q}$.
\end{lemma}
\begin{proof}
  \[ f(x) = x^{p-1} \text{ in Young's inequality} \]
  \[ y = x^{p-1} \iff x = y^{\frac{1}{p-1}} \]
  \[ \frac{1}{p-1} = q - 1 \text{ is immediate, because } \]
  \[ \frac{1}{p-1} = q - 1 \iff 1 = pq - p - q + 1 \iff p + q = pq \]
  So $f^{-1}(y) = y^{\frac{1}{p-1}} = y^{q - 1}$.
  By Young's inequality:
  \[ AB \leq \int_0^A x^{p-1} \, dx + \int_0^B y^{q - 1} \, dy \]
  \[ = \left.\frac{x^p}{p}\right|_0^A + \left. \frac{y^q}{q} \right|_0^B = \frac{A^p}{p} + \frac{B^q}{q} \]
\end{proof}

\begin{remark}
  \[ AB = \frac{A^p}{p} + \frac{B^q}{q} \]
  Equality holds if and only if $B = A^{p-1} \iff B^q = A^{\overbrace{pq-q}^{p}} = A^p$.
\end{remark}

\subsection{H\"older's ineqaulity}

\index{H\"older's inequality}
\index{$L^p$-norm}
\begin{theorem}[H\"older's inequality] % Satz 10
  Let $I$ be an interval with boundary values $a$ and $b$. $-\infty \leq a < b \leq +\infty$.
  Let $p$ and $q$ be conjugate exponents. Let $f_1$ and $f_2$ be regulated function on $I$
  such that
  \[ \int_a^b \card{f_1(x)}^p \, dx < \infty \]
  \[ \int_a^b \card{f_2(x)}^q \, dx < \infty \]
  both exist.

  We let $\norm{f_1}_p \coloneqq \left(\int_a^b \card{f_1(x)}^p \, dx\right)^{\frac1p}$ and $\norm{f_2}_q \coloneqq \left(\int_a^b \card{f_2(x)}^q \, dx\right)^{\frac1q}$.
  They are called $L^p$-norm of $f_1$ and $L^q$-norm of $f_2$.

  Then it holds that
  \[ \int_a^b \card{f_1(x) \cdot f_2(x)} \, dx < \infty \]
  exists and
  \[ \int_a^b \card{f_1(x) \cdot f_2(x)} \, dx \leq \norm{f_1}_p \cdot \norm{f_2}_q \]
\end{theorem}

\begin{proof}
  Assume that $\norm{f_1}_p > 0$  and $\norm{f_2}_q > 0$.
  Let $A = \frac{\card{f_1(x)}}{\norm{f_1}_p}$ and $B = \frac{\card{f_2(x)}}{\norm{f_2}_q}$.
  By Lemma~\ref{lemma14},
  \[
    \frac{\card{f_1(x)}}{\norm{f_1}_p} \cdot \frac{\card{f_2(x)}}{\norm{f_2}_q}
    \leq \frac{1}{q} \cdot \frac{\card{f_1(x)}^p}{\norm{f_1}^p_p} + \frac1q \cdot \frac{\card{f_2(x)}^q}{\norm{f_2}_q^q}
  \]
  We integrate the inequality,
  \[
    \frac{1}{\norm{f_1}_p \cdot \norm{f_2}_q} \cdot \int_a^b \card{f_1(x) \cdot f_2(x)} \, dx
  \] \[
    \leq \frac{1}{p} \cdot \frac{1}{\norm{f_1}^p_p} \cdot \underbrace{\int_a^b \card{f_1(x)^p} \, dx}_{= \norm{f_1}_1^p}
    + \frac1q \cdot \frac{1}{\norm{f_2}_q^q} \underbrace{\int_a^b \card{f_2(x)}^q \, dx}_{= \norm{f_2}_q^q}
    = \frac1p + \frac1q = 1
  \]

  \[
    \frac{1}{\norm{f_1}_p \cdot \norm{f_2}_q} \cdot \int_a^b \card{f_1(x) \cdot f_2(x)} \, dx
    \implies \int_a^b \card{f_1(x) f_2(x)} \, dx
    \leq \norm{f_1}_p \cdot \norm{f_2}_q
  \]
  Special case: Let $\norm{f_1}_p = 0$
  \[
    \implies
    \left(\int_a^b \card{f_1(x)}^p \, dx\right)^{\frac1p} = 0
    \implies \int_a^b \underbrace{\card{f_1(x)}^p}_{\geq 0} \, dx = 0
  \]
  By Theorem~\ref{satz7countable}, $f_1(x) = 0 \forall x \in [a,b] \setminus A$
  and $A$ is at most countable.
  \[ \implies f_1(x) \cdot f_2(x) = 0 \forall x \in [a,b] \setminus A \]
  \[ \implies \int_a^b \card{f_1(x) \cdot f_2(x)} \, dx = 0 \]
  \[ \implies 0 = 0 \text{ in H\"older's inequality} \]
\end{proof}

\index{Cauchy-Schwarz inequality}
\begin{remark}[Special case of H\"older's inequality]
  Let $p = q = 2$, $\frac12 + \frac12 = 1$.
  \[ \int_a^b \card{f_1(x) \cdot f_2(x)} \, dx \leq \norm{f_1}_2 \norm{f_2}_2 \]
  is called Cauchy-Schwarz inequality for $L^2$ functions.

  \[ \int_a^b f_1(x) f_2(x) \, dx = \angel{f_1,f_2}_2 = \angel{f_1, f_2}_{L^2} \]
  is an inner product on a proper space of functions.
\end{remark}

\section{Elaboration on differential calculus}

We consider a metric space $X$ and functions $f: X \to \mathbb C$.
We define a concept of uniform convergence of such sequences:
\[ f_n: X \to \mathbb C \quad (n \in \mathbb N) \text{ and } f: X \to \mathbb C \]
We say, $(f_n)_{n \in \mathbb N}$ converges uniformly towards $f$ if $\forall \varepsilon > 0 \forall N \in \mathbb N$
such that $\forall x \in X$ and $\forall n \geq N$ it holds that
\[ \underbrace{\card{f_n(x) - f(x)}}_{\text{absolute value in } \mathbb C} < \varepsilon \]
\[ \iff \sup\set{\card{f_n(x) - f(x)}: x \in X} < \varepsilon \]

\begin{remark}
  Do not use $\norm{f}_{\infty}$ for the definition of uniform convergence,
  because $f_n$ and $f$ must not be necessarily bounded. Hence,
  \[ \norm{f}_\infty = \set{\card{f(x)}: x \in X} \]
  must not be finite.
\end{remark}

\begin{theorem} % Satz 1
  \label{satz1cont}
  Let $X$ be a metric space, $f_n: X \to \mathbb C$ be a sequence of continuous functions and $f: X \to \mathbb C$ such that
  $f_n \to f$ uniform on $X$. Then $f$ is also continuous on $X$.
\end{theorem}

\dateref{2018/05/03}

\begin{proof}
  Let $\varepsilon > 0$ be arbitrary.
  Choose $x \in X$. Show: $f$ is continuous in $x$.

  Compare with Figure~\ref{img:uniconv}.

  \begin{figure}[t]
    \begin{center}
      \includegraphics{img/25_uniform_convergence_of_fN_to_f.pdf}
      \caption{Uniform convergence of $f_N$ to $f$}
      \label{img:uniconv}
    \end{center}
  \end{figure}

  Because of uniform convergence $f_n \to f$, there exists $N \in \mathbb N$ such that $\card{f_N(z) - f(z)} < \frac\varepsilon3 \forall z \in X$.
  Let $N$ be fixed. Because $f_N$ is continuous in $x$, there exists $\delta > 0$ such that $d(x, \xi) < \delta \implies \card{f_N(\xi) - f_N(x)} < \frac\varepsilon3$.

  We consider now $\xi \in X$ with $d_X(x, \xi) < \delta$. Then it holds that
  \begin{align*}
    \card{f(x) - f(\xi)}
      &= \card{f(x) - f_N(x) + f_N(x) - f_N(\xi) + f_N(\xi) - f(\xi)} \\
      &\leq \underbrace{\card{f(n) - f_N(x)}}_{< \frac\varepsilon2} + \underbrace{\card{f_N(x) - f_N(\xi)}}_{< \frac\varepsilon3} + \underbrace{\card{f_N(\xi) - f(\xi)}}_{< \frac\varepsilon3} \\
      &= \varepsilon
  \end{align*}
  by uniform convergence, by continuity and by uniform convergence respectively.

  Thus, $f$ is continuous in $x$.
\end{proof}

\begin{theorem} % Satz 2
  Let $P(z) = \sum_{k=0}^\infty a_k z^k$ be a power series in $\mathbb C$ with convergence radius $\rho_P > 0$.
  Furthermore, let $0 < r < \rho_P$.
  Let $P_n(z) = \sum_{k=0}^n a_k z^k$ ($n$-th partial sum of $P$).
  Then $P_n \to P$ uniformly on $\overline{K_r(0)}$.
\end{theorem}

\begin{remark}
  \begin{figure}[t]
    \begin{center}
      \includegraphics{img/26_uniform_convergence.pdf}
      \caption{We cannot make a general statement about convergence/divergence. But on every small closed sphere $P$ converges absolutely for every $z$}
      \label{img:uconv}
    \end{center}
  \end{figure}
\end{remark}

\begin{proof}
  Approximation theorem for power series.
  Lettl Analysis 1, lecture notes, section 5, theorem~10.

  Let $0 < r < \rho_P$. Choose $\overline{r}$ with $r < \overline{r} < \rho_P$.
  Then it holds for $z \in \overline{K_r(0)}$ that
  \[ \card{P(z) - P_n(z)} < \frac{\overline{r}}{\overline{r} - r} \cdot \left(\frac{r}{\overline{r}}\right)^n \]
  \[ \frac{r}{\overline{r}} < 1 \]
  hence $\left(\frac{r}{\overline{r}}\right)^n$ is arbitrary small, for every $n$ sufficiently large.
  \[ \implies \sup\set{\card{P(z) - P_n(z): z \in \overline{K_r(0)}}} \leq \underbrace{\frac{\overline r}{\overline r - r}}_{\text{fixed}} \cdot \underbrace{\left(\frac{r}{\overline{r}}\right)^n}_{\substack{\text{arbitrary small for $n$} \\ \text{sufficiently large}}} \]
  Hence, $P_n \to P$ uniform on $\overline{K_r(0)}$.
\end{proof}

\begin{corollary}
  $P$ is continuous on $K_{\rho_P}(0)$.
\end{corollary}

\begin{theorem}
  Let $I \subseteq \mathbb R$ be an interval.
  Let $f_n: I \to \mathbb R$ be continuously differentiable on $I \forall n \in \mathbb N$.
  It holds that
  \begin{enumerate}
    \item $\exists g: I \to \mathbb R$ such that $f_n' \to g$ uniform on $I$
    \item $\exists f: I \to \mathbb R$ such that $\forall x \in I$ it holds that $f(x) = \lim_{n\to\infty} f_n(x)$ (\enquote{pointwise convergence}).
  \end{enumerate}
  Then it holds that $f$ is continuously differentiable on $I$ and $g = f'$.
\end{theorem}

\begin{proof}
  $g$ is continuous as uniform limit of continuous $f_n'$ (Theorem~\ref{satz1cont}).
  For $f_n$, the Fundamental Theorem of Differential Calculus can be applied ($f_n'$ is continuous, hence a regulated function).
  Let $x_0 \in I$. Then it holds that
  \[ f_n(x) = f_n(x_0) + \int_{x_0}^x f_n'(\xi) \, d\xi \]
  Convergence for $n \to \infty$:
  \[ f_n(x) \to f(x) \qquad f_n(x_0) \to f(x_0) \]
  (Pointwise convergence)
  \[ \int_{x_0}^x f_n'(\xi) \, d\xi \to \int_{x_0}^x g(\xi) \, d\xi \]
  Therefore, for $n \to \infty$,
  \[ f(x) = f(x_0) + \int^x_{x_0} g(\xi) \, d\xi \]
  The right-hand side is continuously differentiable by $x$ according to the Fundamental Theorem, variant 1,
  with
  \[ \left(f(x_0) + \int_{x_0}^x g(\xi) \, d\xi\right)'(x) = g(x) \]
  Hence, by $f(x) = f(x_0) + \int^x_{x_0} g(\xi) \, d\xi$ it follows that
  \[ f'(x) = g(x) \qquad \forall x \in I \]
\end{proof}

To finish our proof, we need a result we missed in the section about Integrals.

\begin{lemma}
  Let $(f_n)_{n \in \mathbb N}$ be a sequence of regulated functions on $[a,b]$ and
  $f_n \to f$ uniform on $[a,b]$. Then it holds that
  \[ \int_a^b \card{f_n - f} \, dx \to 0 \qquad \text{ for } n \to \infty \qquad \text{ especially } \int_a^b f_n \, dx \to \int_a^b f \, dx \]
\end{lemma}

\begin{proof}
  $f$ as a uniform limit of regulated functions is a regulated function.
  The proof has been done in the practicals.

  Let $N \in \mathbb N$ large enough such that
  \[ \forall n \geq N \forall x \in [a,b]: \card{f_n(x) - f(x)} < \frac{\varepsilon}{b - a} \]
  Then it holds that
  \[ \int_a^b \card{f_n(x) - f(x)} \, dx < \int_a^b \frac{\varepsilon}{b - a} \, dx = \frac{\varepsilon}{b - a} (b - a) = \varepsilon \]
  Hence,
  \[ \lim_{n\to\infty} \int_a^b \card{f_n(x) - f(x)} \, dx = 0 \]
  \[ \underbrace{\card{\int_a^b f_n \, dx - \int_a^b f \, dx}}_{\implies \to 0 } \leq \underbrace{\int_a^b \card{f_n - f} \, dx}_{\to 0} \]
  So,
  \[ \int_a^b f \, dx = \lim_{n\to\infty} \int_a^b f_n \, dx \]
\end{proof}

\subsection{Higher derivatives and Taylor's Theorem}

\begin{definition} % Definition 1
  Let $f: I \to \mathbb R$, $I \subseteq \mathbb R$ is an interval.
  We define inductively:
  \[ f^{(0)}(x) = f(x) \]
  Assume $f^{(n-1)}$ is defined continuously on $I$ and differentiable in $x \in I$.
  Then we let
  \[ f^{(n)}(x) = \left(f^{(n-1)}\right)'(x) \]
  $f^{(n)}(x)$ is called $n$-th derivative of $f$ in $x$.
  
  Notational remark:
  \[ f^{(0)} = f \qquad f^{(1)} = f' \qquad f^{(2)} = f'' \qquad f^{(3)} = f''' \qquad f^{(4)} = f'''' \]

  Furthermore, we let
  \[ \mathcal C^n(I) \coloneqq \set{f: I \to \mathbb R: f^{(k)}(x) \text{ exists } \forall x \in I \text{ and } x \mapsto f^{(k)}(x) \text{ is continuous } \forall 0 \leq k \leq n} \]
  We call $\mathcal C$ the space of $n$-times continuously differentiable functions on $I$.
\end{definition}

\begin{remark}
  $\mathcal C^n(I)$ is a vector space.
  If $I = [a,b]$ is compact, then
  \[ \norm{f}_{\mathcal C^n} = \max\set{\sup{\card{f^{(k)}(x)}: x \in I}: 0 \leq k \leq n} \]
  defines a norm on $\mathcal C^n(I)$ with $\sup{\card{f^{(k)}(x)}: x \in I} = {\norm{f^{(k)}}_{\infty}}$.
\end{remark}

\begin{remark}[New topic]
  Let $f \in \mathcal C^n(I)$ and $x_0 \in I$. Find an appropriate polynomial $T$ which approximated $f$ in an environment of $x_0$ in the \enquote{best} way.
\end{remark}

\begin{definition} % Definition 2
  Let $P(x) = \sum_{k=0}^n a_k x^k$ be a polynomial with $a_n \neq 0$
  (hence degree of $P$ is $n$).
  \[ P \in \mathbb R[x] \dots \text{ set of all polynomials with coefficients in } \mathbb R \]
  This set of polynomials is a ring.

  $x_0 \in \mathbb R$ is called $k$-times root of $P$ ($k \in \mathbb N$) if $Q \in \mathbb R[x]$ exists such that
  $P(x) = (x - x_0)^k Q(x)$ with $Q(x_0) \neq 0$.
\end{definition}

\begin{remark}
  $P(x) = (x - x_0)^k \cdot Q(x)$ means that division of $P$ by $(x - x_0)^k$ gives no remainder.
  Recall that division with remainder means that $\exists \hat Q, \hat R$ that are polynomials of degree $\hat R < k$,
  \[ P(x) = (x - x_0)^k \cdot \hat Q(x) + \hat R(x) \]
  $\hat Q, \hat R$ is unique.
  If $P(x) = (x - x_0)^k \cdot Q(x) \implies \hat R = 0, \hat Q = Q$.
\end{remark}

\begin{lemma} % Lemma 2
  Let $P(x) = \sum_{l=0}^n a_l x^l$ with $a_n \neq 0$.
  Let $1 \leq k \leq n$. Then it holds that $x_0 \in \mathbb R$ is a $k$-times root of polynomial $P \iff$
  $P^{(j)}(x_0) = 0$ for $j = 0, \ldots, k-1$ and $P^{(k)}(x_0) \neq 0$.
\end{lemma}

\begin{proof}
  Proof by complete induction.

  \begin{description}
    \item[Induction begin]
      Consider $k=1$. Direction $\implies$.

      Let $x_0$ be a simple root of $P$, then it holds that $P(x) = (x - x_0) \cdot Q(x)$ and $Q(x_0) \neq 0$.
      Hence, $P(x_0) = (x_0 - x_0) \cdot Q(x_0) = 0$ and $P'(x) = Q(x) + (x - x_0) \cdot Q'(x)$.
      Thus, $P'(x_0) = Q(x_0) + (x_0 - x_0) \cdot Q'(x_0) = Q(x_0) \neq 0$.

      Direction $\impliedby$.

      Let $P(x_0) = 0$ and $P'(x_0) \neq 0$. Division with remainder: $P(x) = (x - x_0) \cdot Q(x) + R(x)$
      with $\operatorname{degree}(R) \leq \operatorname{degree}(x - x_0) = 1$. Thus, $R$ is constant.
      We insert $x_0$. This gives $P(x_0) = (x_0 - x_0) \cdot Q(x_0) + R$ with $P(x_0) = 0$ and $(x_0 - x_0) = 0$.
      Hence, $R = 0$ is the zero polynomial and $P(x) = (x - x_0) \cdot Q(x)$.
      It remains to show that $Q(x_0) \neq 0$. $P'(x) = 1\cdot Q(x_0) + (x - x_0) \cdot Q'(x_0)$.
      We insert $x = x_0 \implies 0 \neq P'(x_0) = Q(x_0) + (x_0 - x_0) \cdot Q'(x)$.
      Thus is holds that $Q(x_0) = P'(x_0) \neq 0$.
    \item[Induction step]
      \begin{claim}[Auxiliary claim]
        Let $P(x) = (x - x_0) \cdot \tilde P(x)$.
        Let $P, \tilde P$ be polynomials. Then it holds $\forall j \in \mathbb N$ that
        \[ P^{(j)}(x) = (x - x_0) \cdot \tilde P^{(j)}(x) + j \cdot \tilde P^{(j-1)}(x) \]
      \end{claim}
      \begin{proof}
        Proof by complete induction.

        Let $j = 1$.
        \[ P'(x) = 1 \cdot \underbrace{\tilde P(x)}_{\tilde P^{(0)}(x)} + (x - x_0) \cdot \underbrace{\tilde P'(x)}_{\tilde P^{(1)}(x)} \]

        Consider $j \to j + 1$.
        \begin{align*}
          P^{(j+1)}(x) &= \left(P^{(j)}\right)'(x) \\
          &\underbrace{=}_{\substack{\text{induction} \\ \text{assumption}}} ((x - x_0) \cdot \tilde P^{(j)}(x) \\
          &+ j \tilde P^{(j-1)}(x))' (x - x_0) \tilde P^{(j+1)}(x) + \tilde P^{(j)}(x) + j \cdot \tilde P^{(j)}(x) \\
          &= (x - x_0) \tilde P^{(j+1)}(x) + (j + 1) \cdot \tilde P^{j}(x)
        \end{align*}
      \end{proof}

      We continue with the induction step after verifying our auxiliary claim. Direction $\implies$.

      Let $x_0$ be an $k+1$ times zero of $P$. Hence $P(x) = (x - x_0)^{k+1} \cdot Q(x)$. $Q(x_0) \neq 0$.
      Let $\tilde P(x) = (x - x_0)^k \cdot Q(x)$. 
      We can apply the induction assumption on $\tilde P$. Hence
      \[ \tilde P^{(j)} = 0 \qquad \text{ for } j = 0, \ldots, k-1 \qquad \text{ and } \qquad \tilde P^{(k)}(x_0) \neq 0 \]
      \[ P(x) = (x - x_0) \cdot \tilde P(x) \]
      By the auxiliary claim, $P^{(j)}(x) = (x - x_0) \cdot \tilde P^{(j)}(x) + j \cdot \tilde P^{(j-1)}(x)$.
      Therefore
      \[
        P^{(j)}(x_0) = j \cdot \tilde P^{(j-1)}(x) = \begin{cases}
          0 & \text{ for } j = 0, \dots, k \\
          (k+1)\tilde P^{(k)}(x_0) \neq 0 & \text{ for } j = k+1
        \end{cases}
      \]
      Hence, our claim about the derivatives is true (all derivatives are zero).

      Direction $\impliedby$.

      Let $P^{(j)}(x_0) = 0$ for $j = 0, \dots, k$ and $P^{(k+1)}(x_0) \neq 0$ and induction assumption holds for $k$.
      Division with remainder and $P^{(0)}(x_0) = 0 \implies P(x) = (x - x_0) \cdot \tilde P(x)$.
      By our auxiliary claim, we get
      \[ P^{(j)}(x) = (x - x_0) \cdot \tilde P^{(j)}(x) + j \tilde P^{(j-1)}(x) \]
      we insert $x = x_0$ and use $P^{(j)}(x_0) = 0$ for $j = 0, \dots, k$
      \[ \implies \tilde P^{(j)}(x_0) = 0 \qquad \text{ for } j = 0, \dots, k-1 \]
      By the induction assumption, $\tilde P(x) = (x - x_0)^k Q(x)$ with $Q(x_0) \neq 0$
      \[ \implies P(x) = (x - x_0) \cdot \tilde P(x) = (x - x_0)^{k+1} Q(x) \]

  \end{description}
\end{proof}


\end{document}
